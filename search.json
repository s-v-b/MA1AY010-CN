[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA1AY010 Class Notes",
    "section": "",
    "text": "Warm up",
    "crumbs": [
      "Warm up"
    ]
  },
  {
    "objectID": "index.html#a-short-undergraduate-course-on-probability-theory-in-an-applied-mathematics-curriculum",
    "href": "index.html#a-short-undergraduate-course-on-probability-theory-in-an-applied-mathematics-curriculum",
    "title": "MA1AY010 Class Notes",
    "section": "A short undergraduate course on Probability theory in an applied mathematics curriculum",
    "text": "A short undergraduate course on Probability theory in an applied mathematics curriculum\nThis Probability course is a core part of Master ISIFAR, a curriculum that delivers training in Statistics, Mathematical Finance, and Computer Science at Université Paris Cité. The Probability course is related to other courses from the same curriculum.\n\nStatistique M1\nMathématiques financières M1\nMathématiques financières M2\nApprentissage statistique M2\n\nThis list is not exhaustive.\nThis course is geared towards applications. We borrow examples and applications of probability theory from statistics, computer science, big data engineering. As we have a limited amount of time, we sweep a lot of dust under the carpet. We take for granted key results from integration and measure theory. Nevertheless, we build on rigorous definitions, state and invoke theorems in a consistent way.\n\n Homepage of the course.\n Moodle page of the course.",
    "crumbs": [
      "Warm up"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "MA1AY010 Class Notes",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course builds on two Licence-level courses:\n\nProbabilités Licence 3\nIntégration",
    "crumbs": [
      "Warm up"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Hashing\nIn this chapter we survey the basic definitions of Probability Theory starting from a simple modeling problem from computer science. The notions are formally defined in next chapters. The simple context allows us to carry out computations and to outline the kind of results we will look for during the course: moments, tail bounds, law of large numbers, central limit theorems, and possibly other kind of weak convergence results.\nHashing is a computational technique that is used in almost every area of computing, from databases to compilers through (big) datawarehouses. Every book on algorithms contain a discussion of hashing, see for example Introduction to Hashing by Jeff Erickson.\nUnder idealized conditions, hashing \\(n\\) items to \\(m\\) values consists of applying a function picked uniformly at random among the \\(m^n\\) functions from \\(1, \\ldots, n\\) to \\(1, \\ldots, m\\). The performance of a hashing method (how many cells have to be probed during a search operation?) depends on the typical properties of such a random function.\nIt is convenient to think of the values in \\(1, \\ldots, m\\) as numbered bins and of the items as \\(n\\) numbered balls. Picking a random function amounts to throw independently the \\(n\\) balls into the \\(m\\) bins. The probability that a given ball falls into a given bin is \\(1/m\\).\nQuestions around the random functions can be rephrased.\nHave a look at the http://stephane-v-boucheron.fr/post/2019-09-02-idealizedhashing/ and download the notebook from there.\nThis toy yet useful model is an opportunity to recall basic notions of probability theory.\nIn the sequel, we call this framework the random alllocations experiment.\nTable 1.1: An outcome of the random allocation experiment with n=10 and m=5\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n\\(\\omega\\)\n2\n3\n1\n2\n4\n4\n2\n3\n2\n5\nIn Table 1.1, line \\(\\omega\\) represents the outcome of a random allocation with \\(n= 10\\), \\(m= 5\\): \\(\\omega_4= 2\\), \\(\\omega_5= 4\\), …",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-hashing",
    "href": "01-intro.html#sec-hashing",
    "title": "1  Introduction",
    "section": "",
    "text": "How many empty bins on average?\nDistribution of the number of empty bins?\nHow many bins with \\(r\\) balls?\nWhat is the maximum number of balls in a single bin?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#a-probability-space",
    "href": "01-intro.html#a-probability-space",
    "title": "1  Introduction",
    "section": "1.2 A Probability space",
    "text": "1.2 A Probability space\nThe set of outcomes is called the universe. In the random allocations setting it is the set of \\(1, \\ldots, m\\)-valued sequences of length \\(m\\). This is also a function mapping \\(\\{1, \\ldots, n\\}\\) to \\(\\{1, \\ldots, m\\}\\). We denote a generic outcome by \\(\\omega\\). The \\(i^{\\text{th}}\\) element of \\(\\omega\\) is denoted by \\(\\omega_i\\). This universe is denoted by \\(\\Omega\\), here it is finite with cardinality \\(m^n\\).\nIn this simple setting, the uniform probability distribution on the universe assigns to each subset \\(A\\) of \\(\\Omega\\) the probability \\(|A|/|\\Omega|\\). When the universe is finite or countable, all subsets of the universe are events, assigning a probability to every subset of the universe is not an issue.\nRecall that a probability distribution \\(P\\) maps a collection \\(\\mathcal{F}\\) of subsets of the universe (\\(\\mathcal{F} \\subseteq 2^\\Omega\\)) to \\([0,1]\\) and satisfies:\n\n\\(P(\\emptyset)=0\\)\n\\(P(\\Omega)=1\\)\nfor any countable collection of pairwise disjoint events \\(A_1, A_2, \\ldots, n, \\ldots\\), \\(P(\\cup_{n=1}^\\infty A_n) = \\sum_{n=1}^\\infty P(A_n)\\)\n\nSee Section 2.3.\nThis entails \\(P(A_1 \\cup A_2 \\cup \\ldots \\cup A_k) = \\sum_{i=1}^k P(A_i)\\) for all finite collection of pairwise disjoint subsets \\(A_1,  \\ldots, A_k\\).\nFor the domain of \\(P\\) to be well-defined, the collection of subsets \\(\\mathcal{F}\\) has to be closed under countable unions, countable intersections and complementation, to contain the empty set \\(\\emptyset\\) and the universe \\(\\Omega\\). In words, it has to be a \\(\\sigma\\)-algebra, see Section 2.2.\nNote that other probability distributions make sense on this simple universe. See for example the balanced allocations scenario.\n\nIn the ballanced allocations scenario, the random functions from \\(1, \\ldots, n\\) to \\(1, \\ldots, m\\) are constructed sequentially. We first construct \\(\\omega_1\\) by picking a number uniformly at random from \\(1, \\ldots, n\\). Now, assume we have constructed \\(\\omega_1, \\ldots, \\omega_i\\) for some \\(i&lt;n\\). In order to determine \\(\\omega_{i+1}\\), we pick uniformly at random two numbers from \\(1, \\ldots, n\\), say \\(j\\) and \\(k\\). We compute\n\\[\nc_j = \\Big|\\{ \\ell : 1\\leq \\ell \\leq i, \\omega_\\ell = j\\}\\Big| \\qquad\\text{and} \\qquad c_k = \\Big|\\{ \\ell : 1\\leq \\ell \\leq i, \\omega_\\ell = k\\}\\Big| \\, .\n\\]\nIf \\(c_j &lt; c_k\\), \\(\\omega_{i+1}= j\\) otherwise \\(\\omega_{i+1}= k\\).\nThis iterative construction defines a (unique) probability distribution over \\(\\{1, \\ldots, m\\}^n\\) that differs from the uniform probability distribution. It is non-trivial to show that it achieves a non-trivial balancing guarantee for the size of the preimages induced by \\(\\omega\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#random-variables-and-independence",
    "href": "01-intro.html#random-variables-and-independence",
    "title": "1  Introduction",
    "section": "1.3 Random variables and independence",
    "text": "1.3 Random variables and independence\nConsider the real valued functions from \\(\\Omega\\) to \\(\\mathbb{R}\\) defined by:\n\\[X_{i, j}(\\omega) = \\begin{cases}\n  1 & \\text{if } \\omega_i = j \\\\\n  0 & \\text{otherwise} \\, .\n\\end{cases}\n\\]\nThis function is a special case of a random variable see Section 2.5.\nIn the toy example outlined in Table 1.1, we have \\(X_{4,1}(\\omega)=1, X_{5,1}(\\omega)=0, ...\\).\n\nNote that the definition of the random variable has nothing to do with the probability distribution we have considered so far. There is nothing random in a random variable. Moreover, a random variable is not a variable, it is a function. You may question this terminology, but it has been sanctified by tradition.\n\nIn the probability space \\((\\Omega, 2^\\Omega, \\Pr)\\), the distribution of the random variable \\(X_{i,j}\\) is a Bernoulli distribution with parameter \\(1/m\\).\n\\[\n    \\Pr \\Big\\{ X_{i,j} = 1\\Big\\} = \\frac{1}{m} \\qquad \\Pr \\Big\\{ X_{i,j} = 0\\Big\\}  = 1 - \\frac{1}{m} \\, ,\n\\]\nsee Section 4.1 for more on Bernoulli distributions. This comes from\n\\[\n    \\Pr \\Big\\{\\omega : X_{i,j}(\\omega) = 1\\Big\\} = \\frac{\\Big|\\{ \\omega : X_{i,j}(\\omega) = 1 \\}\\Big|}{m^n} = \\frac{m^{n-1}}{m^n} = \\frac{1}{m} \\, .\n\\]\nRecall that \\(\\Pr \\Big\\{ X_{i,j} = 1\\Big\\}\\) is a shorthand for \\(\\Pr \\Big\\{\\omega : X_{i,j}(\\omega) = 1\\Big\\}\\).\nFor a while, we fix some \\(j \\in \\{1, \\ldots, m\\}\\) and consider the collection of random variables \\((X_{i, j})_{i \\leq n}\\).\nFor each \\(i\\), we can define events (subsets of \\(\\Omega\\)) from the value of \\(X_{i,j}\\):\n\\[\\begin{align*}\n  & \\Big\\{ \\omega : X_{i,j}(\\omega) = 1\\Big\\} \\\\\n  & \\Big\\{ \\omega : X_{i,j}(\\omega) = 0\\Big\\}\n\\end{align*}\\]\nand together with \\(\\Omega, \\emptyset\\) they form the collection \\(\\sigma(X_{i,j})\\) of events that are definable from \\(X_{i,j}\\).\nRecall the definition of independent events or rather the definition of a collection of independent events.\n\nA collection of events \\(E_1, E_2, \\ldots, E_k\\) from \\((\\Omega, 2^{\\Omega})\\) is independent with respect to \\(\\Pr\\) if for all \\(I \\subseteq \\{1, \\ldots, n\\}\\),\n\\[\n\\Pr \\Big\\{\\cap_{i \\in I} E_i \\Big\\} = \\prod_{i \\in I} \\Pr \\{ E_i \\}\n\\]\n\nOne can check that for each fixed \\(j \\leq m\\), \\((X_{i, j})_{i \\leq n}\\) is a collection of independent random variables under \\(\\Pr\\). By this we mean that each collection \\(E_1, E_2, \\ldots, E_n\\) of events where \\(E_i \\in \\sigma(X_{i,j})\\) for each \\(i \\in \\{1, \\ldots, n\\}\\), \\(E_1, E_2, \\ldots, E_n\\) is an independent collection of events under \\(\\Pr\\).\nThe notion of independence is a cornerstone of probability theory, see Chapter ?sec-independence.\nConcretely, this means that for any sequence \\(b_1, \\ldots, b_n \\in \\{0,1\\}^n\\) (a possible outcome for the sequence of random variables \\(X_{1,j}, X_{2,j}, \\ldots, X_{n,j}\\)), we have\n\\[\\begin{align*}\n\\Pr \\Big\\{ \\bigwedge_{i=1}^n X_{i,j}(\\omega) = b_i \\Big\\}\n& = \\prod_{i=1}^n  \\Pr \\Big\\{  X_{i,j}(\\omega) = b_i \\Big\\} \\\\\n& = \\prod_{i=1}^n \\left(\\frac{1}{m}\\right)^{b_i} \\left(1-\\frac{1}{m}\\right)^{1-b_i} \\\\\n& = \\left(\\frac{1}{m}\\right)^{\\sum_{i=1}^n b_i} \\left(1-\\frac{1}{m}\\right)^{n- \\sum_{i=1}^n b_i}  \\, .\n\\end{align*}\\]\nObserve that the outcome of the sequence \\(X_{i,j}\\) for \\(i \\in 1,\\ldots,n\\) is \\(b_1, \\ldots, b_n\\) only depends on \\(\\sum_{i=1}^n b_i= Y_j\\). This greatly simplifies computations.\nWe are interested in the number of elements from \\(1, \\ldots, n\\) that are mapped (allocated) to \\(j\\) through the random function \\(\\omega\\). Let us define\n\\[\n    Y_j(\\omega) =  \\sum_{i=1}^n X_{i, j}(\\omega) \\, .\n\\]\nIn the toy example described in Table 1.1, \\(Y_3(\\omega) = 4\\) while \\(Y_5(\\omega)=1\\) and \\(Y_4(\\omega)=0\\):\n\n\n\n\n\nTable 1.2\n\n\n\n\n\nOccupancy scores for the random allocation experiment with n=10 and m=5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(j\\)\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\\(Y_j\\)\n\n\n1\n\n\n4\n\n\n2\n\n\n2\n\n\n1\n\n\n\n\nOccupancy scores\n\n\n\n\n\n\nIn the probability space \\((\\Omega, 2^\\Omega, \\Pr)\\), the random variable \\(Y_j\\) is distributed as a sum of independent, identically distributed Bernoulli random variables, that is, according to a Binomial distribution, see Section 4.1.\n\\[\n    \\Pr \\Big\\{ Y_j = r \\Big\\} =  \\binom{n}{r} p^r (1-p)^{n-r} \\qquad \\text{with} \\quad p =\\frac{1}{m}\n\\]\nfor \\(r \\in 0, \\ldots, n\\).\nIndeed, recall\n\\[\\begin{align*}\n\\Pr \\Big\\{ Y_j = r \\Big\\}\n  &  = \\sum_{\\omega : Y_j(\\omega) = r} \\Pr\\Big\\{\\omega\\Big\\} \\\\\n  &  = \\sum_{\\omega : Y_j(\\omega) = r} \\left(\\frac{1}{m}\\right)^{r} \\left(1-\\frac{1}{m}\\right)^{n- r} \\\\\n  &  = \\left| \\Big\\{ \\omega : \\omega \\in \\Omega, Y_j(\\omega) = r \\Big\\} \\right|\n  \\times \\left(\\frac{1}{m}\\right)^{r} \\left(1-\\frac{1}{m}\\right)^{n- r} \\\\\n  & = \\binom{n}{r} \\left(\\frac{1}{m}\\right)^{r} \\left(1-\\frac{1}{m}\\right)^{n- r} \\, .\n\\end{align*}\\]\nFor large \\(n,m\\), this Binomial distribution tends to be concentrated around its mean value or expectation\n\\[\n    \\mathbb{E} Y_j =  \\sum_{r=0}^n r \\times \\Pr \\Big\\{ Y_j = r \\Big\\} = \\frac{n}{m} \\, .\n\\]\nSee Chapter 3 for a systematic approach to expectation, variance and higher moments, based on Integration theory.\nThe last chapter ?sec-chapConcentration is dedicated the development of tail bounds for random variables like \\(Y_j\\) that are smooth functions of independent random variables.\nFor the moment recall that on a countable probability space, the expectation of random variable \\(Z\\) can be defined as\n\\[\n\\mathbb{E} Z =  \\sum_{\\omega \\in \\Omega} \\Pr\\{\\omega\\} \\times Z(\\omega)\n\\]\nprovided the series is absolutely convergent.\nThis is illustrated by Figure 1.1. In principle, a binomial random variable with parameters \\(n=5000\\) and \\(p=.001\\) can take any value between \\(0\\) and \\(5000\\). However, most (more than \\(95\\%\\)) of the probability mass is supported by \\(\\{1, \\ldots, 10\\}\\).\n\n\n\n\n\n\n\n\n\nFigure 1.1: Probability mass function of Binomial(5000,0.001)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-conv2poisson",
    "href": "01-intro.html#sec-conv2poisson",
    "title": "1  Introduction",
    "section": "1.4 Convergences",
    "text": "1.4 Convergences\nIf we let \\(n,m\\) tend to infinity while \\(n/m\\) tends toward \\(c&gt;0\\), we observe that, for each fixed \\(r\\geq 0\\) the sequence \\(\\Pr \\Big\\{ Y_j = r \\Big\\} = \\binom{n}{r} (1/m)^r (1-1/m)^{n-r}\\) tends towards \\[\n\\mathrm{e}^{-c} \\frac{c^r}{r !}\n\\]\nwhich is the probability that a Poisson distributed random variable with expectation \\(c\\)equals \\(r\\) (see Section 4.2 for more on Poisson distributions).\nThis is an instance of the law of rare events, a special case of convergence in distribution see Chapter 11.\nThe ability to approximate a Poisson distribution using an appropriate Binomial distribution is illustrated in Figure 1.2. The difference between the probability mass functions of the Binomial distributions with parameters \\(n=250, m=0.02\\), and \\(n=2500, m=0.002\\) and the Poisson distribution with parameter \\(5\\) is small. If we chose parameters \\(n=2500, m=0.002\\), the difference between Binomial and Poisson is barely visible.\n\n\n\n\n\n\n\n\n\nFigure 1.2: Probability mass functions of Binomial(250,0.02) (left), Binomial(2500,0.002) (middle) and Poisson(5) (right)\n\n\n\n\n\n\nThe proximity between Binomial\\((n, \\lambda/n)\\) and Poisson\\((\\lambda)\\) can be quantified in different ways. A simple one consists in computing \\[\n\\sum_{x \\in \\mathbb{N}}  \\Big| p_{n, \\lambda/n}(x) - q_\\lambda(x) \\Big|\n\\] where \\(p_{n, \\lambda/n}\\) (resp. \\(q_{\\lambda}\\)) stands for Binomial (resp. Poisson). This quantity is called the variation distance between the two probability distributions. A general definition is provided in Chapter 11. In Figure 1.3, this distance between Binomial distribution with parameters \\(n,5/n\\) and Poisson(5) is plotted against \\(n\\) (beware logarithmic scales). This plot suggests that the variation distance decays like \\(1/n\\). This is checked in Chapter 11.\n\n\n\n\n\n\n\n\n\nFigure 1.3: Law of rare events: distance between Binomial(\\(n\\), \\(5/n\\)) and Poisson(5) as a function of \\(n\\)\n\n\n\n\n\n\nIn the probability space \\((\\Omega, 2^\\Omega, \\Pr)\\), the random variables \\(Y_j, Y_j', j\\neq j'\\) are not independent. In order to show that \\(Y_j, Y_j', j\\neq j'\\) are not independent, it suffices to check that two events \\(E_j, E_{j'}\\) are not independent with \\(\\omega \\in E_j\\) being a function of \\(Y_j\\) and \\(\\omega \\in E_{j'}\\) being a function of \\(Y_{j'}\\) (later, we will concisely say \\(E_j \\in \\sigma(X_j)\\) or \\(E_j\\) being \\(Y_j\\)-measurable). Choose \\(E_j = \\{ \\omega : Y_j(\\omega) =r\\}\\) and \\(E_{j'} = \\{ \\omega : Y_{j'}(\\omega) =r\\}\\).\n\\[\\begin{align*}\n\\Pr(E_j) & = \\binom{n}{r} \\left(\\frac{1}{m}\\right)^r \\left(1 - \\frac{1}{m}\\right)^{n-r} \\\\\n\\Pr(E_j \\cap E_{j'}) & = \\binom{n}{r} \\times \\binom{n-r}{r}  \\left(\\frac{1}{m}\\right)^{2r} \\left(1 - \\frac{2}{m}\\right)^{n-2r} \\,\n\\end{align*}\\]\n\\[\n\\frac{\\Pr(E_j \\cap E_{j'}) }{\\Pr(E_j) \\times \\Pr(E_{j'})} =\n\\frac{\\left(1 - \\frac{2}{m}\\right)^{n-2r}}{\\left(1 - \\frac{1}{m}\\right)^{2n-2r}}\n\\frac{((n-r)!)^2}{n!(n-2r)!} \\neq 1 \\, .\n\\] Hence, if we define\n\\[\n    K_{n,r}(\\omega) = \\sum_{j=1}^m \\mathbb{I}_{Y_j(\\omega)=r}\n\\] as the number of elements of \\(1, \\ldots, m\\) that occur exactly \\(r\\) times in \\(\\omega\\), the random variable \\(K_{n,r}\\) is not described as a sum of independent random variables. Nevertheless, it is possible to gather a lot of information about its moments and distribution. If we let again \\(n,m\\) tend to infinity while \\(n/m\\) tends toward \\(c&gt;0\\), we observe that the distribution of \\(K_{n,r}/m\\) tends to concentrate around \\(\\mathrm{e}^{-c}    \\frac{c^r}{r !}\\). This is an example of convergence in probability, see Chapter 10.\nNow, if we consider the sequence of recentered and rescaled random variables \\((K_{n,r} - \\mathbb{E}K_{n,r})/\\sqrt{\\operatorname{var}(K_{n,r})}\\), we observe that its distribution function (see Section 2.7) converges pointwise towards the distribution function of the Gaussian distribution.\n\n\n\n\nProfile of example 2.1, count of empty urns is omitted\n\n\n$K_{n,1}$\n$K_{n,2}$\n$K_{n,4}$\n\n\n\n\n2\n2\n1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#intro-summary",
    "href": "01-intro.html#intro-summary",
    "title": "1  Introduction",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn this chapter, we investigated a toy stochastic model: random allocations. This toy model was motivated by the analysis of hashing, a widely used technique from Computer science. To perform the analysis, we introduced notation and notions from probability theory:\n\nUniverse,\nEvents,\n\\(\\sigma\\)-algebras,\nProbability distributions,\nPreimages,\nRandom variables,\nExpectation,\nVariance,\nIndependence of events,\nIndependence of random variables,\nBinomial distribution,\nPoisson distribution.\n\nThrough numerical simulations, we got a feeling of several important phenomena:\n\nLaw of rare events: approximation of Poisson distribution by certain Binomial distributions.\nLaw of large numbers for normalized sums of identically distributed random variables that are not independent.\nCentral limit theorems for normalized and centered sums of identically distributed random variables that are not independent\n\nAt that point, our elementary approach did not provide us with the notions and tools that make possible the rigorous analysis of these phenomena.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-language.html",
    "href": "02-language.html",
    "title": "2  A modicum of measure theory",
    "section": "",
    "text": "2.1 Roadmap\nPerforming stochastic modeling in a comfortable way requires consistent foundations and notation. In this chapter, we set the stage for further development. Probability theory started as the interaction between combinatorics and games of chance (XVIIth century). At that time, the set of outcomes was finite, and it was legitimate to think that any set of outcomes had a well-defined probability. When mathematicians started to perform stochastic modeling in different branches of sciences (astronomy, thermodynamics, genetics, …), they had to handle uncountable sets of outcomes. Designing a sound definition of what a probability distribution is, took time. Progress in integration and measure theory during the XIXth century and the early decades of the XXth century led to the modern, measure-theoretical foundation of probability theory.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "02-language.html#sec-tribe",
    "href": "02-language.html#sec-tribe",
    "title": "2  A modicum of measure theory",
    "section": "2.2 Universe, powerset and \\(\\sigma\\)-algebras",
    "text": "2.2 Universe, powerset and \\(\\sigma\\)-algebras\nA universe is a set (of possible outcomes) we decide to call a universe. The universe is often denoted by \\(\\Omega\\). Generic elements of \\(\\Omega\\) (outcomes) are denoted by \\(\\omega\\).\n\n\n\n\nExample 2.1 If we think of throwing a dice as a random phenomenon, the set of outcomes is the set of labels on the faces \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\). If we are throwing two dices, the set of outcomes is made of couples of labels \\(\\Omega' = \\{(1,1), (1, 2), (1, 3), \\ldots, (6,6)\\} =\\Omega^2\\).\n\n\n\n\n\nExample 2.2 In the idealized hashing problem (Section 1.1), the universe is the set of functions from \\(1, \\ldots, n\\) to \\(1, \\ldots, m\\). The size of the universe is \\(m^n\\).\n\n\n\n\nA universe may or may not be finite or countable. If the universe is countable, all its subsets may be called events. Events are assigned probabilities. If the universe is countable, it is possible to assign a probability to each of its subsets. When the universe is not countable (for example \\(\\mathbb{R}\\)), Assigning a probability to all subsets is not possible. We have to restrict the collection of subsets in order to assign probabilities to the collection members in a consistent way.\nIn the sequel \\(2^{\\Omega}\\) denotes the collection of all subsets of \\(\\Omega\\) (the powerset of \\(\\Omega\\)).\nA sensible collection of events has to be a \\(\\sigma\\)-algebra.\n\n\n\n\nDefinition 2.1 (\\(\\sigma\\)-algebra”) Given a set \\(\\Omega\\), a collection \\(\\mathcal{G}\\) of subsets of \\(\\Omega\\) (\\(\\mathcal{G} \\subseteq 2^{\\Omega}\\)) is called a \\(\\sigma\\)-algebra (a sigma algebra) iff\n\n\\(\\mathcal{G}\\) is closed under \\(countable\\) union\n\\(\\emptyset \\in \\mathcal{G}\\)\n\\(\\mathcal{G}\\) is closed under complementation (\\(A \\in \\mathcal{G} \\Rightarrow A^c = \\Omega \\setminus A \\in \\mathcal{G}\\))\n\n\n\n\n\n\nWhat the smallest \\(\\sigma\\)-algebra (with respect to set inclusion) that contains subset \\(A\\) of \\(\\Omega\\)?\n\n\n\n\nThe next proposition shows that \\(\\sigma\\)-algebras are stable under countable set-theoretical operations. We could have replaced countable union by countable intersection in the definition of \\(\\sigma\\)-algebras. This is consequence of De Morgan’s laws:\n\\[\n(A \\cup B)^c  = A^c \\cap B^c \\qquad \\text{and} (A \\cap B)^c  = A^c \\cup B^c\n\\]\n\nA \\(\\sigma\\)-algebra of subsets is closed under countable intersections.\n\n\n\n\n\nProof. For \\(A \\subseteq \\Omega\\), let \\(A^c = \\Omega \\setminus A\\). Let \\(A_1, \\ldots, A_n, \\ldots\\) belong to \\(\\sigma\\)-algebra \\(\\mathcal{G}\\) of subsets of \\(\\Omega\\). For each \\(n\\), \\(A_n^c \\in \\mathcal{G}\\), by definition of \\(\\sigma\\)-algebra, \\[\\begin{align*}\n\\cap_n A_n\n& = \\Big(\\big(\\cap_n A_n\\big)^c\\Big)^c \\\\\n& = \\Big(\\cup_n A_n^c\\Big)^c  \\qquad \\text{De Morgan} \\, .\n\\end{align*}\\] By definition of a \\(\\sigma\\)-algebra, \\(\\cup_n A_n^c \\in \\mathcal{G}\\), and for the same reason, \\(\\Big(\\cup_n A_n^c \\Big)^c \\in \\mathcal{G}\\).\n\n\n\n\nThe next proposition allows us to talk about the smallest \\(\\sigma\\)-algebra containing a collection of subsets, this leads to the notion of generated \\(\\sigma\\)-algebra.\n\n\n\n\nThe intersection of two \\(\\sigma\\)-algebras of subsets of \\(\\Omega\\) is a \\(\\sigma\\)-algebra of subsets of \\(\\Omega\\).\n\n\n\n\n\nProof. Let \\(\\mathcal{G}\\) and \\(\\mathcal{G}'\\) be two \\(\\sigma\\)-algebras of subsets of \\(\\Omega\\). The intersection of the two \\(\\sigma\\)-algebras is\n\\[\n\\Big\\{ A : A\\subseteq \\Omega, A \\in \\mathcal{G}, A \\in \\mathcal{G}' \\Big\\}\\, .\n\\]\n\n\n\n\nIndeed, the intersection of a possibly uncountable collection of \\(\\sigma\\)-algebras is a \\(\\sigma\\)-algebra (check this). Because of this property, the notion of a \\(\\sigma\\)-algebra generated by a collection of subsets is well-founded.\n\n\n2.2.1 Generated \\(\\sigma\\)-algebra\nGiven a collection \\(\\mathcal{C}\\) of subsets of \\(\\Omega\\), there exists a unique smallest \\(\\sigma\\)-algebra containing all subsets in \\(\\mathcal{C}\\), it is called the \\(\\sigma\\)-algebra generated by \\(\\mathcal{H}\\) and denoted by \\(\\sigma(\\mathcal{C})\\).\n\n\n\n\n\n\nExercise 2.1 Check the preceding proposition.\n\n\n\n\n\nExample 2.3 Consider we are throwing a dice, \\(\\Omega = \\{1, \\ldots, 6\\}\\), let \\[\\mathcal{H} = \\Big\\{\\{1, 3, 5\\}\\Big\\}\\, .\\] This is a collection made of one event (the outcome is odd). The algebra generated by \\(\\mathcal{H}\\) is \\[\\sigma(\\mathcal{H}) = \\Big\\{ \\{1, 3, 5\\}, \\{2, 4, 6\\}, \\emptyset, \\Omega \\Big\\} \\, .\\]\n\n\nTwo kinds of \\(\\sigma\\)-algebras play a prominent role in a basic probability course:\n\nthe powerset of countable or finite sets.\nthe Borel \\(\\sigma\\)-algebras of topological spaces.\n\n\n\nDefinition 2.2 (Borel sigma-algebra) The Borel \\(\\sigma\\)-algebra over \\(\\mathbb{R}\\) is the \\(\\sigma\\)-algebra generated by open sets. It is denoted by \\(\\mathcal{B}(\\mathbb{R})\\).\n\n\nThis definition works for every topological space. Recall that a topology on a set \\(E\\) is defined by a collection \\(\\mathcal{E}\\) of open sets. This collection is defined by the following list of properties:\n\n\\(\\emptyset, E \\in \\mathcal{E}\\)\nA (possibly uncountable) union of elements of \\(\\mathcal{E}\\) (open sets) belongs to \\(\\mathcal{E}\\) (is an open set)\nA finite intersection of open sets is an open set.\n\nIn the usual topology on \\(\\mathbb{R}\\), a set \\(A\\) is open if for any \\(x \\in A\\), there exists some \\(r&gt;0\\) such that \\(]x-r, x+r[ \\subseteq A\\). Any interval of the form \\(]a,b[\\) is open (these are the so-called open intervals).\nThis topology can be generalized to any finite dimension \\(\\mathbb{R}^d\\).\n\n\nExercise 2.2 Consider the \\(\\sigma\\)-algebra generated by open-intervals of \\(\\mathbb{R}\\). Is it the Borel \\(\\sigma\\)-algebra?\n\n\nExercise 2.3 Consider the \\(\\sigma\\)-algebra generated by open-intervals of \\(\\mathbb{R}\\) with rational bounds. Is it the Borel \\(\\sigma\\)-algebra?\n\n\nExercise 2.4 Consider any metric space \\((E,d)\\). The metric \\(d\\) defines a topology on \\(E\\). Does the Borel \\(\\sigma\\)-algebra on \\((E, d)\\) coincide with the \\(\\sigma\\)-algebra generated by open balls \\(B(x,r) = \\Big\\{  y : y\\in E, d(x,y)&lt;r\\Big\\}\\)?\n\nWe are now ready to set the stage of stochastic modeling. The playground always consists of a measurable space.\n\nDefinition 2.3 (Measurable space) A universe \\(\\Omega\\) endowed with a \\(\\sigma\\)-algebra of subsets \\(\\mathcal{F}\\) is called a measurable space. It is denoted by \\((\\Omega, \\mathcal{F})\\).\n\n\n\nExample 2.4  \n\nIf \\(\\Omega\\) is a countable or finite set, then \\((\\Omega, 2^\\Omega)\\) is a measurable space.\nIf \\(\\Omega=\\mathbb{R}\\), then \\((\\mathbb{R}, \\mathcal{B}({\\mathbb{R}}))\\) is a measurable space.\n\n\nSo far, we have not talked about probability theory, but, we are now equipped to define probability distributions and to manipulate them.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "02-language.html#sec-distribution",
    "href": "02-language.html#sec-distribution",
    "title": "2  A modicum of measure theory",
    "section": "2.3 Probability distributions",
    "text": "2.3 Probability distributions\nA probability distribution maps a \\(\\sigma\\)-algebra to \\([0,1]\\). It is an instance of a more general concept called a measure. We state or recall important concept of measure theory. The key idea underneath the elaboration of measure theory is that we should refrain from trying to measure all subsets of a universe (unless this universe is countable). Attempts to measure all subsets of \\(\\mathbb{R}\\) lead to paradoxes and of little practical use. Measure theory starts by recognizing the desirable properties any useful measure should possess, then measure theory builds objects satisfying these properties on as large as possible \\(\\sigma\\)-algebras of events, for example on Borel \\(\\sigma\\)-algebras.\nThis motivates the definition of \\(\\sigma\\)-additivity.\n\nDefinition 2.4 (Sigma-additivity) Given \\(\\Omega\\) and \\(\\mathcal{A} \\subseteq 2^\\Omega\\), a function \\(\\mu\\) mapping \\(\\mathcal{A}\\) to \\([0,\\infty)\\) is said to be \\(\\sigma\\)-additive on \\(\\mathcal{A}\\) if for any countable collection of pairwise disjoint subsets \\((A_n)_{n \\in \\mathbb{N}} \\in \\mathcal{A}\\), with \\(\\cup_n A_n \\in \\mathcal{A}\\) we have \\[\\mu (\\cup_{n \\in \\mathbb{N}} A_n) = \\sum_{n \\in \\mathbb{N}} \\mu(A_n)  \\, .\\]\n\nNote that if \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra, \\(\\Big(\\cup_{n \\in \\mathbb{N}} A_n\\Big) \\in \\mathcal{F}\\). \\(\\sigma\\)-additivity fits well with \\(\\sigma\\)-algebras, but it makes sense to define \\(\\sigma\\)-additivity with respect to more general collections of subsets.\n\n\n\n\nProposition 2.1 Given \\(\\Omega\\), a \\(\\sigma\\)-algebra \\(\\mathcal{A} \\subseteq 2^\\Omega\\), a \\(\\sigma\\)-_additive_function \\(\\mu\\) mapping \\(\\mathcal{A}\\) to \\([0,\\infty)\\) satisfies\n\nfor any increasing sequence \\((A_n)_{n \\in \\mathbb{N}}\\) of elements of \\(\\mathcal{A}\\)\n\\[\\lim_n \\mu(A_n) = \\mu\\left(\\cup_{n} A_n\\right)\\]\nfor any decreasing sequence \\((A_n)_{n \\in \\mathbb{N}}\\) of elements of \\(\\mathcal{A}\\)\n\\[\\lim_n \\mu(A_n) = \\mu\\left(\\cap_{n} A_n\\right)\\]\n\\[\\mu(\\emptyset) = 0\\]\n\n\n\nProof. a.) Let \\(B_{1} = A_1\\), and \\(B_{n+1} =  A_{n+1} \\setminus A_n\\) for each \\(n\\), then \\((B_n)_n\\) is a sequence of pairwise disjoints elements of \\(\\mathcal{A}\\). We have \\(\\cup_n B_n = \\cup_n A_n\\) and and by \\(\\sigma\\)-additivity, \\(\\mu(\\cup_n A_n) = \\sum_n \\mu(B_n)\\)\n\\[\\sum_{n\\leq m} \\mu(B_n) = \\mu\\left(\\cup_{n\\leq m} B_m\\right) = \\mu(A_m)\\]\nHence \\(\\lim_{m\\to \\infty} \\mu(A_m) = \\sum_{n\\in \\mathbb{N}} \\mu(B_n)= \\mu(\\cup_n A_n)\\)\nb.) The second statement is proved in a similar way.\nc.) Let \\((A_n)_n\\) be such that \\(A_n = \\emptyset\\) for each \\(n\\), this is a sequence of pairwise disjoint elements of \\(\\mathcal{A}\\), by \\(\\sigma\\)-additivity, we have\n\\[\\sum_{n\\in \\mathbb{N}} \\mu(\\emptyset) = \\mu(\\emptyset)\\]\nwhich implies \\(\\mu(\\emptyset)=0.\\)\n\\(\\square\\)\n\n\nDefinition 2.5 (Positive measure) Given a measurable space \\((\\Omega, \\mathcal{F})\\), a \\(\\sigma\\)-additive function \\(\\mu\\) mapping \\(\\mathcal{F}\\) to \\([0,\\infty)\\) is called a positive measure over \\((\\Omega, \\mathcal{F})\\).\nThe tuple \\((\\Omega, \\mathcal{F}, \\mu)\\) is called a measure space.\n\nBy Proposition 2.1, for any positive measure \\(\\mu\\), we have \\(\\mu(\\emptyset)=0\\). When \\(\\mu(\\Omega)\\) is finite, \\(\\mu\\) is said to be finite positive measure.\n\nExercise 2.5 Let \\(\\Omega = \\{0,1\\}^*\\) the set of infinite sequences of \\(0\\) and \\(1\\) (indexed from \\(1\\)). Let \\(\\mathcal{F}_n \\subseteq 2^\\Omega\\) be the \\(\\sigma\\)-algebra generated by events of the following form: \\(\\{ \\omega : \\omega \\in \\Omega, \\omega_i = 1\\}\\) for \\(1\\leq i\\leq n\\).\n\nDefine a \\(\\sigma\\)-additive function on \\((\\Omega, \\mathcal{F}_n)\\).\nWhat is the \\(\\sigma\\)-algebra generated by \\(\\cup_{n\\geq 1} \\mathcal{F}_n\\)?\nCan you define a \\(\\sigma\\)-additive function on \\((\\Omega, \\sigma(\\cup_{n\\geq 1} \\mathcal{F}_n))\\).\n\n\n\nA positive measure \\(\\mu\\) is not necessarily a probability distribution. For example, the counting measure \\(\\mu\\) on \\(\\mathbb{N}\\) satisfies \\(\\mu(A) = |A|\\) for all \\(A \\subseteq \\mathbb{N}\\), so we have \\(\\mu(\\mathbb{N})=\\infty.\\)\n\n\nDefinition 2.6 (Probability distribution) Given a measurable space \\((\\Omega, \\mathcal{F})\\), a function \\(\\mu\\) mapping \\(\\mathcal{F}\\) to \\([0,\\infty)\\) is a probability distribution over \\((\\Omega, \\mathcal{F})\\) if\n\n\\(\\mu\\) is a positive measure on \\((\\Omega, \\mathcal{F})\\) and\n\\(\\mu(\\Omega)=1\\).\n\n\n\nExercise 2.6 If you think \\((\\mathbb{R}, 2^{\\mathbb{R}})\\) is a measurable space, define a \\(\\sigma\\)-additive measure on it. Try even to define a probability measure.\n\n\nRemark 2.1. The notion of \\(\\sigma\\)-additivity is strictly stronger than finite additivity. Assuming the (as usual when working in Analysis or Probability), there exists a function \\(\\mu\\) that map \\(2^{\\mathbb{N}}\\) to \\([0,1]\\), that is additive (\\(\\mu(A \\cup B)= \\mu(A) + \\mu(B)\\) for all \\(A, B, A\\cap B=\\emptyset\\)), zero on all finite subsets of \\(\\mathbb{N}\\) and such that \\(\\mu(\\mathbb{N})=1\\). Such a function is not \\(\\sigma\\)-additive.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "02-language.html#sec-lebesgue",
    "href": "02-language.html#sec-lebesgue",
    "title": "2  A modicum of measure theory",
    "section": "2.4 Lebesgue measure",
    "text": "2.4 Lebesgue measure\n\nWe take the existence of Lebesgue’s measure for granted. This is the content of the next theorem.\n\n\nTheorem 2.1 (Existence of Lebesgue’s measure) There exists a unique \\(\\sigma\\)-additive measure \\(\\ell\\) on \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) such that \\(\\ell((a, b]) = b - a\\) for all finite \\(a &lt; b\\).\n\n\nTheorem 2.1 is typical of statements of measure theory. It defines a complex object (a measure) by its trace on a simple collection of sets (intervals).\nThe proof of Theorem 2.1 can be cut in several meaningful pieces. First define a length function on intervals. Show that this function can be extended to an additive function on finite union, finite intersection and complements of intervals. Then check that the extension is in fact \\(\\sigma\\)-additive on the closure of intervals under finite set-theoretical operations (which is not a \\(\\sigma\\)-algebra).\nOnce this additive extension is constructed, use Carathéodory’s extension theorem below to prove that the length function can be extended to a \\(\\sigma\\)-additive function on the \\(\\sigma\\)-algebra generated by intervals (the Borel \\(\\sigma\\)-algebra).\nThen it remains to check that the extension is unique. This can be done by a generating set argument, for example the monotone class Lemma Lemma 2.4.\n\nTheorem 2.2 (Carathéodory’s extension theorem”) Let \\(\\mathcal{A} \\subseteq 2^{\\Omega}\\). Assume \\(\\mathcal{A}\\) contains \\(\\emptyset, \\Omega\\), and is closed under finite unions, and complementation. Assume \\(\\rho : \\mathcal{A} \\to [0, \\infty]\\) is \\(\\sigma\\)-additive on \\(\\mathcal{A}\\).\nThen there exists a measure \\(\\mu\\) on \\(\\sigma(\\mathcal{A})\\) such that \\(\\mu(A)=\\rho(A)\\) for all \\(A \\in \\mathcal{A}\\).\n\nThe Lebesque measure existence theorem guarantees that we can define the uniform probability distribution over a finite interval \\([a,b]\\). If we denote Lebesgue measure by \\(\\ell\\), the uniform probability distribution over \\([a,b]\\) assign probability\n\\[\nP(A) = \\frac{\\ell(A)}{b-a} = \\frac{\\ell(A)}{\\ell([a,b])}\n\\]\nto any \\(A \\in \\mathcal{B}(\\mathbb{R}) \\cap [a,b]\\).\n\n\n\nThe uniform distribution over \\(([0,1], \\mathcal{B}([0,1]))\\) looks like an academic curiosity with no practical utility. This superficial opinion should be dispelled. Using a generator for the uniform distribution, it is possible to build a generator for any probability distribution over \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\). This can be done using a device called the quantile transform. In this sense, the uniform distribution is the mother of all distribution.\nAn outcome \\(\\omega\\) of the uniform distribution is a real number. How does a typical outcome look? A real number \\(\\omega \\in [0,1]\\) has binary expansions: \\(\\omega = \\sum_{i=1}^\\infty b_i 2^{-1}\\) with \\(b_i \\in \\{0,1\\}\\). What is the probability there is a unique binary expansion? First, check whether this probability is well-defined. Assuming the binary expansion is unique, \\(\\omega\\) is said to be normal if \\(\\lim_n \\frac{1}{n}\\sum_{i=1}^n b_i(\\omega) = 1/2\\). Is the probability of obtaining a normal number well-defined? If yes, compute it.\n\n\n\n\nExercise 2.7 Check that \\(\\mathcal{B}(\\mathbb{R}) \\cap [a,b] =  \\Big\\{ A \\cap [a,b] : A \\in \\mathcal{B}(\\mathbb{R}) \\Big\\}\\) is the \\(\\sigma\\)-algebra generated by the trace of the usual topology of \\(\\mathbb{R}\\) on \\([a,b]\\).\n\nThe Lebesgue existence theorem can be extended. Indeed, any sensible definition of the length of an interval can serve as a starting point.\nRecall that a real function is CADLAG if it is right-continuous everywhere, and has left-limits everywhere.\nThe next Theorem can be established in a way that parallels the construction of Lebesgue’s measure.\n\n\n\n\nTheorem 2.3 Any non-decreasing CADLAG function \\(F\\) on \\(\\mathbb{R}\\) defines a \\(\\sigma\\)-additive measure \\(\\mu\\) on \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) that satisfies:\n\\[\n\\mu((a, b]) = F(b) - F(a)\n\\]\n\n\n\n\nWe recover Lebesgue’s existence Theorem by taking \\(F(x)=x\\).\nIf we focus on functions \\(F\\) that satisfy \\(\\lim_{x \\to -\\infty} F(x)=0\\) and \\(\\lim_{x \\to \\infty} F(x)=1\\), Theorem Theorem 2.3 defines probability distributions through their cumulative distribution functions (more on this topic in Section 2.7).\n\n\n\n\nExercise 2.8 Do we really to assume that the function \\(F\\) has left-limits in Theorem Theorem 2.3?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "02-language.html#sec-measurablefunctionsrv",
    "href": "02-language.html#sec-measurablefunctionsrv",
    "title": "2  A modicum of measure theory",
    "section": "2.5 Measurable functions and random variables",
    "text": "2.5 Measurable functions and random variables\nSo far, we only talked probability and measure of sets (events). As stochastic modeling is at the root of quantitative analysis, we introduce the notion of measurable function. This allows us handle numerical functions that map outcomes to \\(\\mathbb{R}\\) or \\(\\mathbb{R}^d\\).\nNot every numerical function is measurable. To define what we call a measurable function, we need the notion of inverse image or preimage.\n\nDefinition 2.7 (Preimage) Let \\(f\\) be a function from \\(\\mathcal{X}\\) to \\(\\mathcal{Y}\\), we denote by \\(f^{-1}\\) the function that maps \\(2^{\\mathcal{Y}}\\) to \\(2^{\\mathcal{X}}\\) defined by\n\\[\\begin{align*}\nf^{-1}: \\qquad 2^{\\mathcal{Y}} & \\rightarrow  2^{\\mathcal{X}} \\\\\nB & \\mapsto f^{-1}(B) = \\Big\\{ x : x \\in \\mathcal{X}, f(x) \\in B \\Big\\} \\, .\n\\end{align*}\\]\nThe set \\(f^{-1}(B)\\) is called the preimage or inverse image of \\(B\\) under \\(f\\).\n\nNote that \\(f^{-1}\\) does not denote the inverse of function \\(f\\) which may not be injective. In this course, \\(f^{-1}\\) is a set function from the powerset of the codomain of \\(f\\) to the powerset of the domain of \\(f\\). The inverse function if it exists (or the generalized inverse function) is denoted by \\(f^\\leftarrow\\). The inverse function, when it exists, maps \\(f(\\mathcal{X})\\subseteq \\mathcal{Y}\\) to \\(\\mathcal{X}\\).\n\nExample 2.5 Recall the idealized hashing setting from Section 1.1. Let \\(\\Omega\\) denote the set of functions from \\(1, \\ldots, n\\) to \\(1, \\ldots, m\\) (assume \\(n \\leq m\\)). For \\(\\omega \\in \\Omega\\) (\\(\\omega\\) is function, but it is also a \\(1, \\ldots, m\\)-valued sequence of length \\(n\\)), let \\(f(\\omega)\\) be the number of values in \\(1, \\ldots, m\\) that have no occurrence in \\(\\omega\\) (the number of empty bins in the allocation defined by \\(\\omega\\)). The function \\(f\\) is a numerical function that maps \\(\\Omega=\\{1, \\ldots, m\\}^n\\) . For \\(B \\in \\mathbb{N}\\), \\(f^{-1}(B)\\) is the subset of allocations which have \\(k\\) empty bins, \\(k \\in B\\).\n\nThe preimage operation works well with set-theoretical operations.\nElementary properties of measurable functions follow from properties of inverse images. Inverse image preserves set-theoretical operations.\n\nProposition 2.2 Let \\(f: E \\mapsto F\\), then for \\(A, B, A_1, \\ldots,A_n , \\ldots   \\subseteq F\\),\n\\[\\begin{align*}\nf^{-1}(A \\cup B) & = f^{-1}(A) \\cup f^{-1}(B) \\\\\nf^{-1}(A \\cap B) & = f^{-1}(A) \\cap f^{-1}(B) \\\\\nf^{-1}(\\cup_{n \\in \\mathbb{N}}A_n ) & = \\cup_{n \\in \\mathbb{N}} f^{-1}(A_n) \\\\\nf^{-1}(\\cap_{n \\in \\mathbb{N}}A_n ) & = \\cap_{n \\in \\mathbb{N}} f^{-1}(A_n) \\\\\nf^{-1}(F \\setminus A) & =  f^{-1}(F) \\setminus f^{-1}(A)\n\\end{align*}\\]\n\n\n\n\n\nExercise 2.9 Check Proposition Proposition 2.2 from Section 2.7\n\n\n\n\n\nTaking the preimages of elements of a \\(\\sigma\\)-algebra defines a \\(\\sigma\\)-algebra.\n\n\nExercise 2.10 Let \\((\\Omega, \\mathcal{F})\\) and \\((\\Omega', \\mathcal{G})\\) be two measurable spaces. Let \\(f\\) map \\(\\Omega\\) to \\(\\Omega'\\), prove that \\[\\mathcal{H} = \\Big\\{ f^{-1}(B) : B \\in \\mathcal{G}\\Big\\}\\] is a \\(\\sigma\\)-algebra of subsets of \\(f^{-1}(\\Omega')\\).\n\n\n\nDefinition 2.8 (Measurable functions) Let \\((\\Omega, \\mathcal{F})\\) and \\((\\Omega', \\mathcal{G})\\) be two measurable spaces. A function \\(f: \\Omega \\to \\Omega'\\) is said to be \\(\\mathcal{F}/\\mathcal{G}\\)-measurable iff for \\(B \\in \\mathcal{G}\\), \\(f^{-1}(B) \\in \\mathcal{F}\\).\n\n\nUnder which condition on \\(\\mathcal{H}\\) is \\(f\\) \\(\\mathcal{F}/\\mathcal{G}\\)-measurable?\n\nExample 2.6 Recall the idealized hashing scenario from Section 1.1.\n\n\nExercise 2.11 Check that if \\(\\Omega\\) is a topological space and \\(\\mathcal{F}\\) the associated Borelian \\(\\sigma\\)-algebra, then any continuous function from \\(\\Omega\\) to \\(\\mathbb{R}\\) is measurable.\n\n\nExercise 2.12 If \\(\\Omega=\\mathbb{R}^d\\) is the Borel \\(\\sigma\\)-algebra, is it the smallest \\(\\sigma\\)-algebra that makes all continuous functions measurable?\n\n\nProposition 2.3 The pointwise limit of measurable functions is a measurable function: if \\((f_n)_n\\) is a sequence of measurable functions from \\((\\Omega, \\mathcal{F})\\) to \\((\\mathcal{X}, \\mathcal{G})\\), and \\(f_n \\to f\\) pointwise, then \\(f\\) is a measurable function.\n\n\n\nExercise 2.13 Prove Proposition Proposition 2.3\n\n\n\nProposition 2.4 The sum of measurable functions is a measurable function: if \\(f, g\\) are measurable functions from \\((\\Omega, \\mathcal{F})\\) to \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\), then \\(a f + b g\\) is a measurable function for all \\(a,b \\in \\mathbb{R}\\).\n\n\n\nExercise 2.14 Prove Proposition Proposition 2.4\n\n\n\nProposition 2.5 The composition of measurable functions is a measurable function: if \\(f\\) is a measurable function from \\((\\Omega, \\mathcal{F})\\) to \\((\\mathcal{X}, \\mathcal{G})\\), and \\(g\\) is a measurable function from \\((\\mathcal{X}, \\mathcal{G})\\) to \\((\\mathcal{Y}, \\mathcal{H})\\), then \\(g \\circ f\\) (\\(g \\circ f(\\omega) = g(f(\\omega))\\) for all \\(\\omega\\)) is a measurable function from \\((\\Omega, \\mathcal{F})\\) to \\((\\mathcal{Y}, \\mathcal{H})\\).\n\n\n\n\nExercise 2.15 Prove Proposition Proposition 2.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "02-language.html#sec-dynkin",
    "href": "02-language.html#sec-dynkin",
    "title": "2  A modicum of measure theory",
    "section": "2.6 The Monotone class theorem",
    "text": "2.6 The Monotone class theorem\nThe monotone class theorem or lemma is a powerful example of the generating class arguments that can be used to prove that two probability measures or maybe two \\(\\sigma\\)-finite measures are equal.\n\nDefinition 2.9 (\\(\\pi\\)-class) A collection \\(\\mathcal{G}\\) of subsets of \\(\\Omega\\) is said to be a \\(\\pi\\)-class if:\n\n\\(\\Omega \\in \\mathcal{G}\\)\nit is stable/closed by finite intersection \\[A, B \\in \\mathcal{G} \\Rightarrow A \\cap B \\in \\mathcal{G} \\, .\\]\n\n\n\nA \\(\\sigma\\)-algebra is a \\(\\pi\\) class, but the converse is false.\n\n\nDefinition 2.10 (Monotone class) A collection \\(\\mathcal{M}\\) of subsets of \\(\\Omega\\) is said to be a monotone class or a \\(\\lambda\\)-system if it satisfies the following properties:\n\n\\(\\Omega \\in \\mathcal{M}\\)\nIf \\(A, B \\in \\mathcal{M}\\), and \\(A \\subseteq B\\) then \\(B \\setminus A \\in \\mathcal{M}\\)\nIf \\(A_n \\in \\mathcal{M}\\) and \\(A_n \\subseteq A_{n+1}\\) for every \\(n \\in \\mathbb{N}\\) then \\(\\lim_n A_n = \\cup_{n \\in \\mathbb{N}} A_n \\in \\mathcal{M}\\).\n\n\n\nA \\(\\sigma\\)-algebra is a \\(\\lambda\\)-system.\n\nThe intersection of a collection of \\(\\lambda\\)-systems is a \\(\\lambda\\)-system. Hence, it makes sense to talk about the smallest \\(\\lambda\\)-system containing a collection of sets.\n\nThe next easy proposition makes \\(\\lambda\\)-system very useful when we want to check that two probability distributions are equal.\n\nProposition 2.6 The class of sets over which two probability distributions coincide is a \\(\\lambda\\)-system.\n\n\n\n\nProof. Let \\((\\Omega, \\mathcal{F})\\) be a measurable space. Let \\(P, Q\\) be two probability distributions over \\((\\Omega, \\mathcal{F})\\). Let \\(\\mathcal{C} \\subseteq \\mathcal{F}\\) be defined by\n\\[\n\\mathcal{C} = \\Big\\{ A : A \\in \\mathcal{F},  P(A)=Q(A) \\Big\\} \\, .\n\\]\nBy the very definition of measures we have \\(P(\\Omega)=Q(\\Omega)\\), hence \\(\\Omega \\in \\mathcal{C}\\).\nIf \\(A \\subseteq B\\) both belong to \\(\\mathcal{C}\\), again by the very definition of measures,\n\\[\nP (B \\setminus A) = P(B) - P(A) = Q(B) - Q(A) = Q(B \\setminus A) \\, ,\n\\]\nhence, \\(B\\setminus A \\subseteq \\mathcal{C}\\).\nLet \\(A_1 \\subseteq A_2 \\subseteq A_n \\subseteq \\ldots\\) be a non-decreasing sequence of elements of \\(\\mathcal{C}\\), again by the very definition of measures,\n\\[\nP(\\cup_n A_n) = \\lim_n \\uparrow P(A_n) = \\lim_n \\uparrow Q(A_n) = Q(\\cup_n A_n) \\, .\n\\]\nHence \\(\\mathcal{C}\\) is closed by monotone limits.\n\\(\\square\\)\n\n\n\n\nExercise 2.16 What happens if we consider the collections of measurable sets over which two measures are equal? What happens if we assume that the two measures are finite?\n\n\nDefinition 2.11 (\\(\\sigma\\)-finite measures) A measure \\(\\mu\\) on \\((\\Omega, \\mathcal{F})\\) is \\(\\sigma\\)-finite iff there exists \\((A_n)_n\\) with \\(\\Omega \\subseteq \\cup_n A_n\\) and \\(\\mu(A_n) &lt; \\infty\\) for each \\(n\\).\n\nFinite measures (this encompasses probability measures) are \\(\\sigma\\)-finite. Lebesgue measure is \\(\\sigma\\)-finite. The counting measure on \\(\\mathbb{R}\\) is not \\(\\sigma\\)-finite.\n\n\nWhat happens if we only assume that the two measures are \\(\\sigma\\)-finite?\n\n\n\n\nTheorem 2.4 (Monotone class lemma) If \\(\\mathcal{A}\\) is a \\(\\pi\\)-systen in \\(\\Omega\\) and \\(\\mathcal{M}\\) a \\(\\lambda\\)-system in \\(\\Omega\\) such that \\(\\mathcal{A} \\subseteq \\mathcal{M}\\), then the \\(\\sigma\\)-algebra generated by \\(\\mathcal{A}\\), \\(\\sigma(A)\\), is the smallest \\(\\lambda\\)-system larger than \\(\\mathcal{A}\\): \\[\\sigma(\\mathcal{A}) \\subseteq \\mathcal{M} \\,.\\]\n\n\n\n\nProof. Let \\(\\mathcal{M}\\) denote the intersection of all monotone classes that contain tyhe \\(\\pi\\)-system \\(\\mathcal{A}\\). As a \\(\\sigma\\)-algebra is a monotone class (a \\(\\lambda\\)-system), we have \\(\\mathcal{M} \\subseteq \\sigma(\\mathcal{A})\\), the only point that has to be checked is \\(\\sigma(\\mathcal{A}) \\subseteq \\mathcal{M}\\). It is enough to check that \\(\\mathcal{M}\\) is indeed a \\(\\sigma\\)-algebra.\nIn order to check that \\(\\mathcal{M}\\) is a \\(\\sigma\\)-algebra, it is enough to check that it is closed under finite union or equivalently under finite intersection.\nFor each \\(A \\in \\mathcal{A}\\), let \\(\\mathcal{M}_A\\) be defined by \\[\n\\mathcal{M}_A = \\Big\\{ B : B \\in \\mathcal{M}, A \\cap B \\in \\mathcal{M}\\Big\\} \\, .\n\\]\nRemember that \\(\\mathcal{A}\\) is a \\(\\pi\\)-system, and \\(\\mathcal{A} \\subseteq \\mathcal{M}\\), we have \\(\\mathcal{A} \\subseteq \\mathcal{M}_A\\). To show that \\(\\mathcal{M} = \\mathcal{M}_A\\), it suffices to show that \\(\\mathcal{M}_A\\) is a monotone class.\nIf \\((B_n)_n\\) is an increasing sequence of elements of \\(\\mathcal{M}_A\\), then\n\\[\n(\\cup_n B_n) \\cap A = \\cup_n \\Big(\\underbrace{B_n \\cap A }_{\\in \\mathcal{M}}\\Big) \\, ,\n\\]\nthe right-hand-side belongs to \\(\\mathcal{M}\\) since \\(\\mathcal{M}\\) is monotone. Hence \\(\\mathcal{M}_A\\) is closed by monotone increasing limit.\nTo check closure by complementation, let \\(B \\subseteq C\\) with \\(B, C \\in \\mathcal{M}_A\\). As\n\\[\nA \\cap (C \\setminus B) = \\Big(\\underbrace{A \\cap C}_{\\in \\mathcal{M}}\\Big) \\setminus \\Big(\\underbrace{A \\cap B}_{\\in \\mathcal{M}}\\Big)\n) \\,\n\\]\nthe closure of \\(\\mathcal{M}\\) under complementation entails \\(A \\cap (C \\setminus B) \\in \\mathcal{M}\\) and \\(C \\setminus B  \\in \\mathcal{M}_A.\\)\nNow, let \\(\\mathcal{M}^{\\circ}\\) be defined as \\[\n\\mathcal{M}^\\circ = \\Big\\{  A : A \\in \\mathcal{M}, \\forall B \\in \\mathcal{M},   A \\cap B \\in \\mathcal{M} \\Big\\} \\,.\n\\]\nWe just established that \\(\\mathcal{A} \\subseteq \\mathcal{M}^{\\circ}\\). Using the same line of reasoning allows us to check that \\(\\mathcal{M}^\\circ\\) is also a monotone class. This shows that \\(\\mathcal{M}^\\circ= \\mathcal{M}\\).\nWe are done.\n\\(\\square\\)\n\n\n\n\nCombining Proposition 2.6 and the Monotone Class Lemma (Theorem 2.4) leads to the next useful corollary.\n\n\n\n\nIf two probabilities \\(P, Q\\) on \\((\\Omega, \\mathcal{F})\\) coincide on a \\(\\pi\\)-system \\(\\mathcal{A}\\) that generates \\(\\mathcal{F}\\):\n\\[\n\\mathcal{A} \\subseteq \\{ A : A \\in \\mathcal{F}  \\text{ and } P(A)=Q(A)\\} \\qquad\\text{and} \\qquad \\mathcal{F} \\subseteq \\sigma(\\mathcal{A})\n\\]\nthen \\(P, Q\\) coincide on \\(\\mathcal{F}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "02-language.html#sec-distribfun",
    "href": "02-language.html#sec-distribfun",
    "title": "2  A modicum of measure theory",
    "section": "2.7 Probability distributions on the real line",
    "text": "2.7 Probability distributions on the real line\nA probability distribution is a complex object: it maps a large collection of sets (a \\(\\sigma\\)-algebra) to \\([0,1]\\). Fortunately, it is possible to characterize a probability distribution by simpler object. If we focus on probability distributions over \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\), they can be characterized by real functions on \\(\\mathbb{R}\\).\n\nDefinition 2.12 (Distribution function) Given a probability distribution \\(P\\) on \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\), the distribution function \\(F\\) of \\(P\\) maps \\(\\mathbb{R}\\) to \\([0,1]\\), it is defined by\n\\[\nx \\mapsto F(x) = P(-\\infty, x].\n\\]\n\nA probability distribution defines a unique distribution function. What is perhaps surprising is that a distribution function defines a unique probability distribution function.\n\nProposition 2.7 Let \\(F\\) be a function from \\(\\mathbb{R}\\) to \\([0,1]\\).\nThe function \\(F\\) is the distribution function of a probability distribution on \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\), iff the following five properties are satisfied:\n\n\\(F\\) is non-decreasing,\n\\(F\\) is right-continuous\n\\(\\lim_{y \\nearrow x} F(y)\\) exists at every \\(x \\in \\mathbb{R}\\) (\\(F\\) has left-limits everywhere)\n\\(\\lim_{x \\to -\\infty}F(x)=0\\)\n\\(\\lim_{x \\to \\infty} F(x)= 1.\\)\n\n\nThis is a rephrasing of Theorem 2.3.\nFigure Figure 2.1 shows the cumulative distribution function of Poisson distributions for different values of the parameter (see Sections Section 1.4 and Section 4.2 for more on Poisson distributions). For parameter \\(\\mu\\), \\(F_{\\mu}(x) = \\sum_{k \\leq x}  \\mathrm{e}^{-\\mu} \\frac{\\mu^k}{k!}\\).\n\n\n\n\n\n\n\n\n\nFigure 2.1: Cumulative distribution functions for Poisson distributions with different parameters. Observe that, apparently, \\(\\mu \\leq \\nu \\Rightarrow F_{\\mu} \\geq F_{\\nu}\\). How would you establish this domination property?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "02-language.html#sec-randomvariablesperse",
    "href": "02-language.html#sec-randomvariablesperse",
    "title": "2  A modicum of measure theory",
    "section": "2.8 General random variables",
    "text": "2.8 General random variables\nA real random variable is neither a variable, nor random. A real random variable is a measurable function from some measurable space to the real line endowed with the Borel \\(\\sigma\\)-algebra. There is nothing random in a random variable.\n\nDefinition 2.13 (Real valued random variable) Given a measurable space \\((\\Omega, \\mathbb{F})\\), a mapping \\(X\\) from \\(\\Omega\\) to \\(\\mathbb{R}\\) is a real valued random variable such that for every \\(B \\in \\mathcal{B}(\\mathbb{R})\\) the inverse image of \\(B\\):\n\\[\nX^{-1}(B) = \\{ \\omega : \\omega \\in \\Omega, X(\\omega) \\in {B}\\}\n\\]\nbelongs to \\(\\mathcal{F}\\)\n\nOnce a measurable space is endowed with a probability distribution, is it possible to define the (probability) distribution of a random variable.\n\nDefinition 2.14 Given \\((\\Omega, \\mathcal{F}, P)\\) and a real valued random variable \\(X\\), the law or probability distribution of \\(X\\), denoted by \\(P \\circ X^{-1}\\), is the probability distribution on \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) defined by\n\\[\n(P \\circ X^{-1})(B) =  P(X^{-1}(B)) \\quad \\text{for all } B \\in \\mathcal{B}(\\mathbb{R}).\n\\]\n\nRandom variables may be vector-valued, function-valued, etc. General random variables are defined as measurable functions between measurable spaces.\n\nDefinition 2.15 (Random variable) Given two measurable spaces \\((\\Omega, \\mathcal{F})\\), and \\((\\Omega', \\mathcal{G})\\) a mapping \\(X\\) from \\(\\Omega\\) to \\(\\Omega'\\) is a \\(\\mathcal{F}/\\mathcal{G}\\)-random variable if for every \\(B \\in \\mathcal{G}\\) the inverse image of \\(B\\):\n\\[\nX^{-1}(B) = \\{ \\omega : \\omega \\in \\Omega, X(\\omega) \\in {B}\\}\n\\]\nbelongs to \\(\\mathcal{F}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "02-language.html#bibliolanguage",
    "href": "02-language.html#bibliolanguage",
    "title": "2  A modicum of measure theory",
    "section": "2.9 Bibliographic remarks",
    "text": "2.9 Bibliographic remarks\nThere are many beautiful books on Probability Theory. They are targetted at different audiences. Some may be more suited to the students of the dual curriculum Mathématiques-Informatique. I found the following ones particularly useful.\nYoussef (2019) is a clear and concise collection of class notes designed for a Master I-level Probability course that is substantially more ambitious than this minimal course.\nDudley (2002) delivers a self-contained course on Analysis and Probability. The book can serve both as an introduction and a reference book. Beyond cautious and transparent proofs, it contains historical notes that help understand the connections between landmark results.\nPollard (2002) introduces measure and integration theory to an audience that has been exposed to discrete probability theory and that is familiar with probabilistic reasoning.\n\n\n\n\nDudley, R. M. (2002). Real analysis and probability (Vol. 74, p. x+555). Cambridge: Cambridge University Press.\n\n\nPollard, D. (2002). A user’s guide to measure theoretic probability (Vol. 8, p. xiv+351). Cambridge University Press, Cambridge.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A modicum of measure theory</span>"
    ]
  },
  {
    "objectID": "031-moments.html",
    "href": "031-moments.html",
    "title": "3  A modicum of integration",
    "section": "",
    "text": "3.1 Roadmap\nWe start by reviewing basic definitions and results from integration theory. We follow the measure-theoretic approach. First, we define simple functions, a subclass of piecewise measurable functions in Section 3.2). Defining the integral of a simple function with respect to a measure in Section 3.3) is straightforward. Some more work allows us to derive useful properties: linearity, monotonicity, to name a few. In Section 3.3), we define the integral of a non-negative measurable function as a supremum of integrals of simple functions. This definition is theoretically sound and it lends itself to computations. Section 3.4) states three convergence theorems culminating with the dominated convergence theorem.\nIn Section 3.6), we relate the notion of expectation of a random variable and the notion of integral. The Transfer Theorem ( Theorem 3.4) is a key instrument in the characterization of image distributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-simplefunctions",
    "href": "031-moments.html#sec-simplefunctions",
    "title": "3  A modicum of integration",
    "section": "3.2 Simple functions",
    "text": "3.2 Simple functions\nThe integral of a \\(\\{0,1\\}\\)-valued measurable function \\(f\\) with respect to a measure \\(\\mu\\) is defined \\[\\int_{\\Omega} f \\mathrm{d}\\mu = \\mu\\Big(f^{-1}(\\{1\\})\\Big) \\, ,\\] alternatively \\[\\int_{\\Omega} \\mathbb{I}_A \\mathrm{d}\\mu = \\mu(A) \\qquad \\text{for any measurable set } A\\,.\\] The next step consists in defining the integral of finite linear combinations of \\(\\{0,1\\}\\)-valued measurable function \\(f\\).\n\nDefinition 3.1 (Simple function) Let \\((\\Omega, \\mathcal{F})\\) be a measurable space. The function \\(f : \\Omega \\to \\mathbb{R}\\) is said to be simple iff\n\n\\(f\\) takes finitely many values: \\(\\Big|\\big\\{ f(x) : x \\in \\Omega\\big\\} \\Big|&lt;\\infty\\)\nFor each \\(y \\in f(\\Omega) \\subset \\mathbb{R}\\), \\(f^{-1}(\\{y\\}) \\in \\mathcal{F}\\).\n\n\nA simple function defines a partition of \\(\\Omega\\) into finitely many measurable classes. The simple function is constant on each class.\nIf \\(f\\) is a simple function, then the \\(\\sigma\\)-algebra \\(f^{-1}(\\mathcal{B}(\\mathbb{R}))\\) is finite.\n\nSimple functions are finite linear combinations of set characteristic (indicator) functions.\n\nFor each \\(A \\in \\mathcal{F}\\), \\(\\mathbb{I}_A\\) is simple\nFor any finite collection \\(A_1, \\ldots, A_n\\) of measurable subsets of \\(\\Omega\\), any sequence \\(c_1, \\ldots, c_n\\) of real numbers, \\(\\sum_{i \\leq n} c_i \\mathbb{I}_{A_i}\\) is a simple function\nFor any measurable function \\(f: \\Omega \\to \\mathbb{R}\\), and \\(n \\in \\mathbb{N}\\), the function \\(g_n\\) defined by \\[g_n(\\omega) =  n \\wedge (-n \\vee \\lfloor f(\\omega) \\rfloor)\\] is simple.\n\n\nThe definition of the integral of a simple function with respect to a measure is straightforward: it is a finite sum.\n\nDefinition 3.2 (Integral of a simple function) Let \\((\\Omega, \\mathcal{F}, \\mu)\\) be a measured space. Let \\(f : \\Omega \\to \\mathbb{R}\\) be a non-negative simple function which is defined by a finite partition of \\(\\Omega\\) into measurable sets \\(A_1, A_2, \\ldots, A_n\\) and numbers \\(f_1, \\ldots, f_n\\): \\[f(\\omega) = \\sum_{i \\leq n} f_i \\mathbb{I}_{A_i}(\\omega) \\,.\\] The integral of \\(f\\) with respect to \\(\\mu\\) is defined by \\[\\int_\\Omega f \\mathrm{d}\\mu = \\sum_{i \\leq n} f_i \\mu(A_i) \\, .\\]\n\n\n\nNote that if measure \\(\\mu\\) is not finite, the integral of a simple non-negative function may be infinite.\nIf \\(\\mu(A_i)=\\infty\\) and \\(f_i=0\\), we agree on \\(f_i \\mu(A_i) =0\\).\n\n\nIf we turn to signed simple functions, it is enough to notice than if \\(f\\) is simple, so are \\((f)_+\\) and \\((f)_-\\) and to define \\(\\int_\\Omega f \\mathrm{d}\\mu\\) as \\[\\int_\\Omega (f)_+ \\mathrm{d}\\mu - \\int_\\Omega (f)_- \\mathrm{d}\\mu\\] provided at leat one of the two summands is finite.\n\nAlthough they are simple, simple functions have interesting approximation capabilities. Any non-negative measurable function can be approximated from below by non-negative simple functions.\n\n\nProposition 3.1 (Approximation of measurable functions) Let \\((\\Omega, \\mathcal{F})\\) be a measurable space. Any non-negative measurable function \\(f: \\Omega \\to \\mathbb{R}\\) is the monotone pointwise limit of simple functions: there exists a sequence of simple function \\(f_1, \\ldots, f_n, \\ldots\\) such that for each \\(\\omega \\in \\Omega\\), the following holds: \\[f_1(\\omega) \\leq f_2(\\omega) \\leq \\ldots \\leq f_n(\\omega) \\leq \\ldots \\leq f(\\omega)\\] and \\[\\lim_n f_n(\\omega) = f(\\omega) \\, .\\]\n\n\nProof. Define \\(f_n\\) as \\[  f_n(\\omega) = n  \\wedge \\Big(2^{-n} \\big\\lfloor 2^n f(\\omega) \\big\\rfloor \\Big) \\, .\\] As \\[\\big\\lfloor 2^n f(\\omega) \\big\\rfloor \\leq 2^n f(\\omega)\\] we have \\(f_n(\\omega)\\leq f(\\omega)\\) for all \\(\\omega\\).\nThe range of function \\(f_n\\) is \\(i \\times 2^{-n}\\) for \\(i=0, \\ldots, n \\times 2^n\\). For each \\(i \\in 0, \\ldots, (n-1) \\times 2^n\\)\n\\[f_n^{-1}\\Big(\\{i \\times 2^{-n}\\}\\Big) =f^{-1}\\Big(\\Big[\\frac{i}{2^n}, \\frac{i+1}{2^n}\\Big)\\Big)\\]\nwhich is in \\(\\mathcal{F}\\) because \\(f\\) is measurable and \\(\\Big[\\frac{i}{2^n}, \\frac{i+1}{2^n}\\Big) \\in \\mathcal{B}(\\mathbb{R})\\).\nLikewise \\(f_n^{-1}\\Big(\\{n\\}\\Big) =f^{-1}\\big(\\big[n, \\infty\\big)\\big)\\) belongs to \\(\\mathcal{F}\\).\nTo check that \\(f_n \\leq f_{n+1}\\), we consider two cases.\n\n\\(f_{n+1}(\\omega)\\geq n\\). This entails \\(f(\\omega)\\geq n\\) and thus \\(f_n(\\omega)=n &lt;f_{n+1}(\\omega)\\)\n\\(f_{n+1}(\\omega) = k + i 2^{-n-1}\\) for \\(k&lt;n\\) and \\(i&lt;2^{n+1}\\). This entails \\(f_{n}(\\omega) = k + \\lfloor i/2\\rfloor  2^{-n} \\leq f_{n+1}(\\omega)\\).\n\nFinally if \\(f(\\omega) \\leq n\\), \\(0 \\leq f(\\omega) - f_n(\\omega) \\leq 2^{-n}\\). This implies that \\(\\lim_n f_n(\\omega)=f(\\omega)\\) for all \\(\\omega\\).\n\nFigure Figure 3.1 illustrates the approximation capabilities of simple functions.\n\n\n\n\n\n\n\n\n\nFigure 3.1: Approximation of the exponential function by simple functions \\(n \\wedge \\Big(2^{-n} \\big\\lfloor 2^n \\exp(\\omega) \\big\\rfloor \\Big)\\) for \\(n=2, 3, 4\\).\n\n\n\n\n\n\n\nIf \\(f,g\\) are two non-negative simple functions on \\((\\Omega, \\mathcal{F})\\), then for all \\(a, b\\in [0,\\infty)\\), \\(a f + b g\\) and \\(fg\\) are non-negative simple functions.\n\n\n\nCheck the proposition.\n\n\n\nProposition 3.2 (Monotonicity of integration of simple functions) If \\(f,g\\) are two non-negative simple functions and \\(\\mu\\) a non-negative measure on \\((\\Omega, \\mathcal{F})\\) such that \\[\\mu\\Big\\{ \\omega: f(\\omega)&gt; g(\\omega)\\Big\\} = 0 \\, .\\] (\\(f\\) is less of equal than \\(g\\) \\(\\mu\\)-almost everywhere), then \\[\\int  f  \\, \\mathrm{d}\\mu \\leq \\int  g  \\, \\mathrm{d}\\mu \\, .\\]\n\n\n\nCheck Proposition 3.2\n\n\n\nProposition 3.3 (Linearity of integration of simple functions) If \\(f,g\\) are two non-negative simple functions and \\(\\mu\\) a non-negative measure on \\((\\Omega, \\mathcal{F})\\), then for all \\(a, b\\in [0,\\infty)\\), \\[\\int a f + b g \\, \\mathrm{d}\\mu = a \\int  f  \\, \\mathrm{d}\\mu + b \\int  g \\, \\mathrm{d}\\mu \\, .\\]\n\n\n\nCheck Proposition Proposition 3.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-integration",
    "href": "031-moments.html#sec-integration",
    "title": "3  A modicum of integration",
    "section": "3.3 Integration",
    "text": "3.3 Integration\nLet \\(\\mathcal{S}_+\\) denote the set of non-negative simple functions on \\((\\Omega, \\mathcal{F})\\).\n\nDefinition 3.3 (Integration with respect to a measure) Let \\(f\\) be a non-negative measurable function on \\((\\Omega, \\mathcal{F}, \\mu)\\), then for any \\(A \\in \\mathcal{F}\\), the integral of \\(f\\) over \\(A\\) with respect to measure \\(\\mu\\) is defined by:\n\\[\n\\int_A f \\, \\mathrm{d}\\mu = \\sup_{s \\in \\mathcal{S}_+: s \\leq f} \\int_A s \\, \\mathrm{d}\\mu\n\\]\n\n\n\nIf the supremum is finite, the function is said to be integrable with respect to \\(\\mu\\), or to be \\(\\mu\\)-integrable.\n\n\n\nProposition 3.4 (Monotonicity of integration) If \\(f,g\\) are two non-negative measurable functions and \\(\\mu\\) a non-negative measure on \\((\\Omega, \\mathcal{F})\\) such that\n\\[\\mu\\Big\\{ \\omega: f(\\omega)&gt; g(\\omega)\\Big\\} = 0 \\, .\\]\n(\\(f\\) is less of equal than \\(g\\) \\(\\mu\\)-almost everywhere), then\n\\[\\int  f \\,  \\mathrm{d}\\mu \\leq \\int  g  \\, \\mathrm{d}\\mu \\, .\\]\n\n\n\nProve Proposition Proposition 3.4.\n\n\n\nProposition 3.5 (Linearity of integration) If \\(f,g\\) are two non-negative measurable functions and \\(\\mu\\) a non-negative measure on \\((\\Omega, \\mathcal{F})\\), then for all \\(a, b\\in [0,\\infty)\\),\n\\[\\int a f + b g \\, \\mathrm{d}\\mu = a \\int  f \\,  \\mathrm{d}\\mu + b \\int  g \\, \\mathrm{d}\\mu \\, .\\]\n\n\n\nProve Proposition Proposition 3.5.\n\n\nThe integral of a signed measurable functions is defined by a decomposition argument. Let \\(f\\) be a measurable function and \\(f= (f)_+ - (f)_-\\), then\n\\[\n\\int_{\\Omega} f \\mathrm{d}\\mu = \\int_{\\Omega} (f)_+ \\mathrm{d}\\mu - \\int_{\\Omega} (f)_- \\mathrm{d}\\mu\n\\]\nprovided at least one of \\(\\int_{\\Omega} (f)_+ \\mathrm{d}\\mu\\) and \\(\\int_{\\Omega} (f)_- \\mathrm{d}\\mu\\) is finite.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-biglimittheorems",
    "href": "031-moments.html#sec-biglimittheorems",
    "title": "3  A modicum of integration",
    "section": "3.4 Limit theorems",
    "text": "3.4 Limit theorems\nIn this section, measurable functions are meant to be real-valued, and \\(\\mathbb{R}\\) is endowed with the Borel \\(\\sigma\\)-algebra (\\(\\mathcal{B}(\\mathbb{R})\\)).\nTheorems Theorem 3.1, Theorem 3.2, Theorem 3.3 below are the three pillars of integral calculus.\n\nTheorem 3.1 (Monotone convergence theorem) Let \\((\\Omega, \\mathcal{F}, \\mu)\\) be a measured space. Let \\((f_n)_n\\) be a non-decreasing sequence of non-negative measurable functions converging towards \\(f\\). Then\n\\[\\int \\lim_n \\uparrow f_n \\, \\mathrm{d}\\mu = \\lim_n \\uparrow \\int f_n \\, \\mathrm{d}\\mu.\\]\n\n\nThe proof of the monotone convergence theorem boils down to the definition of positive measure and property \\(\\mu(\\lim_n \\uparrow A_n)= \\lim_n \\uparrow \\mu(A_n)\\).\n\n\nProof. Let function \\(f\\) be defined by \\(f(\\omega)=\\lim_n \\uparrow f_n(\\omega)\\) for all \\(\\omega \\in \\Omega\\). Note that if \\(f(\\omega)=0\\), then \\(f_n(\\omega)=0\\) for all \\(n\\in \\mathbb{N}\\).\nThe function \\(f\\) is positive measurable. In order to prove the monotone convergence theorem it is enough to check that for every non-negative simple function \\(g\\) such that \\(g \\leq f\\) everywhere, for any \\(a\\in [0, 1)\\), the following holds:\n\\[\na \\int g \\, \\mathrm{d} \\mu \\leq \\lim_n \\uparrow \\int f_n \\, \\mathrm{d}\\mu \\,.\n\\tag{3.1}\\]\nFor each \\(n \\in \\mathbb{N}\\), define\n\\[E_n = \\Big\\{ \\omega : f_n(\\omega) \\geq a g(\\omega)\\Big\\}.\\]\nNote that as \\((f_n)_n\\) is non-decreasing, the sequence \\((E_n)\\) is non-decreasing. Moreover, if \\(f(\\omega)&gt;0\\) as \\(\\lim_n \\uparrow f_n(\\omega)=f(\\omega) &gt; a f(\\omega) \\geq a g(\\omega)\\). Hence for all \\(\\omega \\in \\Omega\\), \\(\\mathbb{I}_{E_n}(\\omega)=1\\) for all sufficiently large \\(n\\) (beware there is no uniformity guarantee). We have\n\\[\\lim_n \\uparrow E_n = \\Omega\\, .\\]\nCombining the different remarks, we have for all \\(n\\), \\(\\mathbb{I}_{E_n} a g \\leq f_n\\) everywhere. Monotonicity of integration entails, for all \\(n\\)\n\\[\\int \\mathbb{I}_{E_n} a g \\,\\mathrm{d}\\mu \\leq \\int f_n \\,\\mathrm{d}\\mu \\qquad\\forall n\\, .\\]\nNow, for each \\(n\\), \\(\\mathbb{I}_{E_n} a g\\) is a non-negative simple function, and the sequence \\((\\mathbb{I}_{E_n} a g)_n\\) is a non-decreasing sequence of non-negative simple functions converging towards simple function \\(ag\\).\nLet \\(g = \\sum_{i \\leq k} c_i \\mathbb{I}_{A_i}\\) where \\((A_i)_{i\\leq k}\\) is a finite partition of \\(\\Omega\\) into measurable subsets.\n\\[\\mathbb{I}_{E_n}  g =  \\sum_{i \\leq k} c_i \\mathbb{I}_{A_i \\cap E_n} \\, .\\]\nHence\n\\[\n\\begin{array}{rl}\n  \\int \\mathbb{I}_{E_n} a g\\, \\mathrm{d}\\mu & = \\sum_{i \\leq k} c_i \\int \\mathbb{I}_{A_i \\cap E_n}\\, \\mathrm{d}\\mu \\\\\n    & = \\sum_{i \\leq k} c_i  \\mu(A_i \\cap E_n) \\, .\n\\end{array}\n\\]\nFor each \\(i \\leq k\\), we have \\(\\lim_n \\uparrow c_i  \\mu(A_i \\cap E_n)  = c_i \\mu(A_i)\\). We have:\n\\[\n\\int \\lim_n \\uparrow \\mathbb{I}_{E_n} a g \\,\\mathrm{d}\\mu = \\lim_n \\uparrow \\int \\mathbb{I}_{E_n} a g \\, \\mathrm{d}\\mu \\, .\n\\]\nThis proves that Equation 3.1 holds for all \\(a\\in [0,1)\\) and \\(g \\in \\mathcal{S}_+\\) with \\(g \\leq f\\):\n\\[\\forall g \\in \\mathcal{S}_+ \\text{ with } \\forall a \\in [0,1),\\]\n\n\n\n\n\nThe non-negativity assumptiom on \\(f_n\\) is not necessary. It is enough to assume \\(\\int f_1 \\mathrm{d}\\mu &gt; - \\infty\\). Prove this.\n\n\n\n\n\n\nLet \\((f_n)_n\\) be a monotone decreasing sequence of non-negative measurable functions. Let \\(f = \\lim_n \\downarrow f_n\\) (check the existence of \\(f\\)).\nIs it true that \\(\\int \\lim_n \\downarrow f_n \\mathrm{d}\\mu = \\lim_n \\downarrow \\int f_n \\mathrm{d}\\mu\\)?.\nAnswer the same question assuming \\(\\int f_1 \\mathrm{d}\\mu &lt; \\infty\\).\nAnswer the same question if \\(\\mu\\) is assumed to be a probability measure.\n\n\n\nTheorem 3.2 (Fatou’s Lemma) Let \\((\\Omega, \\mathcal{F}, \\mu)\\) be a measured space. Let \\((f_n)_n\\) be a sequence of non-negative measurable functions. Then\n\\[\\int \\liminf_n  f_n \\mathrm{d}\\mu \\leq  \\liminf_n  \\int f_n \\mathrm{d}\\mu.\\]\n\n\n\nProof. Define \\(h_n(\\omega) = \\inf_{m\\geq n} f_n(\\omega)\\). Each \\(h_n\\) is also non-negative and measurable. By monotonicity, \\[\\int h_n \\mathrm{d}\\mu \\leq \\inf_{m\\geq n} \\int f_m \\mathrm{d}\\mu \\, .\\]\nThe sequence \\(h_n\\) is non-decreasing. And \\(\\lim \\uparrow h_n(\\omega) = \\liminf f_n(\\omega)\\) for all \\(\\omega \\in \\Omega\\). For each \\(n\\), by the monotone convergence theorem (Theorem 3.1):\n\\[\\int \\lim_n \\uparrow h_n \\mathrm{d}\\mu = \\lim_n \\uparrow \\int h_n \\mathrm{d}\\mu\\]\nso that \\[\\int \\liminf_n f_n \\mathrm{d}\\mu = \\lim_n \\uparrow \\int h_n \\mathrm{d}\\mu\\]\nand \\[\\int \\liminf_n f_n \\mathrm{d}\\mu \\leq \\lim_n \\inf_{m\\geq n} \\int f_m \\mathrm{d}\\mu = \\liminf_{n} \\int f_n \\mathrm{d}\\mu\\]\n\n\n\n\nTheorem 3.3 (Dominated convergence theorem) Let \\((\\Omega, \\mathcal{F}, \\mu)\\) be a measured space. Let \\((f_n)_n\\) be a sequence of measurable functions that converges pointwise towards function \\(f\\). Assume that there exists a integrable function \\(g\\) that dominates \\((f_n)_n\\): for all \\(n\\), all \\(\\omega \\in \\Omega\\), \\(|f_n(\\omega)|\\leq g(\\omega)\\). Then \\(f\\) is integrable and\n\\[\\int f \\mathrm{d}\\mu = \\int \\lim_n  f_n \\mathrm{d}\\mu =  \\lim_n  \\int f_n \\mathrm{d}\\mu.\\]\n\n\n\nProof. Let us first check that \\(f\\) is integrable.\nObserve that \\(\\lim_n |f_n| = |f|\\) and thus \\(\\liminf |f_n| = |f|\\).\nBy Theorem 3.2,\n\\[\n\\int |f| \\mathrm{d}\\mu\n= \\int \\liminf_n |f_n| \\mathrm{d}\\mu\n\\leq \\liminf_n \\int |f_n| \\mathrm{d}\\mu\n= \\int |g| \\mathrm{d}\\mu &lt; \\infty \\,.\n\\]\nNow define \\(h_n = \\inf_{m\\geq n} f_m\\) and \\(j_n = \\sup_{m \\geq n}f_m\\). We have \\(\\lim_n \\uparrow h_n = f\\) and \\(\\lim_n \\downarrow j_n=f.\\)\nNote also that\n\\[\\int h_n \\mathrm{d}\\mu \\leq \\int f \\mathrm{d}\\mu \\leq \\int j_n \\mathrm{d}\\mu \\, .\\]\nBy monotone convergence \\(\\int h_n \\mathrm{d}\\mu \\uparrow \\int f\\mathrm{d}\\mu\\) and \\(\\int j_n \\mathrm{d}\\mu \\downarrow \\int f\\mathrm{d}\\mu\\). This entails \\(\\lim \\int f_n \\mathrm{d}\\mu\\).\n\n\n\n\n\n\nLet \\(g: \\Omega \\times \\mathbb{R} \\to \\mathbb{R}\\) be a function of two variables such that for each \\(t \\in \\mathbb{R}\\), \\(g(\\cdot, t)\\) is measurable. Assume that for each \\(t \\in \\mathbb{R}\\), \\(g(\\cdot, t)\\) is \\(\\mu\\)-integrable and that for each \\(\\omega \\in \\Omega\\), \\(g(\\omega, \\cdot)\\) is differentiable. Define \\(G(t)= \\int_{\\Omega} g(\\omega, t) \\mathrm{d}\\mu(\\omega)\\).\nIs it always true that \\(G\\) is differentiable at every \\(t\\)?\nProvide sufficient conditions for \\(G\\) to be differentiable and\n\\[G'(t) = \\int \\frac{\\partial g}{\\partial s}(\\omega, s)_{|s=t} \\mathrm{d}\\mu(\\omega) \\, .\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-densities",
    "href": "031-moments.html#sec-densities",
    "title": "3  A modicum of integration",
    "section": "3.5 Probability distributions defined by a density",
    "text": "3.5 Probability distributions defined by a density\n\nLet \\((\\Omega, \\mathcal{F})\\) be a measurable space and \\(\\mu\\) be a \\(\\sigma\\)-finite measure over \\((\\Omega, \\mathcal{F})\\). Let \\(f\\) be a non-negative measurable real function over \\((\\Omega, \\mathcal{F})\\).\nLet \\(\\nu : \\mathcal{F} \\to [0, \\infty)\\) be defined by\n\\[\\nu(A) = \\int \\mathbb{I}_A f \\, \\mathrm{d}\\mu = \\int_A f\\, \\mathrm{d}\\mu \\,.\\]\n\\(\\nu\\) is a measure over \\((\\Omega, \\mathcal{F})\\). The function \\(f\\) is said to be a density of \\(\\nu\\) with respect to \\(\\mu\\).\n\n\n\n\n\nProof. The fact that \\(\\nu(\\emptyset)=0\\) is immediate.\nThe fact that \\(\\nu\\) is \\(\\sigma\\)-additive follows from the monotone convergence theorem ( Theorem 3.1).\nIf \\(A_1, \\ldots, A_n, \\ldots\\) is a collection or pairwise disjoint measurable sets,\n\\[\n\\begin{array}{rl}\n\\nu(\\cup_n A_n)\n& = \\int \\mathbb{I}_{\\cup_n A} f \\, \\mathrm{d}\\mu \\\\\n& = \\int \\Big(\\lim_n \\sum_{k\\leq n}\\mathbb{I}_{A_k}\\Big) f \\, \\mathrm{d}\\mu  \\\\\n& = \\int \\Big(\\lim_n \\sum_{k\\leq n}\\mathbb{I}_{A_k} f \\Big)  \\, \\mathrm{d}\\mu \\\\\n& = \\lim_n  \\sum_{k\\leq n} \\int \\mathbb{I}_{A_k} f   \\, \\mathrm{d}\\mu \\\\\n& = \\lim_n  \\sum_{k\\leq n} \\int \\mathbb{I}_{A_k} f   \\, \\mathrm{d}\\mu \\\\\n& = \\lim_n  \\sum_{k\\leq n} \\nu(A_k) \\\\\n& = \\sum_{k=1}^\\infty \\nu(A_k) \\, .\n\\end{array}\n\\]\nThe fourth equality is justified by the monotone convergence theorem, others equalities follow from the fact that we are handling non-negative series.\n\n\n\n\nLet \\((A_n)_n\\) be such that \\(A_n \\in \\mathcal{F}, \\mu(A_n)&lt;\\infty\\) for each \\(n\\) and \\(\\cup_n A_n = \\Omega\\). For each \\(n\\), we have \\(\\nu(A_n) = \\int_{A_n}  f \\,\\mathrm{d}\\mu \\leq  \\int_{\\Omega}  f \\,\\mathrm{d}\\mu &lt;\n\\infty\\). This proves that if \\(\\mu\\) is \\(\\sigma\\)-finite, so is \\(\\nu\\).\n\n\n\n\nCheck that if \\(\\mu(A)=0\\), then \\(\\nu(A)=0\\) for every \\(A \\in \\mathcal{F}\\).\n\n\n\n\n\n\n\nTODO.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-expectation",
    "href": "031-moments.html#sec-expectation",
    "title": "3  A modicum of integration",
    "section": "3.6 Expectation",
    "text": "3.6 Expectation\nThe expectation of a real random variable is a (Lebesgue) integral with respect to a probability measure. We have to get familiar with probabilistic notation.\n\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space. The random variable \\(X\\) defined on \\((\\Omega, \\mathcal{F})\\) is \\(P\\)-integrable if the measurable function \\(|X|: \\omega \\mapsto |X(\\omega)|\\) is \\(P\\)-integrable: we agree on\n\\[\n\\mathbb{E} X = \\mathbb{E}_P X = \\int_{\\mathcal{X}} X(\\omega) \\mathrm{d}P(\\omega) =\\int X \\mathrm{d}P \\, .\n\\]\n\n\n\n\n\nCheck the consistency of this definition with the definition used in the discrete setting.\n\n\n\nThe next statement called the transfer formula can be used to compute the density of an image distribution or to simplify the computation of an expectation.\n\nTheorem 3.4 (Transfer formula) Let \\((\\mathcal{X}, \\mathcal{F}, P)\\) be a probability space, \\((\\mathcal{Y}, \\mathcal{G})\\) a measurable space, \\(f\\) a measurable function from \\((\\mathcal{X}, \\mathcal{F})\\) to \\((\\mathcal{Y}, \\mathcal{G})\\). Let \\(Q\\) denote the probability distribution that is the image of \\(P\\) by \\(f\\): \\(Q = P \\circ f^{-1}\\).\nThen for all measurable functions \\(h\\) from \\((\\mathcal{Y}, \\mathcal{G})\\) to \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\)\n\\[\n\\mathbb{E}[h(Y)] = \\int_{\\mathcal{Y}} h(y) \\mathrm{d}Q(y)  = \\int_{\\mathcal{X}} h\\circ f(x) \\mathrm{d}P(x) = \\mathbb{E} h\\circ f(X) \\,\n\\]\nif either integral is defined.\n\n\n\n\nProof. Assume first that \\(h= \\mathbb{I}_B\\) where \\(C \\in \\mathcal{G}\\). Then\n\\[\n\\begin{array}{rl}\n\\mathbb{E} h(Y)\n& = \\int_{\\mathcal{Y}} \\mathbb{I}_B(y) \\, \\mathrm{d}Q(y) \\\\\n& = Q(B) \\\\\n& = P \\circ f^{-1}(B) \\\\\n& = P \\Big\\{ x : f(x) \\in B \\Big\\} \\\\\n& = P \\Big\\{ x : h \\circ f(x) =1 \\Big\\}  \\\\\n& = \\int_{\\mathcal{X}}  h \\circ f(x) \\mathrm{d}P(x) \\\\\n& = \\mathbb{E} h\\circ f(X) \\, .\n\\end{array}\n\\]\nThen, by linearity, the transfer formula holds for all simple functions from \\(\\mathcal{Y}\\) to \\(\\mathbb{R}\\). By the definition of the Lebesgue integral, the transfer formula holds for non-negative measurable functions. The usual decomposition argument completes the proof.\n\n\n\nIt is clear that the expectation of a random variable only depends on the probability distribution of the random variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-jensensec",
    "href": "031-moments.html#sec-jensensec",
    "title": "3  A modicum of integration",
    "section": "3.7 Jensen’s inequality",
    "text": "3.7 Jensen’s inequality\nThe tools from integration theory we have reviewed so far serve to compute or approximate integrals and expectations. The next theorem circumvents computations and allows us to compare expectations.\nJensen’s inequality is a workhorse of Information Theory, Statistics and large parts of Probability Theory. It embodies the interaction between convexity and expectation.\nWe first introduce a modicum of convexity theory and notation.\n\nDefinition 3.4 (Lower semi-continuity) A function \\(f\\) from some metric space \\(\\mathcal{X}\\) to \\(\\mathbb{R}\\) is lower semi-continuous at \\(x \\in \\mathcal{X}\\), if\n\\[\\liminf_{x_n \\to x} f(x_n) \\geq f(x) \\, .\\]\n\n\nA continuous function is lower semi-continuous. But the converse is not true. If \\(A \\subseteq \\mathcal{X}\\) is an open set, then \\(\\mathbb{I}_A\\) is lower semi-continuous but, unless it is constant, it is not continuous at the boundary of \\(A.\\)\n\n\n\nDefinition 3.5 (Convex subset) Let \\(\\mathcal{X}\\) be a vector space, a subset \\(C \\subseteq \\mathcal{X}\\) is said to be convex if for all \\(x,y \\in C\\), all \\(\\lambda \\in [0,1]\\):\n\\[\\lambda x + (1-\\lambda) y \\in C \\, .\\]\n\n\n\nLet \\(C\\) be a convex subset of some (topological real) vector space, let \\(\\overline{C}\\) be the closure of \\(C\\). Prove that \\(\\overline{C}\\) and \\(\\overline{C} \\setminus C\\) are convex.\nA convex set may be neither closed nor open. Provide examples.\n\n\nIn the next definition, we consider functions from some vector space to \\(\\mathbb{R} \\cup \\{+\\infty\\}\\).\n\nDefinition 3.6 (Convex functions) Let \\(\\mathcal{X}\\) be a (topological) vector space. Let \\(C \\subseteq \\mathcal{X}\\) be a convex subset. A function \\(f\\) from \\(\\mathcal{C}\\) to \\(\\mathbb{R} \\cup \\{\\infty\\}\\) is convex if for \\(x,y \\in C\\), all \\(\\lambda \\in [0,1]\\),\n\\[f(\\lambda x + (1-\\lambda) y) \\leq \\lambda f(x) + (1-\\lambda) f(y) \\, .\\]\nThe domain of \\(f\\) \\(\\operatorname{Dom}(f)\\) is the subset of \\(C\\) where \\(f\\) is finite.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: The function \\(f : x \\mapsto \\mathbb{I}_{x&lt;0}|x| + \\mathbb{I}_{x\\geq 0} x^2\\) is convex, continuous. It is differentiable everywhere except at \\(x=0\\). The dotted lines define affine functions that are below the cruve \\(y=f(x)\\). The dotted lines define supporting hyperplanes for the epigraph of \\(f\\).\n\n\n\n\n\n\n\nCheck that a convex function \\(f\\) is lower semi-continuous iff sets \\(\\{ x : f(x) \\leq t\\}\\) are closed intervals for all \\(t \\in \\mathbb{R}\\).\n\nThe next result warrants that any convex lower semi-continuous has a dual representation. This dual representation is a precious tool when comparing expectation of random variables.\n\n\nTheorem 3.5 (Fenchel-Legendre duality) Let \\(f\\) be a convex lower-semi-continuous function on \\(\\mathbb{R}\\) with a closed domain.\nThe dual function \\(f^*\\) of \\(f\\) is defined over \\(\\mathbb{R}\\) by\n\\[f^*(y) = \\sup_{x \\in \\text{Dom}(f)} xy - f(x) \\, .\\]\nThen\n\n\\(f^*\\) is convex\n\\(f^*\\) is lower-semi-continuous\nIf \\(f^*(y)= xy - f(x)\\) then \\(y\\) is a sub-gradient of \\(f\\) at \\(x\\).\nIf \\(y\\) is a sub-gradient of \\(f\\) at \\(x\\), \\(f^*(y) = xy -f(x)\\).\n\\(f= (f^{*})^*\\), the dual function of the dual function equals the original function: \\(f(x) = \\sup_{y} xy -f^*(y).\\)\n\n\n\n\nThe next dual pairs will be used in several places.\n\nif \\(f(x) = \\frac{|x|^p}{p}\\) (\\(p&gt; 1\\)), then \\(f^*(y)= \\frac{|y|^q}{q}\\) where \\(q=p/(p-1)\\).\nif \\(f(x) = |x|\\), then \\(f^*(y)= 0\\) for \\(y \\in [-1,1]\\) and \\(\\infty\\) for \\(|y|&gt;1\\).\nif \\(f(x) = \\exp(x)\\) then \\(f^*(y) = y \\log y - y\\) for \\(y&gt;0\\), \\(f^*(y)=\\infty\\) for \\(y&lt;0\\)\n\n\n\n\nProof. The fact that \\(f^*\\) is \\(\\mathbb{R} \\cup \\{\\infty\\}\\)-valued and convex is immediate.\nTo check lower semi-continuity, assume \\(y_n \\to y\\), with \\(y_n \\in \\operatorname{Dom}(f^*)\\) and \\(f^*(y) &gt; \\liminf_n f^*(y_n)\\).\nAssume first that \\(y \\in \\operatorname{Dom}(f^*)\\). Then for some sufficiently large \\(m\\) and some \\(x \\in \\operatorname{Dom}(f)\\)\n\\[\nf^*(y) \\geq xy - f(x) -\\frac{1}{m} &gt; \\liminf_n f^*(y_n) \\geq \\liminf_n y_n x -f(x) = yx -f(x)\n\\]\nwhich is contradictory.\nAssume now that \\(y \\not\\in \\operatorname{Dom}(f^*)\\) and \\(\\liminf_n f^*(y_n) &lt; \\infty\\). Extract a subsequence \\((y_{m_n})_n\\) such that \\(\\lim_n f^*(y_{m_n}) = \\liminf_n f^*(y_n)\\). There exists \\(x \\in \\operatorname{Dom}(f)\\) such that \\[\nf^*(y) &gt; xy -f(x) &gt; \\liminf_n f^*(y_n) = \\lim_n f^*(y_{m_n}) \\geq \\lim_n xy_{m_n} -f (x) = xy - f(x)\n\\]\nwhich is again contradictory.\nThe fact that \\(y\\) is a sub-gradient of \\(f\\) at \\(x\\) if \\(f^*(y)= xy - f(x)\\) is a rephrasing of the definition of sub-gradients.\nNote that if \\(x \\in \\operatorname{Dom}(f)\\) and \\(y\\in \\operatorname{Dom}(f^*)\\) then \\(f(x)+f^*(y)\\geq xy\\).\nThis observation entails that \\((f^*)^*(x)\\leq f(x)\\) for all \\(x \\in \\operatorname{Dom}(f)\\). If there existed some \\(x \\in \\operatorname{Dom}(f)\\) with \\((f^*)^*(x)&gt;x\\), there would exist some \\(y \\in \\operatorname{Dom}(f^*)\\) with \\(xy - f^*(y) &gt; f(x)\\) which is not possible.\nIn order to prove that that \\((f^*)^*(x)\\geq f(x)\\) for all \\(x \\in \\operatorname{Dom}(f)\\), we rely on the convexity, lower semi-continuity of \\(f\\) and \\(f^*\\) and the closure of \\(\\operatorname{Dom}(f)\\). Under these conditions, every point \\(x\\) in \\(\\operatorname{Dom}(f)\\) has a sub-gradient \\(y\\) and this entails \\(f(x) + f^*(y)= xy\\).\n\n\n\nExtend the notion of Fenchel-Legendre duality to lower-semi-continuous convex functions over \\(\\mathbb{R}^k\\).\n\n\n\nAre all convex functions lower-semi-continuous? measurable?\nAre all convex lower-semi-continuous functions measurable?\n\n\n\nIt is possible to define \\(f^*\\) as \\(f^*(y) =\\sup_x xy -f(x)\\) even if \\(f\\) is not convex and lower semi-continuous. The function \\(f^*\\) retains the convexity and lower semi-continuity properties. But \\(f \\neq (f^{*})^*\\), we only get \\(f \\geq (f^{*})^*\\). Indeed \\((f^{*})^*\\) is the largest convex minorant of \\(f\\).\n\n\n\nTheorem 3.6 (Jensen’s inequality) Let \\(X\\) be a real-valued random variable and \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be convex, lower-semi-continuous such that the closed set \\(\\text{Dom}(f) \\subseteq \\text{supp}(\\mathcal{L}(X))\\) and \\(\\mathbb{E} |f(X)|&lt; \\infty.\\), then\n\\[f(\\mathbb{E} X) \\leq \\mathbb{E} f(X) \\, .\\]\n\n\n\nIn view of the definition of convexity and of the fact that taking expectation extends the idea of taking a convex combination, Jensen’s inequality is not a surprise.\n\n\n\nProof. \\[\n\\begin{array}{rl}\n\\mathbb{E} f(X) & = \\mathbb{E} (f^*)^*(X) \\\\\n& = \\mathbb{E} \\Big[ \\sup_y \\Big( yX - f^*(y)\\Big)\\Big] \\\\\n& \\geq   \\sup_y  \\Big( y \\mathbb{E} X - f^*(y)\\Big) \\\\\n& =  (f^*)^*\\Big( \\mathbb{E} X \\Big) \\\\\n& = f\\Big( \\mathbb{E} X \\Big)  \\, .\n\\end{array}\n\\]\n\n\n\n\n\n\nIn the argument above, it is not a priori obvious that \\(\\sup_y \\Big( yX - f^*(y)\\Big)\\) is measurable, since the supremum is taken over a non-countable collection. Check that this is not an issue.\n\nWe will see many applications of Jensen’s inequality:\n\ncomparison of sampling with replacement with sampling without replacement (comparison of binomial and hypergeometric tails)\nCauchy-Schwarz and Hölder’s inequalities\nDerivation of maximal inequalities\nNon-negativity of relative entropy\nDerivation of Efron-Stein-Steele’s inequalities\n…",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-variance",
    "href": "031-moments.html#sec-variance",
    "title": "3  A modicum of integration",
    "section": "3.8 Variance",
    "text": "3.8 Variance\nThe variance (when it is defined) is an index of dispersion of the distribution of a random variable.\n\nProposition 3.6 (Characterizations of variance) Let \\(X\\) be a random variable over some probability space. The variance of \\(X\\) is finite iff \\(\\mathbb{E}X^2 &lt;\\infty\\) and it may be defined using the netx three equalities:\n\\[\n\\begin{array}{rl}\n\\operatorname{var}(X) & =  \\mathbb{E}\\left[(X - \\mathbb{E}X)^2\\right] \\\\\n& = \\inf_{a \\in \\mathbb{R}} \\mathbb{E}\\left[(X - a)^2\\right] \\\\\n& = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 \\,.\n\\end{array}\n\\]\n\nWe need to check that three right-hand-side are finite if one of them is, and that when they are finite, they are all equal.\n\nProof. Assume \\(\\mathbb{E}X^2 &lt; \\infty\\), as \\(|X| \\leq \\frac{X^2}{2} + \\frac{1}{2}\\), this entails \\(\\mathbb{E} |X|&lt;\\infty\\). If \\(\\mathbb{E}X^2 &lt; \\infty\\) then so is \\(\\mathbb{E}|X|\\). The right-hand-side on the third line is finite if \\(\\mathbb{E}X^2 &lt; \\infty\\). As \\((x-b)^2 \\leq 2 x^2 + 2 b^2\\) for all \\(x,b\\), The right-hand-side on the first line, the infimum on the second line are finite when \\(\\mathbb{E} X^2 &lt;\\infty.\\)\nAs \\(X^2 \\leq 2 (X- \\mathbb{E}X)^2 + 2 (\\mathbb{E}X)^2\\), \\(\\mathbb{E}X^2&lt;\\infty\\) if \\(\\mathbb{E}\\left[(X - \\mathbb{E}X)^2\\right] &lt;\\infty.\\)\nAssume now that \\(\\mathbb{E}X^2 &lt; \\infty\\). \\[\n\\begin{array}{rl}\n  \\mathbb{E}\\left[(X - a)^2\\right]\n  & = \\mathbb{E}\\left[(X - \\mathbb{E}X - (a-\\mathbb{E}X))^2\\right] \\\\\n  & = \\mathbb{E}\\left[(X- \\mathbb{E}X)^2 - 2 \\mathbb{E}[(X-\\mathbb{E}X)](a-\\mathbb{E}X) + (a-\\mathbb{E}X)^2 \\right]\\\\\n  & = \\mathbb{E}\\left[(X- \\mathbb{E}X)^2\\right] + (a-\\mathbb{E}X)^2   \\, .\n\\end{array}\n\\]\nAs \\((a- \\mathbb{E}X)^2\\geq 0\\), we have established that \\(\\mathbb{E}\\left[(X - \\mathbb{E}X)^2\\right]   = \\inf_{a \\in \\mathbb{R}} \\mathbb{E}\\left[(X - a)^2\\right]\\). Moreover, the infimum is a minimum, it is achieved at a single point \\(\\mathbb{E}X\\).\n\n\n\n\n\nThe first and second characterizations of variance assert that the expectation minimizes the average quadratic error. A fact of great importance in Statistics.\n\n\n\n\n\nCheck that if \\(P\\left\\{ X \\in [a,b]\\right\\} =1\\), then \\(\\operatorname{var}(X)\\leq \\frac{(b-a)^2}{4}\\) , .",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-highermoments",
    "href": "031-moments.html#sec-highermoments",
    "title": "3  A modicum of integration",
    "section": "3.9 Higher moments",
    "text": "3.9 Higher moments\nIn this Section we relate \\(\\mathbb{E} |X|^p\\) with \\(\\mathbb{E} |X|^q\\) for different values of \\(p, q \\in \\mathbb{R}_+\\). Our starting point is small technical result in real analysis.\n\nProposition 3.7 (Young’s inequality) Let \\(p, q&gt;1\\) be conjugate (\\(1/p + 1/q =1\\)), and \\(x, y&gt;0\\), then \\[xy \\leq \\frac{x^p}{p} + \\frac{y^q}{q} \\,.\\]\n\n\nProof. Note that if \\(p\\) and \\(q\\) are conjugate, then \\(q= p/(p-1)\\) and \\((p-1)(q-1)=1\\).\nIt suffices to check that for all \\(x,y&gt;0\\),\n\\[\\frac{x^p}{p} \\geq xy - \\frac{y^q}{q} \\, .\\]\nFix \\(x&gt;0\\), consider the function over \\([0,\\infty)\\) defined by\n\\[z \\mapsto xz - \\frac{z^q}{q} \\,.\\]\nThis function is differentiable with derivative \\(x - z^{q-1} = x - z^{1/(p-1)}\\). It achieves its maximum at \\(z=x^{p-1}\\) and the maximum is equal to\n\\[x x^{p-1} - \\frac{x^{q(p-1)}}{q} = x^p - \\frac{x^p}{q} = \\frac{x^p}{p} \\, .\\]\n\n\n\n\nFigure 3.3 displays a graphic proof of Young’s inequality.\n\n\n\n\n\n\n\n\n\nFigure 3.3: Graphical illlustration of Young’s inequality. We choose \\(p=`r p`\\) and \\(q= `r q`\\), \\(x = `r x`\\) and \\(y= `r y`\\). The black point is located at \\((x,y)^T\\). The product \\(xy\\) equals the area of the rectangle located between the origin and \\((x,y)^T\\) (delimited by the dashed segments). The dotted line represents function \\(s \\mapsto s^{p-1}\\), and interchanging the axes, the function \\(t \\mapsto t^{q-1} = t^{1/(p-1)}\\). The area of the light grey surface under the dotted line equals \\(\\frac{x^p}{p} = \\int_0^x s^{p-1} \\mathrm{d}s\\), while the area of the darker grey surface below line \\(y=1\\) and above the dotted line, equals \\(\\frac{y^q}{q} = \\int_0^y t^{q-1} \\mathrm{d}t\\). The union of the two disjoint surfaces covers the rectangle located between the origin and \\((x,y)^\\top\\). Equality occurs when the dotted line passes though \\((x,y)^\\top\\), that is when \\(y=x^{p-1}\\).\n\n\n\n\n\n\n\n\n\n\nA special case of Young inequality is obtained by taking \\(p=q=2\\).\n\nWe are now in a position to prove three fundamental inequalities: Cauchy-Schwarz, Hölder and Minkowski.\n\nTheorem 3.7 (Cauchy-Schwarz) Let \\(X\\) and \\(Y\\) be two random variables on the same probability space. Assume both \\(\\mathbb{E}X^2\\) and \\(\\mathbb{E}Y^2\\) are finite. Then\n\\[\\mathbb{E} [XY] \\leq  \\sqrt{\\mathbb{E}X^2} \\times  \\sqrt{\\mathbb{E}Y^2} \\,.\\]\n\n\nProof. If either \\(\\sqrt{\\mathbb{E}X^2}=0\\) or \\(\\sqrt{\\mathbb{E}Y^2}=0\\), the inequality is trivially satisfied.\nSo, without loss of generality, assume \\(\\sqrt{\\mathbb{E}X^2}&gt;0\\) and \\(\\sqrt{\\mathbb{E}Y^2}&gt;0\\). Then, because \\(ab \\leq a^2/2 + b^2/2\\), for all real \\(a,b\\), everywhere,\n\\[\n\\frac{|XY|}{\\sqrt{\\mathbb{E}X^2}\\sqrt{\\mathbb{E}Y^2}}\n\\leq \\frac{|X|^2}{2\\mathbb{E}X^2} + \\frac{|Y|^2}{2\\mathbb{E}Y^2} \\,.\n\\]\nTaking expectation on both sides leads to the desired result.\n\n\n\n\n\nWhy is the inequality trivially satisfied if \\(\\sqrt{\\mathbb{E}X^2}=0\\)?\n\n\n\n\nTheorem 3.7 tells us that if \\(X\\) and \\(Y\\) are square-integrable, then \\(XY\\) is integrable.\n\n\n\nHölder’s inequality generalizes Cauchy-Schwarz inequality. Indeed, Cauchy-Schwarz inequality is just Hölder’s inequality for \\(p=q=2\\) (\\(2\\) is its own conjugate).\n\nTheorem 3.8 (Hölder’s inequality) Let \\(X\\) and \\(Y\\) be two random variables on the same probability space. Let \\(p, q&gt;1\\) be conjugate (\\(1/p + 1/q =1\\)), assume both \\(\\mathbb{E}|X|^p\\) and \\(\\mathbb{E}|Y|^q\\) are finite. Then we have\n\\[\n\\mathbb{E} [XY] \\leq  \\left(\\mathbb{E}|X|^p\\right)^{1/p} \\times  \\left(\\mathbb{E}|Y|^q\\right)^{1/q} \\,.\n\\]\n\n\nProof. If either \\(\\mathbb{E}|X|^p=0\\) or \\(\\mathbb{E}|Y|^q=0\\), the inequality is trivially satisfied.\nAssume that \\(\\mathbb{E}|X|^p &gt; 0\\) and \\(\\mathbb{E}|Y|^q &gt; 0\\).\nFollow the proof of Cauchy-Schwarz inequality, but replace \\(2 ab \\leq a^2 +b^2\\) by Young’s inequality:\n\\[ab \\leq \\frac{|a|^p}{p} + \\frac{|b|^q}{q}\\qquad  \\forall a,b \\in \\mathbb{R}\\] if \\(1/p+ 1/q=1\\).\nThe inequality below is a consequence of Young’s inequality and of the monotonicity of expectation:\n\\[\n\\begin{array}{rl}\n\\frac{\\mathbb{E}|XY|}{\\mathbb{E}[|X|^p]^{1/p}\\mathbb{E}[|Y|^q]^{1/q}}\n& =  \\mathbb{E}\\Big[\\frac{|X|}{\\mathbb{E}[|X|^p]^{1/p}} \\frac{|Y|}{\\mathbb{E}[|Y|^q]^{1/q}} \\Big] \\\\\n& \\leq \\mathbb{E}\\Big[\\frac{|X|^p}{p \\mathbb{E}[|X|^p]} + \\frac{|Y|^q}{q \\mathbb{E}[|Y|^q]} \\\\\n& = \\frac{1}{p} + \\frac{1}{q} \\\\\n& = 1 \\, .\n\\end{array}\n\\]\n\n\n\n\n\nFor \\(1\\leq p &lt; q\\), \\[\\mathbb{E}\\Big[|X|^p\\Big]^{1/p} \\leq \\mathbb{E}\\Big[|X|^q\\Big]^{1/q} \\, .\\]\n\nFor \\(p \\in [0, \\infty)\\) \\(X \\mapsto (\\mathbb{E}|X|^p)^{1/p}\\) defines a semi-norm on the set of random variables for which \\((\\mathbb{E}|X|^p)^{1/p}\\) is finite. Minkowski’s inequality asserts that \\(X \\mapsto (\\mathbb{E}|X|^p)^{1/p}\\) satisfies the triangle inequality.\n\nTheorem 3.9 (Minkowski’s inequality) Let \\(X, Y\\) be two real-valued random variables defined on the same probability space. Let \\(p \\in [1, \\infty)\\). Assume that \\(\\mathbb{E}|X|^p &lt;\\infty\\) and \\(\\mathbb{E}|Y|^p&lt;\\infty.\\) Then we have:\n\\[\n\\left(\\mathbb{E} [| X + Y|^p]\\right)^{1/p}\n\\leq \\left(\\mathbb{E} [| X|^p]\\right)^{1/p} + \\left(\\mathbb{E} [|Y|^p]\\right)^{1/p}\n\\]\nwhich entails \\(\\mathbb{E}|X+Y|^p &lt;\\infty.\\)\n\nThe proof of Theorem 3.9 follows from Hölder’s inequality (Theorem 3.8).\n\nProof. The inequality below also follows from triangle inequality on \\(\\mathbb{R}\\), monotonicity. The last equality follows from linearity of expectation:\n\\[\n\\begin{array}{rl}\n\\mathbb{E} \\Big[ |X+Y|^p\\Big]\n& \\leq \\mathbb{E} \\Big[ (|X|+|Y|) \\times |X+Y|^{p-1}\\Big] \\\\\n& = \\mathbb{E} \\Big[ |X| \\times |X+Y|^{p-1}\\Big] + \\mathbb{E} \\Big[ |Y| \\times |X+Y|^{p-1}\\Big] \\, .\n\\end{array}\n\\]\nThis is enough to handle the case \\(p=1\\).\nFrom now on, assume \\(p&gt;1\\). Hölder’s inequality entails the next inequality and a similar upper bound for \\(\\mathbb{E} \\Big[ |Y| \\times |X+Y|^{p-1}\\Big]\\).\n\\[\n\\begin{array}{rl}\n\\mathbb{E} \\Big[ |X| \\times |X+Y|^{p-1}\\Big]\n& \\leq \\mathbb{E} \\Big[ |X|^p\\Big]^{1/p} \\times  \\mathbb{E} \\Big[ |X+Y|^{p}\\Big]^{(p-1)/p} \\,\n\\end{array}\n\\]\nSumming the two upper bounds, we obtain\n\\[\n\\begin{array}{rl}\n  \\mathbb{E} \\Big[ |X+Y|^p\\Big]\n  & \\leq \\left(\\mathbb{E} \\Big[ |X|^p\\Big]^{1/p} + \\mathbb{E} \\Big[ |Y|^p\\Big]^{1/p}\\right) \\times \\mathbb{E} \\Big[ |X+Y|^{p}\\Big]^{(p-1)/p} \\, .\n\\end{array}\n\\]\nThis prove’s Minkowski’s inequality for \\(p&gt;1\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#medianiqr",
    "href": "031-moments.html#medianiqr",
    "title": "3  A modicum of integration",
    "section": "3.10 Median and interquartile range",
    "text": "3.10 Median and interquartile range\nRobust and non-robust indices of location.\n\nLet \\(X\\) be a real random variable over some probability space. Let \\(F\\) be the cumulative distribution function of \\(X\\). The median of the distribution of \\(X\\) is \\(F^{\\leftarrow}(1/2)\\).\n\nThe median minimizes the mean absolute deviation.\n\nIf \\(m\\) is such that \\(P\\{ X &gt; m\\} = P\\{ X&lt;m\\}\\) then \\(m\\) is median of the distribution of \\(X\\), and if \\(X\\) is integrable:\n\\[\\mathbb{E}\\Big| X - m \\Big| = \\min_{a \\in \\mathbb{R}} \\mathbb{E}\\Big| X - a \\Big|\\]\n\n\n\n\n\nProof. Assume \\(a&lt;m\\),\n\\[\n\\begin{array}{rl}\n  \\mathbb{E} \\left[\\Big| X - a \\Big| - \\Big| X - m \\Big| \\right]\n  & = - (m-a) P(-\\infty, a] + \\int_{(a, m]} (2 X - (a+m)) \\mathrm{d}P(x) + (m-a)P(m,\\infty) \\\\\n  & \\geq  - (m-a) P(-\\infty, a] - (m-a) P(a,m] + (m-a)P(m,\\infty) \\\\\n  & = (m-a) \\Big(P(m,\\infty) - P(-\\infty, m]\\Big) \\\\\n  & = 0 \\, .\n\\end{array}\n\\]\nThe same line of reasoning allows to handle the case \\(a&gt;m\\) and to conclude.\n\n\n\n\nCombining three of the inequalities we have just proved, allows us to establish an interesting connection between expectation, median and standard deviation.\n\nTheorem 3.10 (Lévy’s inequality) Let \\(m\\) be the median of the distribution of \\(X\\), a square-integrable random variable over some probability space. Then\n\\[\\Big| m - \\mathbb{E} X\\Big| \\leq \\sqrt{\\operatorname{var}(X)} \\, .\\]\n\n\n\n\nThe robust and non-robust indices of location differ by at most the standard deviation, which may be infinite.\n\n\n\n\nProof. By convexity of \\(x \\mapsto |x|\\), we have\n\\[\n\\begin{array}{rl}\n\\Big| m - \\mathbb{E} X\\Big| & \\leq \\mathbb{E} \\Big| m - X\\Big| \\\\\n& \\text{by Jensen's inequality} \\\\\n& \\leq \\mathbb{E} \\Big| \\mathbb{E}X - X\\Big| \\\\\n& \\text{the median minimizes the mean absolute error} \\\\\n& \\leq \\left(\\mathbb{E} \\Big| \\mathbb{E}X - X\\Big|^2\\right)^{1/2}  \\\\\n& \\text{by Cauchy-Schwarz inequality.}\n\\end{array}\n\\]\n\n\n\nThe mean and the median may differ. First the median is always defined, while the mean may not. Think for example of the standard Cauchy distribution which has density \\(\\frac{1}{\\pi}\\frac{1}{1+x^2}\\) over \\(\\mathbb{R}\\). If \\(X\\) is Cauchy distributed, then \\(\\mathbb{E}|X|=\\infty\\). The mean is not defined. But as the density is a pair function, \\(X\\) is symmetric (\\(X\\) and \\(-X\\) are distributed the same way), and this implies that the median of (the distribution) of \\(X\\) is \\(0\\).\nConsider the exponential distribution with density \\(\\exp(-x)\\) over \\([0, \\infty)\\), it has mean \\(1\\), median \\(\\log(2)\\), and variance \\(1\\). If we turn to exponential distribution with density \\(\\lambda \\exp(-\\lambda x)\\), it has mean \\(1/\\lambda\\), median \\(\\log(2)/\\lambda\\), and variance \\(1/\\lambda^2\\). Lévy’s inequality does not tell more that what we can compute with bare hands.\nFinally consider Gamma distributions with shape parameter \\(p\\) and intensity parameter \\(\\lambda\\). It has mean \\(p/\\lambda\\), variance \\(p/\\lambda^2\\). The median is not easily computed though we can easily check that it is equal to \\(g(p)/\\lambda\\) where \\(g(p)\\) is the median of the Gamma distribution with parameters \\(p\\) and \\(1\\). Lévy’s inequality tells us that \\(|g(p) - p|\\leq \\sqrt{p}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#sec-lpspaces",
    "href": "031-moments.html#sec-lpspaces",
    "title": "3  A modicum of integration",
    "section": "3.11 \\(\\mathcal{L}_p\\) and \\(L_p\\) spaces",
    "text": "3.11 \\(\\mathcal{L}_p\\) and \\(L_p\\) spaces\nLet \\(p \\in [1, \\infty)\\). Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space. Define \\(\\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\) (often abbreviated to \\(\\mathcal{L}_p(P)\\) or even \\(\\mathcal{L}_p\\) when there is no ambiguity) as\n\\[\n\\mathcal{L}_p(\\Omega, \\mathcal{F}, P) = \\Big\\{ X : X \\text{ is a real random variable over } (\\Omega, \\mathcal{F}, P), \\quad \\mathbb{E}|X|^p &lt; \\infty \\Big\\} \\, .\n\\]\nLet \\(\\| X \\|_p\\) be defined by \\(\\| X\\|_p = \\Big(\\mathbb{E} |X|^p\\Big)^{1/p}\\).\nLet \\(\\mathcal{L}_0(\\Omega, \\mathcal{F}, P)\\) denote the vector space of random variables over \\((\\Omega, \\mathcal{F}, P)\\).\nWe first notice that sets \\(\\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\) form a nested sequence.\n\nProposition 3.8 Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space, then for \\(1 \\leq p \\leq q &lt;\\infty\\):\n\n\\(\\|X\\|_p &lt; \\| X\\|_q\\).\n\\(\\mathcal{L}_q(\\Omega, \\mathcal{F}, P) \\leq \\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\).\n\n\n\nProof. Assume \\(1 \\leq p \\leq q &lt;\\infty\\), as \\(x \\mapsto x^{q/p}\\) is convex on \\([0, \\infty)\\) by Jensen’s inequality (Theorem 3.6), we have\n\\[\n\\begin{array}{rl}\n  \\mathbb{E} [|X|^p]^{q/p} & \\leq \\mathbb{E} [|X|^q] \\,.\n\\end{array}\n\\]\nThis establishes 1.) And 2.) is an immediate consequence of \\(1\\).\n\n\nProposition 3.8 is a about inclusion of sets. The next theorem summarizes several points: that sets \\(\\mathcal{L}_p\\) are linear subspaces of \\(\\mathcal{L}_0\\), and that they are complete as pseudo-metric (pseudo-normed) spaces.\n\nFor \\(p \\in [1, \\infty)\\), let \\(\\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\) and \\(\\|\\cdot\\|_p\\) be defined as above. Then,\n\n\\(\\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\) is a linear subspace of the space of real random variables.\n\\(\\| \\cdot\\|_p\\) is a pseudo-norm on \\(\\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\).\nIf \\((X_n)_n\\) is a sequence in \\(\\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\) that satisfies \\[\\lim_n \\sup_{m\\geq n} \\Big| X_n - X_m \\Big|_p = 0\\] then there exists \\(X \\in \\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\) such that \\(\\lim_n \\| X_n - X\\|_p=0\\).\nThere exists a subsequence \\((X_{m_n})_{n}\\) such that \\(X_{m_n} \\to X\\) \\(P\\)-almost surely.\n\n\n\n\nIn a pseudo-metric space, to prove that a Cauchy sequence converges, it is enough to check convergence of a subsequence. Picking a convenient subsequence, and possibly relabeling elements, we may assume \\(\\Big\\| X_n - X_m \\Big\\|_p \\leq 2^{- n \\wedge m}\\) for all \\(n,m\\).\n\n\n\n\n3.11.1 First Borell-Cantelli Lemma\nLet \\((A_n)_n\\) be a sequence of events from some probability space \\((\\Omega, \\mathcal{F}, P)\\). Assume \\(\\sum_{n} P(A_n) &lt; \\infty\\) then, with probability \\(1\\), only finetely many events \\(A_n\\) are realized:\n\\[P \\left\\{ \\omega : \\sum_n \\mathbb{I}_{A_n}(\\omega) &lt; \\infty \\right\\} = 1 \\,.\\]\n\n\n\nProof. The event \\(\\left\\{ \\omega : \\sum_n \\mathbb{I}_{A_n}(\\omega) = \\infty \\right\\}\\) coincides with \\(\\cap_n \\cup_{m\\geq n} A_n\\):\n\\[\nP \\left\\{ \\sum_n \\mathbb{I}_{A_n}(\\omega) = \\infty\\right\\} = P(\\cap_n \\cup_{m\\geq n} A_n) \\, .\n\\]\nNow, the sequence \\((\\cup_{m\\geq n} A_n)_n\\) is monotone decreasing: \\(\\lim_n \\downarrow \\cup_{m\\geq n} A_n = \\cap_n \\cup_{m\\geq n} A_n \\,.\\)\nBy Fatou’s Lemma,\n\\[\n\\begin{array}{rl}\n\\mathbb{E} \\lim_m \\mathbb{I}_{\\cup_{m\\geq n} A_m}\n  & = \\mathbb{E} \\liminf_n\\mathbb{I}_{\\cup_{m\\geq n} A_m}  \\\\\n  & \\leq  \\liminf_n  \\mathbb{E} \\mathbb{I}_{\\cup_{m\\geq n} A_m} \\\\\n  & \\leq  \\liminf_n  \\sum_{m\\geq n} P(A_m) \\\\\n  & =   0 \\, .\n\\end{array}\n\\]\nThe last equation comes from the fact that the remainders of a convergent series are vanishing.\n\n\nProof. Points 1) and 2) follow from Minkowski’s inequality. This entails that \\(\\|\\cdot\\|_p\\) defines a pseudo-norm on \\(\\mathcal{L}_p\\). If two random variables \\(X,Y\\) from \\(\\mathcal{L}_p\\) satisfy \\(\\| X- Y\\|_p=0\\), then \\(X=Y\\) \\(P\\)-a.s.\nTo establish 3), we need to check that the sequence converges almost surely, and that an almost sure limit belongs to \\(\\mathcal{L}_p\\).\nDefine event \\(A_n\\) by \\[A_n = \\Big\\{ \\omega : \\Big| X_n(\\omega) - X_{n+1}(\\omega) \\Big| &gt; \\frac{1}{n^2}\\Big\\} \\, .\\] By Markov inequality, \\[P(A_n) \\leq \\mathbb{E}\\Big[n^{2p} \\Big| X_n - X_m \\Big|^p \\Big] \\leq n^{2p} 2^{-np} \\, .\\] Hence, \\(\\sum_{n\\geq 1} P(A_n) &lt; \\infty.\\) By the first Borel-Cantelli Lemma, on some event \\(E\\) with probability \\(1\\), only finitely many \\(A_n\\) are realized.\nIf \\(\\omega \\in E\\), the condition \\(\\Big| X_n(\\omega) - X_{n+1}(\\omega) \\Big| &gt; \\frac{1}{n^2}\\) is realized for only finitely many indices \\(n\\). Thus the real-valued sequence \\((X_n(\\omega))_n\\) is a Cauchy sequence. It has a limit we denote \\(X(\\omega)\\). If \\(\\omega \\not\\in E\\), we agree on \\(X(\\omega)=0.\\) On \\(\\Omega\\), we have \\[X(\\omega) = \\lim_n \\mathbb{I}_E(\\omega) X(\\omega) \\, .\\] A limit of random variables is a random variable. Hence \\(X\\) is a random variable.\nIt remains to check that \\(X \\in \\mathcal{L}_p\\). Note first that\n\\[\n\\Big| \\big\\| X_m \\big\\|_p - \\big\\|X_n \\big\\|_p \\Big| \\leq \\big\\| X_m - X_n \\big\\|_p \\,.\n\\]\nHence \\(\\big(\\big\\|X_n \\big\\|_p \\big)_n\\) is a Cauchy sequence and converges to some finite limit. As \\[\n|X(\\omega)|  \\leq  \\liminf |X_n(\\omega)|\n\\]\nby Fatou’s Lemma \\[\n\\mathbb{E} |X|^p  \\leq \\liminf \\mathbb{E} |X_n|^p &lt; \\infty\\, .\n\\]\nHence \\(X \\in \\mathcal{L}_p\\).\nFinally we check that \\(\\lim_m \\|X_n - X\\|_p =0\\). By Fatou’s lemma again,\n\\[\n\\mathbb{E} \\Big| X - X_m \\Big|^p \\leq \\liminf_n \\mathbb{E} \\Big| X_n - X_m \\Big|^p\n\\]\nHence\n\\[\n\\lim_m \\mathbb{E} \\Big| X - X_m \\Big|^p \\leq \\lim_m \\liminf_n \\mathbb{E} \\Big| X_n - X_m \\Big|^p = 0 \\, .\n\\]\n\n\n\n\n\nCan we extend the almost sure convergence to the whole sequence? This is not the case. Consider \\(([0,1], \\mathcal{B}([0,1]), P)\\) where \\(P\\) is the uniform distribution. For \\(k= j+ n(n-1)/2\\), \\(1\\leq j\\leq n\\), let \\(X_n = \\mathbb{I}_{[(j-1)/n, j/n]}\\). The sequence \\(X_n\\) converges to \\(0\\) in \\(\\mathcal{L}_p\\) for all \\(p \\in [1, \\infty)\\). Indeed \\(\\|X_k\\|_p = n^{-p}\\) for \\(k= j+ n(n-1)/2\\), \\(1\\leq j\\leq n\\). For any \\(\\omega \\in [0,1]\\), the sequence \\(X_n(\\omega)\\) oscillates between \\(0\\) and \\(1\\) infinitely many times.\n\n\n\\(\\mathcal{L}_p\\) provide us with a bridge between probability and analysis. In analysis, the fact that \\(\\|\\cdot \\|_p\\) is just a pseudo-norm leads to consider \\(L_p\\) spaces. \\(L_p\\) spaces are defined from \\(\\mathcal{L}_p\\) spaces by taking equivalence classes of random variables. Indeed, define relation \\(\\equiv\\) over \\(\\mathcal{L}_p(\\Omega, \\mathcal{F}, P)\\) by \\(X \\equiv X'\\) iff \\(P\\{X=X'\\}=1\\). This relation is an equivalence relation (reflexive, symmetric and transitive). If \\(X \\equiv X'\\) and \\(Y \\equiv Y'\\), then \\(\\|X -Y\\|_p = \\|X' -Y\\|_p = \\|X' - Y'\\|_p\\). \\(L_p(\\Omega, \\mathcal{F}, P)\\) is the quotient space of \\(\\mathcal{L}_p\\) by relation \\(\\equiv\\). We have the fundamental result.\n\nTheorem 3.11 For \\(p \\in [1, \\infty)\\), \\(L_p(\\Omega, \\mathcal{F}, P)\\) equiped with \\(\\| \\cdot\\|_p\\) is a complete normed space (Banach space).\n\n\nThis eventually allows us to invoke theorems from functional analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "031-moments.html#bibmoments",
    "href": "031-moments.html#bibmoments",
    "title": "3  A modicum of integration",
    "section": "3.12 Bibliographic remarks",
    "text": "3.12 Bibliographic remarks\nDudley (2002) gives a self-contained and thorough treatment of measure and integration theory with probability theory in mind.\nHiriart-Urruty & Lemaréchal (1993) is an excellent and accessible reference on convexity.\n\n\n\n\nDudley, R. M. (2002). Real analysis and probability (Vol. 74, p. x+555). Cambridge: Cambridge University Press.\n\n\nHiriart-Urruty, J.-B., & Lemaréchal, C. (1993). Convex analysis and minimization algorithms. I (Vol. 305, p. xviii+417). Springer-Verlag, Berlin.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A modicum of integration</span>"
    ]
  },
  {
    "objectID": "03-families.html",
    "href": "03-families.html",
    "title": "4  Families of discrete distributions",
    "section": "",
    "text": "4.1 Bernoulli and Binomial\nThe goal of this lesson is getting acquainted with important families of distributions and to get familiar with distributional calculus. Probability distributions will be presented through distribution functions, probability mass functions (discrete distribution), densities (when available).\nA Bernoulli distribution is completely defined by its parameter.\nAssume \\(\\Omega^{\\prime} = \\{0,1\\}^n\\).\nThe connexion between Bernoulli and Binomial distributions is obvious: a Bernoulli distribution is a Binomial distribution with size parameter equal to \\(1\\). This connexion goes further: the sum of independent Bernoulli random variables with same success parameter is Binomial distributed.\nThe expected value of a Bernoulli distribution with parameter \\(p\\) is \\(p\\)! Its variance is \\(p(1-p)\\).\nBy linearity of expectation, the expected value of the binomial distribution with parameters \\(n\\) and \\(p\\) is \\(np\\). The variance of a sum of independent random variables is the sum of the variances, hence the variance of he binomial distribution with parameters \\(n\\) and \\(p\\) is \\(np(1-p)\\).\nFigure 4.1: Binomial probability mass functions with \\(n=20\\) and different values of \\(p\\) : \\(.5, .7, .2\\).\nMore on wikipedia.\nBinomial distributions with the same success parameter",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Families of discrete distributions</span>"
    ]
  },
  {
    "objectID": "03-families.html#sec-bernoulli",
    "href": "03-families.html#sec-bernoulli",
    "title": "4  Families of discrete distributions",
    "section": "",
    "text": "Definition 4.1 A Bernoulli distribution is a probability distribution \\(P\\) on \\(\\Omega=\\{0,1 \\}\\). The parameter of \\(P\\) is \\(P\\{1\\} \\in [0,1]\\).\n\n\n\n\nDefinition 4.2 (Binomial distribution) A binomial distribution with parameters \\(n \\in \\mathbb{N}, p \\in [0,1]\\) (\\(n\\) is size and \\(p\\) is success) is a probability distribution \\(P\\) on \\(\\Omega = \\{0, 1, 2, \\ldots, n\\}\\), defined by \\[P\\{k\\} = \\binom{n}{k} p^k (1-p)^k\\]\n\n\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be independent, identically distributed Bernoulli random variables with success parameter \\(p \\in [0,1]\\), then \\(Y = \\sum_{i=1}^n X_i\\) is distributed according to a Binomial disctribution with size parameter \\(n\\) and success probability \\(p\\).\n\n\nProof. For \\(k \\in 0, \\ldots, n\\)\n\\[\\begin{align*}\nP\\Big\\{ \\sum_{i=1}^n  X_i = k \\Big\\}\n  & = \\sum_{x_1, \\ldots, x_n \\in \\{0,1 \\}^p} \\mathbb{I}_{\\sum_{i=1}^n x_i=k} P \\Big\\{ \\wedge_{i=1}^n X_i = x_i\\Big\\} \\\\\n  & = \\sum_{x_1, \\ldots, x_n \\in \\{0,1 \\}^p} \\mathbb{I}_{\\sum_{i=1}^n x_i=k} \\prod_{i=1}^n P \\Big\\{ X_i = x_i\\Big\\} \\\\\n  & = \\sum_{x_1, \\ldots, x_n \\in \\{0,1 \\}^p} \\mathbb{I}_{\\sum_{i=1}^n x_i=k} \\prod_{i=1}^n  p^{x_i} (1-p)^{1-x_i} \\\\\n  & = \\sum_{x_1, \\ldots, x_n \\in \\{0,1 \\}^p} \\mathbb{I}_{\\sum_{i=1}^n x_i=k}\\,   p^{k} (1-p)^{n-k} \\\\\n  & = \\binom{n}{k} p^{k} (1-p)^{n-k}  \\, .\n\\end{align*}\\]\n\n This observation facilitates the computation of moments of Binomial distribution.\n\n\n\n\n\n\n\n\nLet \\(X,Y\\) be independent over probability space \\((\\Omega, \\mathcal{F}, P)\\) and distributed according to \\(\\text{Bin}(n_1, p)\\) and \\(\\text{Bin}(n_2, p)\\).\nThen \\(X+Y\\) is distributed according to \\(\\text{Bin}(n_1+n_2, p)\\).\n\n\nCheck the preceding proposition.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Families of discrete distributions</span>"
    ]
  },
  {
    "objectID": "03-families.html#sec-poisson",
    "href": "03-families.html#sec-poisson",
    "title": "4  Families of discrete distributions",
    "section": "4.2 Poisson",
    "text": "4.2 Poisson\nThe Poisson distribution appears as a limit of Binomial distributions in a variety of circumstances connected to rare events phenomena.\n\nDefinition 4.3 A Poisson distribution with parameter \\(\\lambda &gt;0\\) is a probability distribution \\(P\\) on \\(\\Omega=\\mathbb{N}\\) with\n\\[P\\{k\\} = \\mathrm{e}^{-\\lambda} \\frac{\\lambda^k}{k!}\\]\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: Poisson probability mass functions with different values of parameter: \\(1, 5, 10\\). Recall that the parameter of a Poisson distribution equals its expectation and its variance. The probability mass function of a Poisson distribution achieves its maximum (called the mode) close to its expectation.\n\n\n\n\n\n\nThe expected value of the Poisson distribution with paramenter \\(\\lambda\\) is \\(\\lambda\\). The variance of a Poisson distribution is equal to its expected value.\n\\[\\begin{align*}\n\\mathbb{E} X\n& = \\sum_{n=0}^\\infty \\mathrm{e}^{-\\lambda} \\frac{\\lambda^n}{n!} \\times n\\\\\n& = \\lambda \\times \\sum_{n=1}^\\infty \\mathrm{e}^{-\\lambda} \\frac{\\lambda^{n-1}}{(n-1)!}  \\\\\n& = \\lambda \\, .\n\\end{align*}\\]\n\nLet \\(X,Y\\) be independent and Poisson distributed over probability space \\((\\Omega, \\mathcal{F}, P)\\), then \\(X+Y\\) is Poisson distributed.\n\n\n\n\n\nProof. We check the proposition in the simplest and most tedious way. We compute the probability mass function of the distribution of \\(X+Y\\). Assume \\(X \\sim \\operatorname{Po}(\\lambda), Y \\sim \\operatorname{Po}(\\mu)\\).\nFor each \\(k \\in \\mathbb{N}\\): \\[\\begin{align*}\n\\Pr \\{ X+Y =k\\}\n& = \\Pr \\{ \\bigvee_{m=0}^k (X =m \\wedge Y =k-m) \\} \\\\\n& = \\sum_{m=0}^k \\Pr \\{ X =m \\wedge Y =k-m \\} \\\\\n& = \\sum_{m=0}^k \\Pr \\{ X =m \\} \\times \\Pr\\{ Y =k-m \\} \\\\\n& = \\sum_{m=0}^k \\mathrm{e}^{-\\lambda} \\frac{\\lambda^m}{m!} \\mathrm{e}^{-\\mu} \\frac{\\mu^{k-m}}{(k-m)!} \\\\\n& = \\mathrm{e}^{-\\lambda - \\mu}  \\frac{(\\lambda+\\mu)^k}{k!} \\sum_{m=0}^k \\frac{k!}{m! (k-m)!}\\left(\\frac{\\lambda}{\\lambda+\\mu}\\right)^m \\left(\\frac{\\mu}{\\lambda+\\mu}\\right)^{k-m} \\\\\n& = \\mathrm{e}^{-\\lambda - \\mu}  \\frac{(\\lambda+\\mu)^k}{k!} \\sum_{m=0}^k \\binom{k}{m}\\left(\\frac{\\lambda}{\\lambda+\\mu}\\right)^m \\left(\\frac{\\mu}{\\lambda+\\mu}\\right)^{k-m} \\\\\n& = \\mathrm{e}^{-\\lambda - \\mu}  \\frac{(\\lambda+\\mu)^k}{k!} \\left( \\frac{\\lambda}{\\lambda+\\mu} + \\frac{\\mu}{\\lambda+\\mu}\\right)^k  \\\\\n& = \\mathrm{e}^{-(\\lambda + \\mu)}  \\frac{(\\lambda+\\mu)^k}{k!}\n\\end{align*}\\] The last expression if the pmf of \\(\\operatorname{Po}(\\lambda + \\mu)\\) at \\(k\\).\n\n\n\n\n\nCheck that the mode (maximum) of a Poisson probability mass function with parameter \\(\\lambda\\) is achieved at \\(k= \\lfloor \\lambda \\rfloor\\). It is always unique?\n\n\nCheck that the median of a Poisson distribution with integer parameter \\(\\lambda\\) is not smaller than \\(\\lambda\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Families of discrete distributions</span>"
    ]
  },
  {
    "objectID": "03-families.html#sec-geometric",
    "href": "03-families.html#sec-geometric",
    "title": "4  Families of discrete distributions",
    "section": "4.3 Geometric",
    "text": "4.3 Geometric\nA geometric distribution is a probability distribution over \\(\\mathbb{N} \\subset \\{0,1\\}\\). It depends on a parameter \\(p&gt;0\\).\nAssume we are allowed to toss a biased coin infinitely many times. The number of times we have to toss the coin until we get a head is geometrically distributed.\nLet \\(X\\) be distributed according to a geometric distribution with parameter \\(p\\). The geometric probability distribution is easily defined by its tail function. In the event \\(X&gt;k\\), the first \\(k\\) outcomes have to be tail. \\[\nP \\{ X &gt; k \\} = (1-p)^k\n\\] The probability mass function of the geometric distribution follows: \\[\nP \\{X = k \\} = (1-p)^{k-1} - (1-p)^k = p \\times (1-p)^{k-1} \\qquad \\text{for } k=1, 2, \\ldots\n\\] On average, we have to toss the coin \\(p\\) times until we get a head: \\[\n\\mathbb{E}X = \\sum_{k=0}^\\infty P \\{ X &gt; k \\} = \\frac{1}{p}\n\\]\n\nIt is also possible to define geometric random variables as the number of times we have to toss the coin before we get a head. This requires modifying quantile function, probability mass function, expectation, and so on. This is the convention R uses.\n\n\n\n\n\n\nGeometric probability mass functions with different values of parameter \\(p\\): \\(1/2, 1/3, 1/5\\). The probability mass function equals \\(p \\times (1-p)^{k-1}\\) at \\(k\\geq 1\\). The mode is achieved at \\(k=1\\) whatever the value of \\(p\\). The expectation equals \\(1/p\\)\n\n\n\n\nSums of independent geometric random variables are not distributed according to a geometric distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Families of discrete distributions</span>"
    ]
  },
  {
    "objectID": "022-product-measures.html",
    "href": "022-product-measures.html",
    "title": "5  Product distributions",
    "section": "",
    "text": "5.1 Product \\(\\sigma\\)-algebras\nIn this lesson, we construct product measures. We start with two measured spaces \\((\\mathcal{X}, \\mathcal{F}, \\mu)\\) and \\((\\mathcal{Y}, \\mathcal{G}, \\nu)\\).\nOur goal is to build a measure space \\((\\mathcal{X}\\times \\mathcal{Y}, \\mathcal{H}, \\rho)\\) and two measurable functions \\(X : \\mathcal{X}\\times \\mathcal{Y} \\to \\mathcal{X}\\) and \\(Y : \\mathcal{X}\\times \\mathcal{Y} \\to \\mathcal{Y}\\) with the additional requirements that\n\\[\\mu = \\rho \\circ X^{-1} \\quad \\text{and} \\quad \\nu = \\rho \\circ Y^{-1}\\]\nas well as\n\\[\\rho(A \\times B) =  \\mu(A) \\times \\nu(B) \\qquad \\forall A \\in \\mathcal{F}, B\\in \\mathcal{G} \\,.\\]\nNote that requiring \\(X\\) and \\(Y\\) to be measurable is prematurate: we have not defined the \\(\\sigma\\)-algebra \\(\\mathcal{H}\\) over \\(\\mathcal{X}\\times \\mathcal{Y}\\).\nIn order to achieve our goal, we first define a \\(\\sigma\\)-algebra \\(\\mathcal{H}\\) of subsets of \\(\\mathcal{X} \\times \\mathcal{Y}\\). We use the so-called product \\(\\sigma\\)-algebra.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Product distributions</span>"
    ]
  },
  {
    "objectID": "022-product-measures.html#sec-algebras",
    "href": "022-product-measures.html#sec-algebras",
    "title": "5  Product distributions",
    "section": "",
    "text": "Definition 5.1 (Product \\(\\sigma\\)-algebra) Let \\((\\mathcal{X}, \\mathcal{F})\\) and \\((\\mathcal{Y}, \\mathcal{G})\\) be two measurable spaces, the product \\(\\sigma\\)-algebra \\(\\mathcal{F} \\otimes \\mathcal{G}\\) is the \\(\\sigma\\)-algebra of subsets of \\(2^{\\mathcal{X} \\times \\mathcal{Y}}\\) that is generated by the so-called rectangles:\n\\[\\Big\\{ A \\times B : A \\in \\mathcal{F}, B \\in \\mathcal{G}\\Big\\} \\, .\\]\nIn words, \\[\\mathcal{F} \\otimes \\mathcal{G} = \\sigma\\left(\\mathcal{F} \\times \\mathcal{G}\\right)\\]\n\n\nProposition 5.1 The product \\(\\sigma\\)-algebra makes the functions \\(X\\) and \\(Y\\) (sometimes called coordinate projections) measurable.\n\n\nExercise 5.1 Check Proposition 5.1.\n\n\nExercise 5.2 Check that the Borel \\(\\sigma\\)-algebra over \\(\\mathbb{R}^2\\) can be described as a product \\(\\sigma\\)-algebra (the \\(\\sigma\\)-algebra generated by the cartesian product of \\(\\mathcal{B}(\\mathbb{R})\\) with itself).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Product distributions</span>"
    ]
  },
  {
    "objectID": "022-product-measures.html#sec-productmeasures",
    "href": "022-product-measures.html#sec-productmeasures",
    "title": "5  Product distributions",
    "section": "5.2 Product measures",
    "text": "5.2 Product measures\nOnce we are equipped with the product \\(\\sigma\\)-algebras, we can proceed to the definition of product measures.\nRecall the definition of \\(\\sigma\\)-finite measures from Section 2.6).\nA measure \\(\\mu\\) on \\((\\Omega, \\mathcal{F})\\) is \\(\\sigma\\)-finite iff there exists \\((A_n)_n\\) with \\(\\Omega \\subseteq \\cup_n A_n\\) and \\(\\mu(A_n) &lt; \\infty\\) for each \\(n\\).\nFinite measures (this encompasses probability measures) are \\(\\sigma\\)-finite. Lebesgue measure is \\(\\sigma\\)-finite. The counting measure on \\(\\mathbb{R}\\) is not \\(\\sigma\\)-finite.\n\nTheorem 5.1 Let \\((\\mathcal{X}, \\mathcal{F}, \\mu)\\) and \\((\\mathcal{Y}, \\mathcal{G}, \\nu)\\) be two measured spaces where \\(\\mu,\\nu\\) are \\(\\sigma\\)-finite.\nThen there exists a unique \\(\\sigma\\)-finite measure \\(\\alpha\\) on \\(\\mathcal{X} \\times \\mathcal{Y}\\) endowed with the product \\(\\sigma\\)-algebra \\(\\mathcal{F} \\otimes \\mathcal{G} = \\sigma(\\mathcal{F} \\times \\mathcal{G})\\) that satisfies\n\\[\\alpha (A \\times B) = \\mu(A) \\times \\nu(B)\\qquad \\forall A \\in \\mathcal{F}, B \\in \\mathcal{G} \\, .\\]\nMoreover, for all \\(E \\in \\mathcal{F} \\otimes \\mathcal{G}\\),\n\nfor each \\(x \\in \\mathcal{X}\\), \\(y \\mapsto \\mathbb{I}_E(x,y)\\) is \\(\\mathcal{G}\\)-measurable;\n\\(x \\mapsto \\int_{\\mathcal{Y}} \\mathbb{I}_E(x,y) \\, \\mathrm{d} \\nu(y)\\) is \\(\\mathcal{F}\\)-measurable;\nfor each \\(y \\in \\mathcal{Y}\\), \\(x \\mapsto \\mathbb{I}_E(x,y)\\) is \\(\\mathcal{F}\\)-measurable;\n\\(y \\mapsto \\int_{\\mathcal{X}}  \\mathbb{I}_E(x,y) \\,  \\mathrm{d}\\mu(x)\\) is \\(\\mathcal{G}\\)-measurable,\n\nand the following holds:\n\\[\\begin{align*}\n\\int_{\\mathcal{X}\\times \\mathcal{Y}} \\mathbb{I}_E \\, \\mathrm{d}\\alpha\n& = \\int_{\\mathcal{X}} \\Big(\\int_{\\mathcal{Y}} \\mathbb{I}_E(x,y) \\, \\mathrm{d} \\nu(y)\\Big) \\, \\mathrm{d}\\mu(x) \\\\\n& = \\int_{\\mathcal{Y}} \\Big( \\int_{\\mathcal{X}}  \\mathbb{I}_E(x,y) \\,  \\mathrm{d}\\mu(x)\\Big) \\,\\mathrm{d} \\nu(y)\n\\end{align*}\\]\nwhere the three integrals are either finite or infinite.\nMeasure \\(\\alpha\\) is called a product measure, it is sometimes denoted by \\(\\mu \\otimes \\nu\\).\n\n\n\n\n\nRemark 5.1. Assuming that both \\(\\mu\\) and \\(\\nu\\) are \\(\\sigma\\)-finite is essential. Choose \\(\\mu\\) as the counting measure on \\([0,1]\\) and \\(\\nu\\) as the Lebesgue measure on \\([0,1]\\). Consider the diagonal \\(E = \\{(x,x) : x \\in [0,1]\\}\\). The set \\(E\\) belongs to \\(\\mathcal{B}(\\mathbb{R}) \\otimes \\mathcal{B}(\\mathbb{R}) = \\mathcal{B}(\\mathbb{R}^2)\\) (check this). But interchanging the order of integration leads to different results:\n\\[\\begin{array}{rl}\n1      & = \\int_{[0,1]} \\Big(\\int_{[0,1]} \\mathbb{I}_E (x,y) \\, \\mathrm{d}\\mu(x)\\Big) \\, \\mathrm{d}\\nu(y) \\\\\n0      & = \\int_{[0,1]} \\Big(\\int_{[0,1]} \\mathbb{I}_E (x,y) \\, \\mathrm{d}\\nu(y)\\Big) \\, \\mathrm{d}\\mu(x)\n\\end{array}\\]\n\n\n\n\nTheorem 5.1 contains three statements:\n\nexistence of a measure over \\((\\mathcal{X} \\times \\mathcal{Y}, \\mathcal{F} \\otimes \\mathcal{G})\\) that satisfies the product property over rectangles;\nuniqueness of this measure;\nthe possibility of computing the measure of \\(E \\in \\mathcal{F} \\otimes \\mathcal{G}\\) by iterated integration in arbitrary order.\n\nThe first statement (existence) is proved using an extension theorem, the second statement (unicity) follows from a monotone class argument (Theorem 2.4)): rectangles form a generating \\(\\pi\\)-class, so the case where both \\(\\mu\\) and \\(\\nu\\) are finite measure is settled. If either \\(\\mu\\) or \\(\\nu\\) is just \\(\\sigma\\)-finite, consider restrictions to rectangles with finite measure, and proceed by approximation. The third statement trivially holds for rectangles.\n\n\nRemark 5.2. If \\(\\mu, \\nu\\) are probability measures, then the product measure \\(\\mu \\otimes \\nu\\) is a probability measures, it is called a product probability measure.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Product distributions</span>"
    ]
  },
  {
    "objectID": "022-product-measures.html#tonellifubini",
    "href": "022-product-measures.html#tonellifubini",
    "title": "5  Product distributions",
    "section": "5.3 Tonelli-Fubini theorem",
    "text": "5.3 Tonelli-Fubini theorem\nIn this section, we consider product measures that are built from \\(\\sigma\\)-finite measures as in Theorem 5.1). The Tonelli-Fubini Theorem shows that (under mild conditions) integration with respect to a product measure reduces to iterated integration over the component measures.\n\nLet be \\(( \\mathcal{X}, \\mathcal{A})\\) and \\((\\mathcal{Y}, \\mathcal{B})\\) two measurable spaces, \\(\\mu\\) and \\(\\nu\\) two \\(\\sigma\\)-finite measures on these spaces, \\(\\mu \\otimes \\nu\\) the product measure, and \\(f\\) a \\(\\mathcal{A} \\otimes \\mathcal{B}\\)-measurable real function such as \\(\\int |f| \\mathrm{d} \\mu \\otimes \\nu &lt; 0\\). The the following properties are satisfied:\n\n\\(\\forall x \\in \\mathcal{X}, \\hspace{1em} y \\mapsto f (x, y)\\) is \\(\\mathcal{B}\\)-measurable.\nThe function \\(x \\mapsto \\int_{\\mathcal{Y}} f (x, y) \\mathrm{d}\\nu(y)\\) is \\(\\mathcal{A}\\)-measurable, finite \\(\\mu\\)- almost everywhere and \\[\\int_{\\mathcal{X} \\times \\mathcal{Y}} f \\mathrm{d} \\mu \\otimes \\nu\n= \\int_{\\mathcal{X}} \\left[ \\int_{\\mathcal{Y}} f (x, y) \\mathrm{d} \\nu (y)\n\\right]  \\mathrm{d} \\mu (x)\\]\n\n\n\n\n\n\nProof. Proof can \\(\\square\\)\n\nThe following characterization of the expectation of non-negative random variables as the integral of the tail function is a simple consequence of the Tonelli-Fubini Theorem.\n\n\n\n\nProposition 5.2 (IPP formula) Let \\(X\\) be a non-negative real-valued random variable, then\n\\[\\mathbb{E}X = \\int_0^\\infty  P\\{ X &gt; t \\} \\mathrm{d}t\\]\n\n\n\n\n\nProof. \\[\\begin{array}{rl}\n\\mathbb{E}X\n  & = \\int_{\\Omega}  X(\\omega) \\, \\mathrm{d}P(\\omega) \\\\\n  & = \\int_{\\Omega}  \\Big( \\int_{[0,\\infty)} \\mathbb{I}_{X(\\omega)&gt; t} \\mathrm{d}t \\Big)\\, \\mathrm{d}P(\\omega) \\\\\n  & =  \\int_{[0,\\infty)}  \\Big( \\int_{\\Omega} \\mathbb{I}_{X(\\omega)&gt; t} \\,  \\mathrm{d}P(\\omega) \\Big) \\mathrm{d}t \\\\\n  & =  \\int_{[0,\\infty)}  \\Big( P\\{ \\omega : X(\\omega) &gt; t \\} \\Big) \\mathrm{d}t\n\\end{array}\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Product distributions</span>"
    ]
  },
  {
    "objectID": "022-product-measures.html#sec-indepproduct",
    "href": "022-product-measures.html#sec-indepproduct",
    "title": "5  Product distributions",
    "section": "5.4 Joint distributions, independence and product distributions",
    "text": "5.4 Joint distributions, independence and product distributions\nLet the two random variables \\(X, Y\\) map \\((\\Omega, \\mathcal{F})\\) to \\((\\mathcal{X}, \\mathcal{G})\\) and \\((\\mathcal{Y}, \\mathcal{H})\\). Equip \\((\\Omega, \\mathcal{F})\\) with probability distribution \\(P\\). Let \\(Q_X = P \\circ X^{-1}\\) and \\(Q_Y =  P \\circ Y^{-1}\\) be the two image distributions (called the marginal distributions). We may define a mapping \\(Z: \\Omega \\to \\mathcal{X} \\times \\mathcal{Y}\\) by \\(Z(\\omega) = (X(\\omega), Y(\\omega))\\), this mapping is \\(\\mathcal{F}/\\sigma(\\mathcal{G}\\times \\mathcal{H})\\) mesurable.\nLet \\(Q\\) be the joint distribution of \\(Z = (X,Y)\\) under \\(P\\), that is the probability distribution over \\(\\mathcal{X} \\times \\mathcal{Y}\\) endowed with \\(\\sigma(\\mathcal{G}\\times \\mathcal{H})\\) that is uniquely defined by\n\\[Q( A \\times B) = P\\Big\\{ \\omega: X(\\omega) \\in A, Y(\\omega) \\in B \\Big\\} \\, .\\]\nNote that \\(Q\\) is not necessarily a product distribution.\nThe next (trivial) proposition tells us that two random variables are independent iff their joint distribution is a product distribution (in fact the product dist)\n\n\\[X \\perp\\!\\!\\!\\perp Y \\text{ under } P \\Longleftrightarrow  Q = Q_X \\otimes Q_Y \\, ,\\]\n\nin words, \\(X\\) and \\(Y\\) are independent iff their joint distribution is the product of their marginal distributions.\n\nProof. TODO: FIX THIS",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Product distributions</span>"
    ]
  },
  {
    "objectID": "022-product-measures.html#sec-indepcollectionsalgebras",
    "href": "022-product-measures.html#sec-indepcollectionsalgebras",
    "title": "5  Product distributions",
    "section": "5.5 Independence of collections of \\(\\sigma\\)-algebras",
    "text": "5.5 Independence of collections of \\(\\sigma\\)-algebras\n\n\n\nIn many applications, independence between two \\(\\sigma\\)-algebras or a finite collection of \\(\\sigma\\)-algebras is not enough. This is the case when deriving or using laws of large numbers. We have to deal with a countable collection of independent random variables. In words, we have to work with a countable collection of \\(\\sigma\\)-algebras and we need to elaborate a notion of a countable collection of independent \\(\\sigma\\)-algebras.\n\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space. Let \\(\\mathcal{G_1}, \\ldots, \\mathcal{G}_n, \\ldots\\) be a countable colletion of sub-\\(\\sigma\\)-algebras.\nThe collection \\(\\mathcal{G_1}, \\ldots, \\mathcal{G}_n, \\ldots\\) is said to be independent under \\(P\\)\nif\nevery finite sub-collection is independent under \\(P\\).\n\n\nConsider the uniform probability distribution over \\([0,1]\\), define \\(X_1, X_2, \\ldots\\) by\n\\[X_n(\\omega) = \\operatorname{sign}\\Big(\\sin\\big(2^{n+1} \\pi \\omega \\big)\\Big)\\]\nthen \\(X_1, \\ldots, X_n, \\ldots\\) form a countable independent collection of random variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Product distributions</span>"
    ]
  },
  {
    "objectID": "022-product-measures.html#infproductspaces",
    "href": "022-product-measures.html#infproductspaces",
    "title": "5  Product distributions",
    "section": "5.6 Infinite product spaces",
    "text": "5.6 Infinite product spaces\nIn many modeling scenarios (random walks, branching processes, asymptotic statistics, …), we rely on the availability of an infinite collection of independent random variables. While it is (relatively) easy to come up with the notion of finite product probability spaces, the notion of infinite product probability spaces is more puzzling. And this remains true even if the individual components are finite probability spaces.\nThinks of \\(\\Omega_i = \\{0,1\\}\\) and each \\(P_i\\) has the balanced Bernoulli distribution. Let \\(\\omega\\) be an infinite sequence of \\(o\\) and \\(1\\), \\(\\{\\omega\\} = \\prod_{i=1}^\\infty \\{ \\omega_i \\}\\) is an infinite Cartesian product of events with probability \\(1/2\\). What should be its probability in the infinite product probability space? Is there a way to assign probabilities in a consistent way? If the answer is positive, is there a unique way to perform this operation?\n\nLet \\((\\Omega_n, \\mathcal{F}_n)_n\\) be a countable collection of measurable spaces, the cylinder \\(\\sigma\\)-algebra is the \\(\\sigma\\)-algebra of subsets of \\(\\prod_{n=1}^\\infty \\Omega_n\\) that is generated by subsets of the form:\n\\[\\prod_{n=1}^m A_n \\times \\prod_{n=m+1}^\\infty \\Omega_n \\qquad\\text{with } A_n \\in \\mathcal{F}_n \\text{ for } n \\leq m\\]\nwhere \\(m\\) is any integer. The subsets are called finite-dimensional rectangles or cylinders.\n\nObserve that cylinders form a \\(\\pi\\)-class.\nIf each \\((\\Omega_n, \\mathcal{F}_n)\\) is endowed with a probability distribution, assigning a probability to cylinders looks straightforward: \\[\\mathbb{P} \\left( \\prod_{n=1}^m A_n \\times \\prod_{n=m+1}^\\infty \\Omega_n \\right) = \\prod_{n=1}^m P_n(A_n) \\times \\prod_{n=m+1}^\\infty P_n(\\Omega_n) =\n\\prod_{n=1}^m P_n(A_n) \\, .\\]\nThe question is: does \\(\\mathbb{P}\\) extends to the cylinder \\(\\sigma\\)-algebra? If an extension exists, is it unique? The answer is yes.\n\nLet \\((\\Omega_n, \\mathcal{F}_n, P_n)_n\\) be a countable collection of probability spaces. Then there exists a unique probability distribution \\(\\mathbb{P}\\) on the cylindrical \\(\\sigma\\)-algebra that satisfy:\n\\[\\mathbb{P} \\left( \\prod_{n=1}^m A_n \\times \\prod_{n=m+1}^\\infty \\Omega_n \\right) =  \\prod_{n=1}^m P_n(A_n)\\]\nfor every finite sequence \\(A_1, \\ldots, A_m\\) in \\(\\mathcal{F}_1 \\times \\ldots \\times \\mathcal{F}_m\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Product distributions</span>"
    ]
  },
  {
    "objectID": "022-product-measures.html#bibproduct",
    "href": "022-product-measures.html#bibproduct",
    "title": "5  Product distributions",
    "section": "5.7 Bibliographic remarks",
    "text": "5.7 Bibliographic remarks\nThis material covering this lesson can be found in any book on measure and integration theory. Section 4.4 from (Dudley, 2002) is dedicated to product measures.\nComplete proofs of the Tonelli-Fubini Theeorem can be found in (Dudley, 2002).\nThe existence theorem for infinite product probabilities is from Section 8.2 from (Dudley, 2002). A full proof of the Theorem can be found there.\n\n\n\n\nDudley, R. M. (2002). Real analysis and probability (Vol. 74, p. x+555). Cambridge: Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Product distributions</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html",
    "href": "032-acfamilies.html",
    "title": "6  Absolutely continuous probability measures",
    "section": "",
    "text": "6.1 Densities and absolute continuity\nBeyond discrete distributions, the simplest probability distributions are defined by a density function with respect to a (\\(\\sigma\\)-finite) measure. This encompasses the distributions of the so-called continuous random variables.\nIf \\(\\mu, \\nu\\) are two probability distributions, and \\(\\mu \\trianglelefteq \\nu\\), then any event which is impossible under \\(\\nu\\) is also impossible under \\(\\mu\\).\nThe next theorem has far-reaching practical consequences.\nThe density is also called the Radon-Nikodym derivative of \\(\\mu\\) with respect to \\(\\nu\\). It is sometimes denoted by \\(\\frac{\\mathrm{d}\\mu}{\\mathrm{d}\\nu}\\).\nIn the next sections, we investigate probability distributions over \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\) that are absolutely continuous with respect to Lebesgue measure.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html#sec-acontinuity",
    "href": "032-acfamilies.html#sec-acontinuity",
    "title": "6  Absolutely continuous probability measures",
    "section": "",
    "text": "Definition 6.1 (Absolute continuity) Let \\(\\mu, \\nu\\) be two positive measures on measurable space \\((\\Omega, \\mathcal{F})\\), \\(\\mu\\) is said to be absolutely continuous with respect to \\(\\nu\\) (denoted by \\(\\mu \\trianglelefteq \\nu\\)) iff for every \\(A \\in \\mathcal{F}\\) with \\(\\nu(A)=0\\), we also have \\(\\mu(A)=0\\).\n\n\n\nExercise 6.1 Answer the two questions:\n\nIs the counting measure on \\(\\mathbb{R}\\) absolutely continuous with respect to Lebesgue measure?\nIs the converse true?\n\n\n\nExercise 6.2 Check that absolute continuity is a transitive relationship.\n\n\n\nTheorem 6.1 (Radon-Nikodym) Let \\(\\mu, \\nu\\) be two positive measures on measurable space \\((\\Omega, \\mathcal{F})\\). Assume \\(\\nu\\) is \\(\\sigma\\)-finite. If \\(\\mu \\trianglelefteq \\nu\\), then there exists a measurable function \\(f\\) from \\(\\Omega\\) to \\([0, \\infty)\\) such that for all \\(A \\in \\mathcal{F}\\), \\[\n\\mu(A) =  \\int_A f(\\omega) \\mathrm{d}\\nu(\\omega) =  \\int \\mathbb{I}_A f \\mathrm{d}\\nu \\, .\n\\] The function \\(f\\) is called a version of the density of \\(\\mu\\) with respect to \\(\\nu\\).\n\n\n\nRemark 6.1. The \\(\\sigma\\)-finiteness assumption is crucial. If we choose \\(\\mu\\) as Lebesgue measure and \\(\\nu\\) as the counting measure, \\(\\nu\\) is not \\(\\sigma\\)-finite, \\(\\mu(A)&gt;0\\) implies \\(\\nu(A)=\\infty\\) which we may consider as larger than \\(0\\). Nevertheless, Lebesgue measure has no density with respect to the counting measure.\n\n\n\nProposition 6.1 If \\(\\rho \\trianglelefteq \\mu \\trianglelefteq  \\nu\\), \\(f\\) is a density of \\(\\rho\\) with repsect to \\(\\mu\\) while \\(g\\) is a density of \\(\\mu\\) with respect to \\(\\nu\\), then \\(fg\\) is a density of \\(\\rho\\) with respect to \\(\\nu\\).\n\n\nExercise 6.3 Prove proposition 6.1.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html#expos",
    "href": "032-acfamilies.html#expos",
    "title": "6  Absolutely continuous probability measures",
    "section": "6.2 Exponential distribution",
    "text": "6.2 Exponential distribution\nThe exponential distribution shows up in several areas of probability and statistics. In reliability theory, its memoryless property make it a borderline case. In the theory of point processes, the exponential distribution is connected with Poisson Point Processes. It is also important in extreme value theory.\n\nThe exponential distribution with intensity parameter \\(\\lambda&gt;0\\) is defined by its desnsity with respect to Lebesgue measure on \\([0,\\infty)\\): \\[\nx \\mapsto \\lambda \\mathrm{e}^{-\\lambda x} \\, .\n\\] The reciprocal of the intensity parameter is called the scale parameter.\n\nNote that geometric and exponential distributions are connected: if \\(X\\) is exponentially distributed, then \\(\\lceil X\\rceil\\) is geometrically distributed. For \\(k\\geq 1\\): \\[\nP \\Big\\{ \\lceil X \\rceil \\geq k \\Big\\} = P \\Big\\{  X  &gt; k - 1 \\Big\\}\n= \\mathrm{e}^{- \\lambda (k-1)} \\, .\n\\]\n\nExercise 6.4 Check that \\(x \\mapsto \\lambda \\mathrm{e}^{-\\lambda x}\\) is a density probability over \\([0, \\infty)\\).\n\n\nExercise 6.5 Compute the tail function and the cumulative distribution function of the exponential distribution function with parameter \\(\\lambda\\).\n\n\nExercise 6.6 Let \\(X_1, \\ldots, X_n\\) be i.i.d. exponentially distributed. Characterize the distribution of \\(\\min(X_1, \\ldots, X_n)\\).\n\n\nIf \\(X\\) is exponentially distributed with scale parameter \\(\\sigma\\), what is the distribution of \\(a X\\)?\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Exponential densities with different parameters: scales \\(1, 2, 1/2\\) or equivalently intensities \\(1, 1/2, 2\\). Expectation equals scale,\nvariance equals squared scale.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html#sec-gammas",
    "href": "032-acfamilies.html#sec-gammas",
    "title": "6  Absolutely continuous probability measures",
    "section": "6.3 Gamma distribution",
    "text": "6.3 Gamma distribution\nSums of independent exponentially distributed random variables are not exponentially distributed. The family of Gamma distributions encompasses the family of exponential distributions. It is stable under addition and satisfies\nRecall Euler’s Gamma function: \\[\n\\Gamma(t) = \\int_0^\\infty x^{t-1}\\mathrm{e}^{-x} \\mathrm{d}x \\qquad \\text{for } t&gt;0\\, .\n\\]\n\nThe Gamma distribution with shape parameter \\(p&gt;0\\) and intensity parameter \\(\\lambda&gt;0\\) is defined by its density with respect to Lebesgue measure on \\([0,\\infty)\\): \\[\nx \\mapsto \\lambda^p \\frac{x^{p-1}}{\\Gamma(p)} \\mathrm{e}^{-\\lambda x} \\, .\n\\] The reciprocal of the intensity parameter is called the scale parameter.\n\n\nExercise 6.7 Check that \\(x \\mapsto \\lambda^p \\frac{x^{p-1}}{\\Gamma(p)} \\mathrm{e}^{-\\lambda x}\\) is a density probability over \\([0, \\infty)\\).\n\n\nExercise 6.8 If \\(X\\) is Gamma distributed with shape parameter \\(p\\) and scale parameter \\(\\sigma\\), what is the distribution of \\(a X\\)?\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: Gamma densities with different parameters: scales \\(1, 1, 1/3, 1, 2\\) and shapes \\(1, 2, 3, 5, 5/2\\). Expectation equals shape times scale,\nvariance equals shape times squared scale.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html#sec-gauss",
    "href": "032-acfamilies.html#sec-gauss",
    "title": "6  Absolutely continuous probability measures",
    "section": "6.4 Univariate Gaussian distributions",
    "text": "6.4 Univariate Gaussian distributions\nGaussian distributions play a central role in Probability theory, Statistics, Information theory, and Analysis.\n\nThe Gaussian or normal distribution with mean \\(\\mu \\in \\mathbb{R}\\) and variance \\(\\sigma^2, \\sigma&gt;0\\) has density \\[\nx \\mapsto \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\mathrm{e}^{- \\frac{(x-\\mu)^2}{2 \\sigma^2}} \\qquad\\text{for } x \\in \\mathbb{R} \\, .\n\\] The standard Gaussian density is defined by \\(\\mu=0, \\sigma=1\\).\n\n\nExercise 6.9 Check that \\(x \\mapsto \\frac{\\mathrm{e}^{-x^2/2}}{\\sqrt{2\\pi}}\\) is a probability density over \\(\\mathbb{R}\\).\n\n\nExercise 6.10 If \\(X\\) is distributed according to a standard Gaussian density, what is the distribution of \\(\\mu + \\sigma X\\)?\n\n\nExercise 6.11 If \\(X\\) is distributed according to a standard Gaussian density, show that \\[\n\\Pr \\{ X &gt; t \\} \\leq \\frac{1}{t} \\frac{\\mathrm{e}^{-t^2/2}}{\\sqrt{2\\pi}} \\qquad\\text{for } t&gt;0\\,.\n\\]\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Gaussian densities. The location parameter \\(\\mu\\) coincides with the mean and the median. The scale parameter is the standard deviation. The Inter-Quartile-Range (IQR) is proportional to the standard deviation. If \\(\\Phi^{\\leftarrow}\\) denotes the quantile function of \\(\\mathcal{N}(0,1)\\) then the interquartile range of \\(\\mathcal{N}(\\mu, \\sigma^2)\\) is \\(\\sigma \\Big(\\Phi^{\\leftarrow}(3/4) - \\Phi^{\\leftarrow}(1/4)\\Big)=2 \\sigma \\Phi^{\\leftarrow}(3/4)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html#sec-cdf_ac",
    "href": "032-acfamilies.html#sec-cdf_ac",
    "title": "6  Absolutely continuous probability measures",
    "section": "6.5 Cumulative distribution functions and absolute continuity",
    "text": "6.5 Cumulative distribution functions and absolute continuity\nIf a cumulative distribution function is defined as the integral of some non-negative Lebesgue integrable function, we know that the corresponding probability distribution is absolutely continuous with respect to Lebesgue measure.\nWe may ask for a criterion that characterises the cumulative distribution function of absolutely continuous probability distribution. Such a criterion is embodied by the next definition. We overload the expression absolutely continuous.\n\nDefinition 6.2 (Absolutely continuous functions) A real valued function \\(F\\) on \\([a,b]\\) is said to be absolutely continuous iff for all \\(\\delta&gt;0\\) there exists \\(\\epsilon&gt;0\\) such that for every collection \\(([a_i, b_i])_{i\\leq n}\\) for non-overlapping sub-intervals (\\([a_i,b_i] ⊆ [a,b]\\) for all \\(i\\leq n\\) and \\(\\ell([a_i,b_i] \\cap [a_j,b_j])=0\\) for \\(i\\neq j\\) ) with \\(∑_{i\\leq n} |b_i-a_i|\\leq ϵ\\), \\[∑_{i\\leq n} |F(b_i)-F(a_i)|\\leq δ\\]\n\nAbsolute continuity, differentiability and integration of derivatives are connected by the next Theorem. This Theorem tells us that a cumulative distribution function is absolutely continuous in the sense of Definition Definition 6.2 iff the corresponding probability distribution is absolutely continuous with respect to Lebesgue measure.\n\nTheorem 6.2 (Fundamental Theorem of Calculus) A real valued function \\(F\\) on \\([a,b]\\) is absolutely continuous iff the next three conditions hold\n\nThe derivative \\(F'\\) exists Lebesgue almost everywhere on \\([a,b]\\)\nThe derivative \\(F'\\) is Lebesgue integrable\nFor every \\(x \\in [a,b]\\), \\(F(x) - F(a) = \\int_{[a,b]} \\mathbb{I}_{[a,x]}(t) F'(t) \\mathrm{d}t\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html#sec-imgdens",
    "href": "032-acfamilies.html#sec-imgdens",
    "title": "6  Absolutely continuous probability measures",
    "section": "6.6 Computing the density of an image probability distribution",
    "text": "6.6 Computing the density of an image probability distribution\nRecall the change of variable formula in elementary calculus. If \\(\\phi\\) is monotone increasing and différentiable from open \\(A\\) to \\(B\\) and \\(f\\) is Riemann integrable over \\(B\\), then \\[\n\\int_B f(y) \\, \\mathrm{d}y = \\int_A f(\\phi(x)) \\, \\phi^{\\prime}(x) \\, \\mathrm{d}x \\,\n\\]\n\nExercise 6.12 Check the elementary change of variable formula.\n\nThe goal of this section is state a multi-dimensional generalization of this elementary formula. This is the content of Theorem 6.4). This extension is then used to establish an off-the-shelf formula for computing the density of an image distribution in Theorem 6.5).\nLet us start with a uni-dimensional warm-up. When starting from the uniform distribution on \\([0,1]\\) and applying a monotone differentiable transformation, the density of the image measure is easily computed.\n\nExercise 6.13 Let \\(\\phi\\) be differentiable and increasing on \\([0,1]\\), and let \\(P\\) be the uniform distribution on \\([0,1]\\).\nCheck that \\(P \\circ \\phi^{-1}\\) has density \\(\\frac{1}{\\phi'\\circ \\phi^\\leftarrow}\\) on \\(\\phi([0,1])\\).\n\nThe next proposition extends this observation.\n\nIf the real valued random variable \\(X\\) is distributed according to \\(P\\) with density \\(f\\), and \\(\\phi\\) is monotone increasing and differentiable over \\(\\operatorname{supp}(P)\\), then the probability distribution of \\(Y = \\phi(X)\\) has density \\[\ng = \\frac{f \\circ \\phi^{\\leftarrow}}{\\phi^{\\prime}\\circ \\phi^{\\leftarrow}}  \\,\n\\] over \\(\\phi\\big(\\operatorname{supp}(P)\\big)\\).\n\n\nProof. By the fundamental theorem of calculus, the density \\(f\\) is a.e. the derivative of the cumulative distribution function \\(F\\) of \\(P\\).\nThe cumulative distribution function of \\(Y=\\phi(X)\\) satisfies: \\[\\begin{align*}\nP \\Big\\{ Y \\leq y \\Big\\}\n  & = P \\Big\\{ \\phi(X) \\leq y \\Big\\} \\\\\n  & = P \\Big\\{ X \\leq \\phi^{\\leftarrow} (y) \\Big\\} \\\\\n  & = F \\circ \\phi^{\\leftarrow}(y)\n\\end{align*}\\] Almost everywhere, \\(F \\circ \\phi^{\\leftarrow}\\) is differentiable, and has derivative \\(\\frac{f \\circ \\phi^{\\leftarrow}}{\\phi' \\circ  \\phi^{\\leftarrow}}\\) in \\(\\phi(\\text{supp}(P))\\), \\(0\\) elsewhere. and \\[\nP \\Big\\{ Y \\leq y \\Big\\} = \\int_{(-\\infty, y] \\cap \\phi(\\text{supp}(P))} \\frac{f \\circ \\phi^{\\leftarrow}(u)}{\\phi' \\circ  \\phi^{\\leftarrow}(u)} \\mathrm{d}u\n\\]\n\\(\\square\\)\n\n\nThe next corollary is as useful as simple.\n\nCorollary 6.1 If the distribution of the real valued random variable \\(X\\) has density \\(f\\) then the distribution of \\(\\sigma X + \\mu\\) has density \\(\\frac{1}{\\sigma}f\\Big(\\frac{\\cdot -\\mu}{\\sigma}\\Big)\\), .\n\n\n\n\nIn univariate calculus, it is easy to establish that if a function is continuous and increasing over an open set, it is invertible and its inverse is continuous and increasing. If the function is differentiable with positive derivative, its inverse is also differentiable. Moreover, the differential and the differential of the inverse are related in transparent way.\nThe Global Inversion Theorem extends the preceding observation to the multivariate setting.\n\nTheorem 6.3 (Global Inversion Theorem) Let \\(U\\) and \\(V\\) be two non-empty open subsets of \\(\\mathbb{R}^d\\). Let \\(\\phi\\) be a continuous bijective from \\(U\\) to \\(V\\). Assume furthermore that \\(\\phi\\) is continuously differentiable, and that \\(D\\phi_x\\) is non-singular at every \\(x \\in U\\).\nThen, the inverse function \\(\\phi^{\\leftarrow}\\) is also continuously differentiable on \\(V\\) and at every \\(y \\in V\\): \\[\nD\\phi^{\\leftarrow}_y = \\Big(D\\phi_{\\phi^{\\leftarrow}(y)} \\Big)^{-1} \\, .\n\\]\n\nThe Jacobian determinant of \\(\\phi\\) is the determinant of the matrix that represents the differential. It is denoted by \\(J_\\phi\\). Recall that: \\[\nJ_{\\phi^{\\leftarrow}}(y) = \\Big(J_{\\phi}(\\phi^{\\leftarrow}(y)) \\Big)^{-1} \\, .\n\\] The multidimensional version of the change of variable formula is stated under the same assumptions as the Global Inversion Theorem. We admit the next Theorem.\n\nTheorem 6.4 (Geometric change of variable formula) Let \\(U\\) and \\(V\\) be two non-empty open subsets of \\(\\mathbb{R}^d\\). Let \\(\\phi\\) be a continuous bijective from \\(U\\) to \\(V\\). Assume furthermore that \\(\\phi\\) is continuously differentiable, and that \\(D\\phi_x\\) is non-singular at every \\(x \\in U\\).\nLet \\(\\ell\\) denote the Lebesgue measure on \\(\\mathbb{R}^d\\).\nFor any a non-negative Borel-measurable function \\(f\\): \\[\n\\int_U f(x) \\mathrm{d}\\ell(x)   = \\int  f(\\phi^\\leftarrow(y)) \\Big|J_{\\phi^\\leftarrow}(y) \\Big| \\mathrm{d}\\ell(y) \\, .\n\\]\n\nMoving from cartesian coordinates to polar/spherical coordinates is easy thanks to an non-trivial application of Theorem 6.4).\nThe Image density formula is a corollary of the geometric change of variable formula.\n\nTheorem 6.5 (Image density formula) Let \\(P\\) have density \\(f\\) over open \\(U \\subseteq  \\mathbb{R}^d\\).\nLet \\(\\phi\\) be bijective fron \\(U\\) to \\(\\phi(U)\\) and \\(\\phi\\) be continuously differentiable over \\(U\\) with non-singular differential.\nThe density \\(g\\) of the image distribution \\(P \\circ \\phi^{-1}\\) over \\(\\phi(U)\\) is given by \\[\ng(y) = f\\big(\\phi^\\leftarrow(y)\\big) \\times \\big|J_{\\phi^\\leftarrow}(y)\\big|  =  f\\big(\\phi^\\leftarrow(y)\\big) \\times \\Big|J_{\\phi}(\\phi^\\leftarrow(y))\\Big|^{-1}\\, .\n\\]\n\nThe proof of Theorem 6.5) from Theorem 6.4) is a routine application of the transfer formula.\n\nProof. Let \\(B\\) be a Borelian subset of \\(\\phi(U)\\). By the transfer formula: \\[\\begin{align*}\nP\\Big\\{ Y \\in B \\Big\\}\n  & =  P\\Big\\{ \\phi(X) \\in B \\Big\\} \\\\\n  & = \\int_U \\mathbb{I}_B(\\phi(x)) f(x) \\mathrm{d}\\ell(x) \\,.\n\\end{align*}\\] Now, we invoke Theorem 6.4): \\[\\begin{align*}\n\\int_U \\mathbb{I}_B(\\phi(x)) f(x) \\mathrm{d}\\ell(x)\n& = \\int_{\\phi(U)} \\mathbb{I}_B(\\phi(\\phi^\\leftarrow(y))) f(\\phi^\\leftarrow(y)) \\Big|J_{\\phi^\\leftarrow}(y)\\Big| \\mathrm{d}\\ell(y) \\\\\n& = \\int_{\\phi(U)} \\mathbb{I}_B(y) f(\\phi^\\leftarrow(y)) \\Big|J_{\\phi^\\leftarrow}(y)\\Big| \\mathrm{d}\\ell(y) \\, .\n\\end{align*}\\] This suffices to conclude that \\(f\\circ \\phi^\\leftarrow \\Big|J_{\\phi^\\leftarrow}\\Big|\\) is a version of the density of \\(P \\circ \\phi^{-1}\\) with respect to Lebesgue measure over \\(\\phi(U)\\).\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html#sec-gammabeta",
    "href": "032-acfamilies.html#sec-gammabeta",
    "title": "6  Absolutely continuous probability measures",
    "section": "6.7 Application: Gamma-Beta calculus",
    "text": "6.7 Application: Gamma-Beta calculus\nThe image density formula is applied to show a remarkable connexion between Gamma and Beta distributions.\n\nProposition 6.2 Let \\(X, Y\\) be independent random variables distributed according to \\(\\Gamma(p, \\lambda)\\) and \\(\\Gamma(q, \\lambda)\\) (the intensity parameter are identical). Let \\(U = X+Y\\) and \\(V= X/(X+Y)\\).\nThe random variables \\(U\\) and \\(V\\) are independent. Random variable \\(U\\) is distributed according to \\(\\Gamma(p+q, \\lambda)\\) while \\(V\\) is distributed according to \\(\\operatorname{Beta}(p, q)\\).\n\n\nProof. The mapping \\(f: ]0, \\infty)^2 \\to ]0, \\infty) \\times ]0,1[\\) defined by \\[\nf(x,y) =  \\Big(x+y, \\frac{x}{x+y} \\Big)\n\\] is one-to-one with inverse \\(f^{\\leftarrow}(u,v) = \\Big(uv,u(1-v)\\Big)\\). The Jacobian matrix of \\(f^{\\leftarrow}\\) at \\((u,v)\\) is \\[\n\\begin{pmatrix}\n  v & u \\\\\n  (1-v) & -u\n\\end{pmatrix}\n\\] with determinant \\(-uv -u +uv=-u\\). The joint image density at \\((u,v) \\in ]0,\\infty) \\times ]0,1[\\) is \\[\\begin{align*}\n& = \\lambda^{p+q}\\frac{(uv)^{p-1}}{\\Gamma(p)} \\frac{(u(1-v))^{q-1}}{\\Gamma(q)}\n\\mathrm{e}^{-\\lambda (uv + u(1-v))} u \\\\\n& = \\Big(\\lambda^{p+q} \\frac{u^{p+q-1}}{\\Gamma(p+q)} \\mathrm{e}^{\\lambda u}\\Big)\n\\times \\Big(\\frac{\\Gamma(p+q)}{\\Gamma(q)\\Gamma(p)} v^{p-1} (1-v)^{q-1}\\Big) \\,.\n\\end{align*}\\] The factorization of the joint density proves that the \\(U\\) and \\(V\\) are independent. We recognize that the density of (the distribution of) \\(U\\) is the Gamma density with shape parameter \\(p+q\\), intensity parameter \\(\\lambda\\). The density of the distribution of \\(V\\) is the Beta density with parameters \\(p\\) and \\(q\\).\n\n\nExercise 6.14 Assume \\(X_1, X_2, \\ldots, X_n\\) form an independent family with each \\(X_i\\) distributed according to \\(\\Gamma(p_i, \\lambda)\\).\nDetermine the joint distribution of \\[\n\\sum_{i=1}^n X_i, \\frac{X_1}{\\sum_{i=1}^n X_i}, \\frac{X_2}{\\sum_{i=1}^n X_i}, \\ldots, \\frac{X_{n-1}}{\\sum_{i=1}^n X_i}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "032-acfamilies.html#sec-bibac",
    "href": "032-acfamilies.html#sec-bibac",
    "title": "6  Absolutely continuous probability measures",
    "section": "6.8 Bibliographic remarks",
    "text": "6.8 Bibliographic remarks\nDudley (2002) and Pollard (2002) provide a full development of absolute continuity and self-contained proofs the Radon-Nikodym’s Theorem.\n\n\n\n\nDudley, R. M. (2002). Real analysis and probability (Vol. 74, p. x+555). Cambridge: Cambridge University Press.\n\n\nPollard, D. (2002). A user’s guide to measure theoretic probability (Vol. 8, p. xiv+351). Cambridge University Press, Cambridge.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Absolutely continuous probability measures</span>"
    ]
  },
  {
    "objectID": "023-discrete-condition.html",
    "href": "023-discrete-condition.html",
    "title": "7  Discrete Conditioning",
    "section": "",
    "text": "7.1 Roadmap\nConditioning is central to probabilistic reasoning. In this lesson, we investigate discrete conditioning. In this setting, the definition of conditional probability is not an issue. The definition of conditional expectation can be deceptively simple. Nevertheless the discrete setting lends itself to intuitive definitions and manipulations.\nThe simplest notion we meet is conditional probability with respect to a specific event with positive probability (Section 7.2). Conditional probability offers an intuitive interpretation of independence.\nIn Section 7.3 we state, check and discuss Bayes formula.\nIn Section 7.4, we define conditional expectation with respect to an atomic \\(\\sigma\\)-algebra. This defines conditional expectation with respect to a discrete random variables.\nIn Section 7.5, we characterize conditional expectation as an optimal predictor. This characterization is very helpful when defining conditional expectation in the general setting.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Conditioning</span>"
    ]
  },
  {
    "objectID": "023-discrete-condition.html#sec-condevent",
    "href": "023-discrete-condition.html#sec-condevent",
    "title": "7  Discrete Conditioning",
    "section": "7.2 Conditioning with respect to an event",
    "text": "7.2 Conditioning with respect to an event\n\nDefinition 7.1 Let \\(P\\) be a probability distribution on \\((\\Omega, \\mathcal{F})\\). Let \\(A \\in \\mathcal{F}\\) be such that \\(P\\{A\\} &gt; 0\\). Let \\(B\\) be another event ( \\(B \\in \\mathcal{F}\\) ), the probability of \\(B\\) given \\(A\\) is defined as\n\\[P\\{B \\mid A\\}= \\frac{P\\{A \\cap B\\}}{P\\{A\\}} .\\]\n\n\n\n\n\nIf \\(X\\) is a standard Gaussian random variable on \\((\\Omega, \\mathcal{F})\\), and event \\(A\\) is defined by \\(\\{ \\omega : X(\\omega) \\geq t\\}\\) for some \\(t\\geq 0\\), we may condition on event \\(A\\) and define \\(P\\{B \\mid A\\}\\) for \\(B = \\{ \\omega : |X(\\omega)|\\geq 2t\\}\\).\nWe get\n\\[P\\{ B \\mid A \\} =  \\frac{P\\{ X \\geq 2 t\\}}{P\\{ X \\geq t\\}} \\,.\\]\n\n\n\nWe may check the next proposition by considering once again the definition of probability distributions.\n\nProposition 7.1 Let \\(P\\) be a probability distribution on \\((\\Omega, \\mathcal{F})\\).\nLet \\(A  \\in \\mathcal{F})\\) be such that \\(P\\{A\\} &gt; 0\\), then \\(P\\{\\cdot \\mid A\\}\\) (\\(P\\) given \\(A\\)) defines a probability distribution over \\((\\Omega, \\mathcal{F}).\\)\n\n\n\n\n\nProof. \\(P(\\cdot \\mid A)\\) maps \\(\\mathcal{F}\\) to \\([0,1]\\).\nWe have \\(P(\\Omega \\mid A)= P(A \\cap \\Omega)/ P(A) = P(A) /P(A)= 1\\)\nLet \\((B_n)_n\\) be a monotone increasing sequence of events, then \\[\n\\begin{array}{rl}\n  P (\\cup_n B_n \\mid A) & = \\frac{P((\\cup_n B_n) \\cap A)}{P(A)} \\\\\n  & = \\frac{P(\\cup_n (B_n \\cap A))}{P(A)} \\\\\n  & = \\frac{\\lim_n P(B_n \\cap A)}{P(A)} \\\\\n  & = \\lim_n P(B_n \\mid A) \\, .\n\\end{array}\n\\]\n\n\n\n\nWe may consider the distribution of random variables on \\((\\Omega, \\mathcal{F})\\) under \\(P\\{\\cdot \\mid A\\}\\). We compute the expectation of \\(X\\) under \\(P\\{ \\cdot \\mid A\\}\\):\n\\[\\mathbb{E}_{P\\{\\cdot \\mid A\\}} X =  \\frac{\\mathbb{E} [\\mathbb{I}_A\\, X]}{P\\{A\\}} \\,.\\]\nThis is often denoted by \\(\\mathbb{E}[X \\mid A]\\), we will try to avoid this possibly misleading notation.\n\n\n\n\nExample 7.1 Assume \\(X\\) is standard normally distributed. One may investigate the distribution of \\(X^2\\) conditionnally on event \\(A = \\{ \\omega : X(\\omega)\\geq t\\}\\). For \\(t&gt;1\\), we have\n\\[\n\\begin{array}{rl}\n\\mathbb{E}_{P\\{\\cdot \\mid X \\geq t\\}} X^2\n& = \\frac{\\int_t^\\infty x^2 \\phi(x) \\mathrm{d}x}{\\int_t^\\infty \\phi(x) \\mathrm{d}x}\\\\\n& \\leq \\frac{t^2}{1-1/t} + 1 \\, .\n\\end{array}\n\\]\nwhere the upper bound is obtained by repeated integration by parts.\nThe distribution of \\(X\\) given \\(A\\) is not Gaussian. Under \\(A\\), \\(X\\) is very concentrated in the neighborhood of \\(t,\\) and tends to be more concentrated as \\(t\\) goes to infinity.\n\n\n\n\nKnowing the probability distribution given event \\(A\\) enables to investigate independence of events with respect to \\(A\\) The next trivial proposition is worth reminding.\n\n\n\nProposition 7.2 If \\(A\\) and \\(B \\in \\mathcal{F}\\) satisfy \\(P\\{A\\}&gt; 0\\), then \\(A\\) and \\(B\\) are independent under \\(P\\) iff\n\\[P\\{B\\mid A\\}= P\\{B\\}.\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Conditioning</span>"
    ]
  },
  {
    "objectID": "023-discrete-condition.html#sec-bayesformula",
    "href": "023-discrete-condition.html#sec-bayesformula",
    "title": "7  Discrete Conditioning",
    "section": "7.3 Bayes formula",
    "text": "7.3 Bayes formula\n\n\n\nBayes formula is sometimes used in probabilistic causation theory. This is a difficult matter. Causality is a subtle notion and we will refrain from making causal interpretations.\n\nProposition 7.3 (Bayes formula) Let \\(P\\) be a probability distribution on \\((\\Omega, \\mathcal{F})\\), let \\((A_i)_{i \\in \\mathcal{I} \\subseteq \\mathbb{N}}\\) be a collection of pairwise disjoint events, with non-zero probability such that \\(\\cup_{i \\in \\mathcal{I}} A_i = \\Omega\\) (\\((A_i)_i\\) form a complete system of events), let \\(B\\) be an event with non-zero probability, then for all \\(i \\in \\mathcal{I}\\),\n\\[P\\{A_i \\mid B\\} = \\frac{P\\{A_i \\} \\times P\\{B \\mid A_i \\}}{\\sum_{j \\in \\mathcal{I}} P\\{A_j\\}\\times P\\{B \\mid A_j\\}}\\]\n\n\n\n\n\nProof. By definition, \\(P\\{A_i \\mid B\\}= P\\{A_i \\cap B\\}/ P\\{B\\}= P\\{A_i \\}\n\\times P\\{B \\mid A_i \\}/ P\\{B\\}\\).\nMorever \\[\n\\begin{array}{rl}\nP \\{ B\\} & = P\\{B \\cap (\\cup_{j \\in \\mathcal{I}} A_j)\\}\\\\\n& = P\\{\\cup_{j \\in \\mathcal{I}} (B \\cap A_j)\\}  \\\\\n& = \\sum_{j \\in \\mathcal{I}} P\\{B \\cap A_j \\} \\\\\n& = \\sum_{j \\in \\mathcal{I}} P\\{A_j \\} \\times P\\{B \\mid A_j \\}.\n\\end{array}\n\\]\n\n\n\n\nIn the preceding proposition, \\(P\\{A_i \\}\\) is called the prior probability of \\(A_i\\) and \\(P\\{A_i \\mid B\\}\\) the posterior probability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Conditioning</span>"
    ]
  },
  {
    "objectID": "023-discrete-condition.html#sec-conddiscretealgebra",
    "href": "023-discrete-condition.html#sec-conddiscretealgebra",
    "title": "7  Discrete Conditioning",
    "section": "7.4 Conditional expectation with respect to a discrete \\(\\sigma\\)-algebra",
    "text": "7.4 Conditional expectation with respect to a discrete \\(\\sigma\\)-algebra\nWhile the general notion of conditional expectation requires some abstraction, we can introduce conditioning with respect to a discrete \\(\\sigma\\)-algebra starting from the elementary notion of conditional probability with respect to an event with positive probability.\n\n\n\n\nDefinition 7.2 Let \\(\\Omega\\) be a universe, \\(\\mathcal{F}\\) a \\(\\sigma\\)-algebra of events on \\(\\Omega\\), \\(P\\) a probability distribution on \\((\\Omega, \\mathcal{F})\\), let \\((A_i)_{i \\in \\mathcal{I} \\subseteq \\mathbb{N}}\\) be pairwise disjoint events, with non-zero probability such that \\(\\cup_i A_i = \\Omega .\\) Let \\(\\mathcal{G}\\) be the atomic \\(\\sigma\\)-algebra generated by \\((A_i)_{i \\in \\mathcal{I}}\\).\nLet \\(X\\) be a random variable from \\((\\Omega, \\mathcal{F})\\) to \\((\\mathcal{X}, \\mathcal{H})\\), the conditional expectation of \\(X\\) with respect to \\(\\mathcal{G}\\) is the random variable defined as\n\\[\n\\mathbb{E} [X \\mid \\mathcal{G}] = \\sum_{i \\in  \\mathcal{I}}  \\mathbb{E}_{P_{\\{\\cdot |A_i\n\\}}} [X] \\times \\mathbf{1}_{A_i} .\n\\]\n\n\n\n\nWhile \\(\\mathbb{E}_{P_{\\{\\cdot |A_i \\}}} [X]\\) is a real number, \\(\\mathbb{E} [X \\mid \\mathcal{G}]\\) is a \\(\\mathcal{G}\\)-measurable function from \\(\\Omega\\) to \\(\\mathcal{X}\\):\n\\[\n\\mathbb{E} [X \\mid \\mathcal{G}](\\omega) =  \\sum_{i \\in  \\mathcal{I}}  \\mathbb{E}_{P_{\\{\\cdot |A_i\n\\}}} [X] \\times \\mathbf{1}_{A_i}(\\omega) \\qquad \\forall \\omega \\in \\Omega\\, .\n\\]\nThese two kinds of objects should not be confused. We will refrain from using notation \\(\\mathbb{E}[X \\mid A_i]\\) since it may be confusing: \\(\\mathbb{E}[X \\mid A_i]\\) might denote either \\(\\mathbb{E}_{P_{\\{\\cdot |A_i     \\}}} [X]\\) or \\(\\mathbb{E} [X \\mid \\sigma(A_i)]\\) where \\(\\sigma(A_i)\\) is the sigma-algebra generated by \\(A_i\\): \\(\\{ A_i, A_i^c, \\Omega, \\emptyset\\}\\).\n\n\n\n\nProposition 7.4 Let \\(P\\) be a probability distribution on \\((\\Omega, \\mathcal{F})\\). Let \\((A_i)_{i \\in \\mathcal{I} \\subseteq \\mathbb{N}}\\) be a collection of pairwise disjoint events, with non-zero probability satisfying \\(\\cup_{i \\in \\mathcal{I}} A_i = \\Omega.\\) Let \\(\\mathcal{G}=\\sigma\\Big((A_i)_{i \\in \\mathcal{I}}\\Big)\\) denote the sigma-algebra generated by \\((A_i)_{i \\in \\mathcal{I}}\\).The random variable \\(X\\) is assumed to be \\(P\\)- integrabe.\n\nThe conditional expectation \\(\\mathbb{E} [X \\mid \\mathcal{G}]\\) is a \\(\\mathcal{G}\\)-measurable random variable, that satisfies\n\n\\[\n\\mathbb{E} \\left[ YX \\right] = \\mathbb{E} \\left[ Y \\mathbb{E} [X \\mid\n\\mathcal{G}] \\right] \\qquad \\forall Y \\in \\sigma(\\mathcal{G}), Y \\text{ bounded.}\n\\]\n\nIf two \\(\\mathcal{G}\\)-measurable random variables \\(Z, T\\) satisfy \\(\\mathbb{E} \\left[ YX \\right] = \\mathbb{E} \\left[ Y Z \\right] = \\mathbb{E}[YT]\\), for all \\(Y \\in \\sigma(\\mathcal{G}), Y \\text{ bounded}\\), then \\(Z=T\\) almost surely.\n\n\n\n\n\n\nProof. We need to ckeck points 1.) and 2.):\n\n\\(\\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]\\) satisfies first property in Proposition Proposition 7.4.\nIf \\(Z\\) satisfies Proposition 7.4, then \\(Z = \\mathbb{E}\\left[ X \\mid \\mathcal{G} \\right]\\) \\(P\\)-almost-surely.\n\nChecking i.)\nIf \\(Y\\) is \\(\\mathcal{G}\\)-measurable, then \\(Y = \\sum_{i \\in \\mathcal{I}} \\lambda_i \\mathbf{1}_{A_i}\\) for some real-valued sequence \\((\\lambda_i)_{i \\in\n\\mathcal{I}}\\) .\nThen \\[\n\\begin{array}{rl}\n\\mathbb{E} [Y \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]] & =\n\\mathbb{E} \\left[ \\left( \\sum_{i \\in \\mathcal{I}}   \\lambda_i\n\\mathbf{1}_{A_i} \\right)    \\left( \\sum_{j \\in \\mathcal{I}}\n\\mathbf{1}_{A_j}  \\frac{\\mathbb{E} [ \\mathbf{1}_{A_j} X]}{P\\{A_j \\}}\n\\right) \\right]\\\\\n& =  \\left.  \\mathbb{E} \\left[ \\sum_{i \\in \\mathcal{I}}\n\\lambda_i  \\mathbf{1}_{A_i}     \\frac{\\mathbb{E} [\n\\mathbf{1}_{A_i} X]}{P\\{A_i \\}} \\right) \\right]\\\\\n& =  \\sum_{i \\in \\mathcal{I}}   \\lambda_i  \\mathbb{E} [\n\\mathbf{1}_{A_i} X] \\frac{\\mathbb{E} \\left[ \\mathbf{1}_{A_i}\n\\right]}{P\\{A_i \\}} \\quad \\text{linearity of expectation}\\\\\n& =  \\sum_{i \\in \\mathcal{I}}   \\lambda_i  \\mathbb{E} [\n\\mathbf{1}_{A_i} X]\\\\\n& =  \\mathbb{E} \\left[ \\left( \\sum_{i \\in \\mathcal{I}} \\lambda_i\n\\mathbf{1}_{A_i} \\right) X   \\right]\\\\\n& =  \\mathbb{E} \\left[ YX \\right] .\n\\end{array}\n\\]\n\n\nChecking ii.)\n\n\nAssume \\(Z\\) satisfies Proposition 7.4.\nLet us define \\(Y\\) using \\(Y = \\mathbf{1}_{A_i}\\), for some index \\(i \\in \\mathcal{I}\\).\nAs \\(Z\\) is \\(\\mathcal{G}\\)-measurable, there exists a real-valued sequence \\(\\left( \\mu_j\n\\right)_{j \\in \\mathcal{I}}\\), such that \\(Z = \\sum_{j \\in \\mathcal{I}} \\mu_j\n\\mathbf{1}_{A_j}.\\)\nThus, relying on the fact that events \\(A_j\\) are pairwise disjoint:\n\\[\n\\begin{array}{rl}\n\\mathbb{E} \\left[ ZY \\right] & =  \\mathbb{E} \\left[ \\sum_{j \\in\n\\mathcal{I}} \\mu_j  \\mathbf{1}_{A_j}  \\mathbf{1}_{A_i}\n\\right] = \\mu_i P\\{A_i \\}\n\\end{array}\n\\]\nBy the defining property of \\(Z\\), we have \\[\\mathbb{E} [ZY] = \\mathbb{E} [XY] = \\mathbb{E} [X \\mathbf{1}_{A_i}]\\,.\\]\nFinally, for all \\(i \\in \\mathcal{I}\\), \\(\\mu_i= \\mathbb{E}[X\\mathbf{1}_{A_i}]/ P\\{A_i \\}.\\)\nWe can now conclude \\(Z = \\mathbb{E}[X \\mid \\mathcal{G}]\\).\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Conditioning</span>"
    ]
  },
  {
    "objectID": "023-discrete-condition.html#sec-condpred",
    "href": "023-discrete-condition.html#sec-condpred",
    "title": "7  Discrete Conditioning",
    "section": "7.5 Conditional expectation as prediction",
    "text": "7.5 Conditional expectation as prediction\nThe next proposition reveals the role of conditional expectation in prediction/approximation problems.\n\n\n\nProposition 7.5 Let \\(Y\\) be a square-integrable random variable on \\((\\Omega, \\mathcal{F}, P)\\) and \\(\\mathcal{G}\\) a discrete sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\). The conditional expectation of \\(Y\\) with respect to \\(\\mathcal{G}\\) minimizes\n\\[\\mathbb{E}\\left[\\big(Y - Z\\big)^2\\right]\\]\namongst \\(\\mathcal{G}\\)-measurable square-integrable random variables.\n\nRecall that a \\(\\mathcal{G}\\)-measurable random variable is a function that remains constant on each \\(A_i, i \\in \\mathcal{I}\\).\n\nProof. If \\(Y\\) is a random variable on \\((\\Omega,\\mathcal{F}),\\) and if we are trying to predict at best \\(Y\\) from a \\(\\mathcal{G}\\)-measurable random variable , we are looking for a sequence of coefficients \\((b_i)_{i\\in \\mathcal{I}}\\) that minimizes:\n\\[\n\\begin{array}{rl}\n\\mathbb{E}_P \\left[ \\Big( Y -  \\sum_{i\\in \\mathcal{I}} b_i \\mathbf{I}_{A_i} \\Big)^2 \\right]\n& = \\mathbb{E}_P \\left[ \\Big(  \\sum_{i\\in \\mathcal{I}} (Y-b_i)\n\\mathbf{I}_{A_i} \\Big)^2 \\right] \\\\\n& =\n\\sum_{i\\in \\mathcal{I}}\n\\mathbb{E}_P \\left[ \\left( Y-b_i\\right)^2\n\\mathbf{I}_{A_i} \\right]\n\\\\\n& = \\sum_{i\\in \\mathcal{I}} P\\{A_i\\}\\,\n\\mathbb{E}_{P\\{\\cdot \\mid A_i\\}} \\left[ \\left( Y-b_i\\right)^2  \\right]\n\\end{array}\n\\]\nThus for each \\(i\\), \\(b_i\\) must coincide with the expectation of \\(Y\\) under \\(P\\{\\cdot \\mid A_i\\}.\\) The best prediction of \\(Y\\), in the sense of the quadratic error, among the \\(\\mathcal{G}\\)-measurable functions is the conditional expectation of \\(Y\\) with respect to \\(\\mathcal{G}\\).\n\\(\\square\\)\n\nThe properties identified by propositions Proposition 7.4 and @ref(prp:espercondpred) serve as a definition for the conditional expectation with respect to a general \\(\\sigma\\)-algebra.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Conditioning</span>"
    ]
  },
  {
    "objectID": "023-discrete-condition.html#sec-easypropcondexp",
    "href": "023-discrete-condition.html#sec-easypropcondexp",
    "title": "7  Discrete Conditioning",
    "section": "7.6 Properties of conditional expectation",
    "text": "7.6 Properties of conditional expectation\nWe state without proof a number of useful properties of conditional expectation with respect to discrete \\(\\sigma\\)-algebras. We shall prove them in full generality later.\n\nProposition 7.6 If \\(X \\leq Y\\), \\(P\\)-a.s., then\n\\[\\mathbb{E}[X \\mid \\mathcal{G}] \\leq \\mathbb{E}[Y \\mid \\mathcal{G}] \\qquad P \\text{-p.s.}\\]\n\n\nProposition 7.7 \\[\n\\mathbb{E}[ aX + bY \\mid \\mathcal{G}] = a \\mathbb{E}[X \\mid \\mathcal{G}] + b \\mathbb{E}[Y \\mid \\mathcal{G}] \\,.\n\\]\n\n\nProposition 7.8 If \\(\\mathcal{H} \\subseteq \\mathcal{G} \\subseteq \\mathcal{F}\\)\n\\[\n\\mathbb{E}\\left[\\mathbb{E}\\left[ X \\mid \\mathcal{G}\\right] \\mid \\mathcal{H} \\right]\n  = \\mathbb{E}\\left[ X \\mid \\mathcal{H} \\right] \\, .\n\\]\n\\[\\mathbb{E}\\left[\\mathbb{E}\\left[ X \\mid \\mathcal{H}\\right] \\mid \\mathcal{G} \\right]\n  = \\mathbb{E}\\left[ X \\mid \\mathcal{H} \\right] \\, .\n\\]\n\n\nProve the proposition.\n\n\n\\[\\mathbb{E} X = \\mathbb{E}[\\mathbb{E}[X \\mid \\mathcal{G}]] \\, .\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Conditioning</span>"
    ]
  },
  {
    "objectID": "023-discrete-condition.html#sec-gw1",
    "href": "023-discrete-condition.html#sec-gw1",
    "title": "7  Discrete Conditioning",
    "section": "7.7 Application: Galton-Watson processes I",
    "text": "7.7 Application: Galton-Watson processes I\nThe size of generation \\(k\\geq 0\\) is defined recursively by\n\\[Z_0 = 1, \\qquad Z_{k+1} = \\sum_{i=1}^{Z_k} X^k_{i} \\, .\\]\nThe \\(\\sigma\\)-algebra \\(\\sigma(Z_k)\\) is discrete/atomic, it is generated by the pairwise disjoint events \\(\\Big\\{ Z_k = a\\Big\\}\\) for \\(a \\in \\mathbb{N}\\). \n\nProposition 7.9 In a Galton-Watson (homogeneous) branching process with reproduction number \\(\\mu\\), the conditional expectation of the size of the size of the \\(k+1^{\\text{th}}\\) generation with respect to the size of the \\(k^{\n\\text{th}}\\) generation is a linear function of the size of the \\(k^{\n\\text{th}}\\) generation:\n\\[\\mathbb{E}\\Big[Z_{k+1} \\mid \\sigma(Z_k)\\Big] = \\mathbb{E}X^0_1 \\times Z_k = \\mu \\times Z_k\\]\n\n\n\nProof. On the event \\(\\Big\\{ Z_k = a\\Big\\}\\), we can determine the conditional distribution of \\(Z_{k+1}\\).\n\\[\n\\begin{array}{rl}\n\\{ Z_{k+1} = b \\wedge Z_k = a\\Big\\}\n  & = \\Big\\{ \\sum_{i=1}^a X^k_i = b \\wedge Z_k = a\\Big\\} \\\\\n  & = \\Big\\{ \\sum_{i=1}^a X^k_i  = b\\Big\\} \\cap \\Big\\{ Z_k = a \\Big\\}\n\\end{array}\n\\]\nwe have \\[\n\\begin{array}{rl}\nP \\Big\\{ Z_{k+1} = b \\mid Z_k = a\\Big\\} = P \\Big\\{ \\sum_{i=1}^a X^k_i  = b \\mid Z_k = a\\Big\\} = P \\Big\\{ \\sum_{i=1}^a X^k_i  = b \\Big\\}\n\\end{array}\n\\]\nOn the event \\(\\{Z_k = a\\}\\), \\(Z_{k+1}\\) is distributed like the sum of \\(a\\) independent copies of \\(X^0_1\\):\n\\[\n\\begin{array}{rl}\n\\mathbb{E} \\Big[ Z_{k+1}\\mid \\sigma(Z_k)\\Big]\n  & = \\sum_{a=0}^\\infty \\mathbb{E}_{P(\\mid Z_k=a)}\\Big[Z_{k+1}\\Big] \\times \\mathbb{I}_{Z_k=a} \\\\\n  & = \\sum_{a=0}^\\infty \\mathbb{E}_{P(\\mid Z_k=a)}\\Big[ \\sum_{i=1}^a X^k_i \\Big] \\times \\mathbb{I}_{Z_k=a} \\\\\n  & = \\sum_{a=0}^\\infty \\mathbb{E}\\Big[ \\sum_{i=1}^a X^k_i \\Big] \\times \\mathbb{I}_{Z_k=a} \\\\\n  & = \\sum_{a=0}^\\infty \\sum_{i=1}^a  \\mathbb{E}\\Big[ X^k_i \\Big] \\times \\mathbb{I}_{Z_k=a} \\\\\n  & = \\sum_{a=0}^\\infty a  \\mathbb{E} X^0_1 \\times \\mathbb{I}_{Z_k=a}  \\\\\n  & = \\mathbb{E} X^0_1 \\times Z_k \\, .\n\\end{array}\n\\]\n\n\nAn immediate corollary is:\n\\[\\mathbb{E}Z_k = (\\mathbb{E} X^0_1)^k  \\qquad\\text{forall } k\\geq 0\\,.\\]\nThe sequence of expected sizes of generations forms a geometric sequence.\nA Galton-Watson process is said to be sub-critical if the expectation of the offspring distribution is smaller than \\(1\\).\n\n\nProposition 7.10 (Extinction under sub-critical offspring distribution) The extinction probability of a sub-critical branching process is equal to \\(1\\).\n\n\n\nProof. Denote by \\(E_k\\) the event \\(\\{ Z_k = 0\\}\\). Observe that the sequence \\((E_k)_k\\) is increasing. Denote by \\(E_{\\infty} = \\cup_{k=0}^\\infty E_k\\).\n\\[P \\{ E_k^c \\} = P\\{ Z_k \\geq 1 \\} \\leq \\mathbb{E} Z_k \\,.\\]\nHence \\(P\\{E_k^c\\} \\downarrow 0\\) and \\(P\\{ E_k\\} \\uparrow 1\\). By monotone convergence \\(P(E_\\infty) = 1\\).\n\n\nThe expected size of the total progeny of subcritical branching process is equal to \\[\n\\sum_{k=0}^\\infty \\mathbb{E} Z_k  = \\sum_{k=0}^\\infty (\\mathbb{E} X^0_1)^k = \\frac{1}{1 - \\mathbb{E} X^0_1} \\, .\n\\]\n\nWorking with discrete conditioning allows us to derive non-trivial statements about the Galton-Watson process without knowing much about the offspring distribution beyond the fact that its expectation is smaller than \\(1\\). We still ignore the the details of the distribution of \\(Z_k\\), let alone of the distribution of \\(\\sum_{k=0}^\\infty Z_k\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discrete Conditioning</span>"
    ]
  },
  {
    "objectID": "04-characterizations.html",
    "href": "04-characterizations.html",
    "title": "8  Characterizations of probability distributions",
    "section": "",
    "text": "8.1 Motivation\nIn full generality, a probability distribution is a complex and opaque object. It is a \\([0,1]\\)-valued function defined over a \\(\\sigma\\)-algebra of subsets. A concrete \\(\\sigma\\)-algebra, let alone the abstract notion of \\(\\sigma\\)-algebra, is not easily grasped. Looking for simpler characterizations of probability distributions is a sensible goal. When facing questions like: ``are two probability distributions equal?“, we know it suffices to check that the two distributions coincide on generating families of events. This makes Cumulative Distribution Functions (CDFs) precious tools. Cumulative Distribution Functions and their generalized inverse functions (quantile functions) are very convenient when handling maxima, minima, or more generally order statistics of collections of independent random variables, but when it comes to handling sums of independent random variables or branching processes, cumulative distribution functions are of moderate help.\nIn this lesson, we review three related ways of characterizing probability distributions through functions defined on the real line: Probability Generating Functions (Section 8.2)), Laplace transforms (Section 8.3)) and characteristic functions which extend Fourier transforms to probability distributions (Section 8.4)). The three methods are distinct in scope but they rely on the same idea and share common features.\nIndeed, Probability Generating Functions can be seen as special case of Laplace transforms. The latter can be seen as special cases of Fourier transforms. All three methods do characterize probability distributions. They are equipped with inversion formulae.\nThe three methods provide us with a seamless treatment of sums of independent random variables.\nAll three methods relate the integrability of probability distributions and the smoothness of transforms.\nIn the next lessons (Chapter 12), we shall see that the three transforms characterize convergence in distribution.\nProbability generating functions, Laplace transforms and characteristic functions deliver an important analytical machinery to Probability Theory. From Analysis, we get off-the-shelf arguments to establish smoothness properties of transforms, and with little more work, we can construct the inversion formulae.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Characterizations of probability distributions</span>"
    ]
  },
  {
    "objectID": "04-characterizations.html#sec-PGFbis",
    "href": "04-characterizations.html#sec-PGFbis",
    "title": "8  Characterizations of probability distributions",
    "section": "8.2 Probability generating function",
    "text": "8.2 Probability generating function\nIn this section, \\(X\\) is an integer-valued random variable, with distribution \\(P\\), cumulative distribution function \\(F\\) and probability mass function \\(p\\). Recall that \\(P\\) is completely characterized by the much simpler objects \\(F\\) and \\(p\\). Now, let \\(Y\\) be another integer-valued random variable living on the same probability space as \\(X\\), independent from \\(X\\), with distribution \\(Q\\), distribution function \\(G\\) and probability mass function \\(q\\). What can we tell about the distribution of \\(X+Y\\)? Is it easy to figure out its cumulative distribution function, its probability mass function?\nThe probability mass function of (the distribution of) \\(X+Y\\) is the convolution of \\(p\\) and \\(q\\)\n\\[\\begin{array}{rl}\\mathbb{P}\\{ X + Y = n\\}  & = \\sum_{k=0}^n \\mathbb{P}\\{ X + Y = n \\wedge X = k\\} \\\\  & = \\sum_{k=0}^n \\mathbb{P}\\{ Y = n - k \\wedge X = k\\}  \\\\  & = \\sum_{k=0}^n \\mathbb{P}\\{ Y = n - k\\} \\times  \\mathbb{P}\\{ X = k\\}\\\\  & = \\sum_{k=0}^n p(k) \\times q(n-k) \\\\  & =  p \\star q (n) \\, ,\\end{array}\\]\nwhere the third equality comes from independence of \\(\\sigma(X)\\) and \\(\\sigma(Y)\\).\nBesides the probability mass function, another function characterizes probability distributions and delivers instantaneous information about the distribution of sums of independent integer-valued random variables and many other things.\n\nDefinition 8.1 (Probability Generating Function) The probability generating function (PGF) of a probability distribution over \\(\\mathbb{N}\\), defined by its probability mass function (PMF) \\(p\\) is the function \\(G: [0,1] \\to \\mathbb{R}\\) defined by:\n\\[G(s) =  \\sum_{n=0}^\\infty p(n) s^n\\, .\\]\n\n\nExample 8.1 The probability generating function of basic discrete distributions is easily computed. The results are useful and suggestive.\n\nBernoulli distribution with parameter \\(p\\): \\[1 - p + p s = 1 + p (s-1)\\]\nBinomial distribution with parameters \\(n\\) and \\(p\\):\n\n\\[\\sum_{k=0}^n \\binom{n}{k} p^k (1-p)^{n-k} s^k =  \\left(ps + 1- p\\right)^n = \\left( 1 + p(s-1)\\right)^n\\]\n\nPoisson distribution with parameter \\(\\mu\\):\n\n\\[\\sum_{n=0}^\\infty \\mathrm{e}^{-\\mu} \\frac{\\mu^n}{n!} s^n = \\mathrm{e}^{\\mu (s-1)} \\,.\\]\n\nThe next observation follows almost immediately from the definition of probability generating functions.\n\nProposition 8.1 A probability generating function \\(G\\) satisfies the following conditions:\n\n\\(G\\) is non-negative over \\([0,1]\\);\n\\(G(0) = P\\{0\\}, \\quad G(1)=1\\);\n\\(G\\) is non-decreasing over \\([0,1]\\);\n\\(G\\) is continuous and convex.\n\n\n\n\n\n\nProof. Properties 1), 2) and 3) are obvious: \\(G\\) is a convex combination of non-negative, non-decreasing, continuous and convex functions.\n\\(\\Box\\)\n\n\n\n\nGeneratingfunctionology lies at the crossing between combinatorics, real analysis, complex analysis, and probability theory. Defining PGF as a power series brings within probability theory a collection of theorems that facilitate the identification of probability distributions or that connect integrability properties of the probability distribution with smoothness properties of the PGF.\nKeep in mind that a generating function defines a function from the set of complex numbers \\(\\mathbb{C}\\) to \\(\\mathbb{C}\\):\n\\[G(z) = \\sum_{n=0}^\\infty p(n) z^n \\qquad\\text{for all } z \\in \\mathbb{C} \\text{ such that the series converges}\\, .\\]\nCharacterizing the domain of a function defined in that way is crucial. The next proposition is at the core of Power Series theory.\n\nProposition 8.2 The radius of convergence of the generating function \\(G\\)\n\\[\nG(z)\\ = \\sum_{n\\in \\mathbb{N}} p(n) z^n, \\qquad z \\in \\mathbb{C}\n\\]\nis the unique \\(R \\in [0, \\infty) \\cup \\{+ \\infty\\}\\) such that:\n\nfor every \\(z \\in \\mathbb{C}\\) with \\(|z| &gt; R\\), the series \\(\\sum_{n\\in \\mathbb{N}} p(n) z^n\\) diverges.\nfor every \\(z \\in \\mathbb{C}\\) with \\(|z| &lt; R\\), the series \\(\\sum_{n\\in \\mathbb{N}} p(n) z^n\\) is absolutely convergent.\n\nThe open disk \\(\\{ z : z \\in \\mathbb{C}, |z| &lt; R \\}\\) is called the disk of convergence of \\(G\\). The circle \\(\\{ z : z \\in \\mathbb{C}, |z| = R \\}\\) is called the circle of convergence of \\(G\\).\nThe radius of convergence \\(R\\) of the probability generating function \\(G(z) = \\sum_{n \\in \\mathbb{N}} p(n)z^n\\) satisfies\n\\[\n\\frac{1}{R} =  \\limsup_n (p(n))^{1/n} \\, .\n\\]\n\nThe last statement is called Hadamard’s rule for determination of the radius of convergence:\nThe radius of convergence of a probability generating function is always at least \\(1\\).\n\nProof. Let \\(R\\) be the supremum of all real numbers \\(r\\) such that for every \\(z \\in \\mathbb{C}\\) with \\(|z| &lt; r\\), the series \\(\\sum_{n\\in \\mathbb{N}} p(n) z^n\\) is absolutely convergent.\nAs \\((p(n))_n\\) defines an absolutely convergent series, for every \\(z\\) with \\(|z|\\leq 1\\), \\(\\sum_n |p(n)z^n| \\leq \\sum_n p(n) =1\\). Hence \\(R\\geq 1\\).\n\\(\\Box\\)\n\n\nExample 8.2 The radius of convergence contains qualitative information about tail behavior:\n\nFor Poisson distributions, the radius of convergence is infinite. This reflects the fast decay of the tail probability of Poisson distributions.\nFor geometric distributions, \\(p(n) = q (1-q)^{n-1}\\), the radius of convergence is \\(1/(1-q)\\).\nFor power law distributions like \\(p(n) = n^{-r}/\\zeta(r)\\) with \\(r&gt;1\\), the radius of convergence is exactly \\(1\\).\n\n\nJust knowing the radius of convergence of a function defined by a Power Series expansion tells us about the smoothness properties of the function.\n\nTheorem 8.1 If \\(G\\) is defined as a power series \\(G(z) = \\sum_{n \\in \\mathbb{N}} a_n z^n\\) its (complex) derivative is \\(G'(z)=  \\sum_{n \\in \\mathbb{N}} (n+1) a_{n+1} z^n\\). The derivative \\(G'\\) and \\(G\\) have the same radius of convergence.\n\nThis general statement about power series entails a very useful corollary for probability generating functions.\n\nCorollary 8.1 (Inversion formula) Let \\(f\\) be the probability generating function associated with the probability mass function \\(p\\). Then \\(f\\) is infinitely many times differentiable over \\([0,1)\\)\n\\[f^{(n)}(s) =  \\sum_{k=n}^\\infty \\frac{k!}{(k-n)!} \\times p(k) s^{k-n} \\, ,\\]\nmore specifically:\n\\[f^{(n)}(0) =  n! \\times p(n) \\, .\\]\nA probability distribution over \\(\\mathbb{N}\\) is characterized by its probability generating function.\n\n\n\n\n\nProof. The property is true for \\(n=0\\).\nAssume it holds for all integers up to \\(n\\). For \\(s\\in [0, 1)\\) and \\(|h|&lt;1-s-\\delta\\) where \\(\\delta\\) is a small positive number,\n\\[\\begin{array}{rl}\n  \\frac{f^{(n)}(s+h) - f^{(n)}(s)}{h}\n  & =  \\sum_{k=n}^\\infty \\frac{k!}{(k-n)!} \\times p(k)\n    \\Big(\\sum_{j=0}^{k-n-1}(s+h)^{k-n-1-j} s^{j}  \\Big)\n\\end{array}\\]\nThe absolute value of the internal sum is smaller than \\((k-n) (1-\\delta)^{k-n-1}\\). As\n\\[\\sum_{k=n}^\\infty \\frac{k!}{(k-n-1)!} \\times p(k) \\times  (1-\\delta)^{k-n-1} &lt; \\infty\\]\nfor all \\(0&lt;\\delta&lt;1\\). By the Dominated Convergence Theorem,\n\\[\\begin{array}{rl}\n\\lim_{h \\to 0} \\frac{f^{(n)}(s+h) - f^{(n)}(s)}{h}\n    & =\n   \\sum_{k=n+1}^\\infty \\frac{k!}{(k-n-1)!} \\times p(k) \\times s^{k-n-1} \\,.\n\\end{array}\\]\n\\(\\Box\\)\n\n\n\n\n\nThe Probability Generating Function of a Poisson distribution with parameter \\(\\mu\\) equals \\(\\exp(\\mu(s-1))\\). If we meet a probability distribution with such a PGF, we know it is a Poisson distribution.\n\n\n\n\nProbability Generating Functions allow for easy investigations of sums of independent random variables.\n\nProposition 8.3 Let \\(X\\), \\(Y\\) be independent integer-valued random variable, with probability generating functions \\(G_X\\) and \\(G_Y\\). The probability generating function \\(G_{X+Y}\\) of \\(X+Y\\) is \\(G_X\\times G_Y\\):\n\\[G_{X+Y} = G_X \\times G_Y \\, .\\]\n\n\n\n\n\nProof. The proof relies on the fact that non-negative convergent series is commutatively convergent.\n\\[\\begin{array}{rl}\n\\sum_{n=0}^\\infty \\mathbb{P}\\{ X + Y = n\\}\\times s^n   & =\n    \\sum_{n=0}^\\infty \\left( \\sum_{k=0}^n p(k) q(n-k) \\right) s^n \\\\\n    &  =\n    \\sum_{k=0}^\\infty p(k) s^k  \\sum_{n\\geq k}^\\infty q(n-k) s^{n-k} \\\\\n    & = G_X(s) \\times G_Y(s)\n\\end{array}\\]\nIn measure theoretical language, the proposition is a consequence of the Tonelli-Fubini Theorem:\n\\[\\begin{array}{rl}\n  G_{X+Y}(s)\n  & = \\mathbb{E}\\left[s^{X+Y}\\right] \\\\\n  & = \\mathbb{E}\\left[s^{X} \\times s^{Y}\\right]  \\\\\n  & = \\int_{\\mathbb{R}^2} s^x s^y \\mathrm{d}P_X \\otimes P_Y(x,y) \\\\\n  & = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} s^x s^y \\mathrm{d}P_X(x) \\mathrm{d}P_Y(y) \\\\\n  & = \\int_{\\mathbb{R}} s^y \\int_{\\mathbb{R}} s^x  \\mathrm{d}P_X(x) \\mathrm{d}P_Y(y) \\\\\n  & = \\int_{\\mathbb{R}} s^y G_X(s) \\mathrm{d}P_Y(y) \\\\\n  & = G_X(s) \\times G_Y(s) \\, .\n\\end{array}\\]\n\n\n\n\n\nExample 8.3 If \\(X\\) and \\(Y\\) are independent Poisson random variables with parameters \\(\\mu\\) and \\(\\nu\\), then \\(G_{X+Y}(s) = \\exp(\\mu(s-1))\\times \\exp(\\nu(s-1))=  \\exp((\\mu+\\nu)(s-1))\\). This is (another) proof that \\(X+Y\\) is Poisson distributed with parameter \\(\\mu+\\nu\\).\n\nA PGF is infinitely many times differentiable inside the (open) disk of convergence. If the radius of convergence is larger than \\(1\\) (as for Poisson distributions), this entails that the PGF is infinitely many times differentiable at \\(1\\), If the radius of convergence is exactly \\(1\\), the differentiability on the circle of convergence is not prescribed by general theory.\n\nTheorem 8.2 (Integrability and probability generating functions) Let \\(X\\) be an integer-valued random variable, with probability generating functions \\(f\\), then\n\\(\\mathbb{E} X^p &lt; \\infty\\)\niff\n\\(f\\) is \\(p\\)-times differentiable at \\(1\\) and\n\\[f^{(p)}(1) = \\mathbb{E}\\left[X (X-1) \\ldots (X-p+1)\\right] \\, .\\]\n\n\n\n\n\nProof. Assume that \\(G\\) is \\(p\\)-times differentiable on the left at \\(1\\).\nWe need to establish that \\(|X|\\) is \\(p\\)-integrable.\nAssume that \\(|X|\\) is \\(p\\)-integrable.\n\n\n\n\nThe next question arises quickly: when is a function from \\([0,1]\\) to \\([0,\\infty)\\) a probability generating function? This question is addressed in a broader perspective in the next section.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Characterizations of probability distributions</span>"
    ]
  },
  {
    "objectID": "04-characterizations.html#sec-laplace",
    "href": "04-characterizations.html#sec-laplace",
    "title": "8  Characterizations of probability distributions",
    "section": "8.3 Laplace transform",
    "text": "8.3 Laplace transform\nLaplace transforms characterize probability distributions on \\([0, \\infty).\\)\n\n8.3.1 Definition and elementary properties\n\nDefinition 8.2 Let \\(P\\) be a probability distribution function over \\([0,\\infty]\\) with cumulative distribution function \\(F\\). The Laplace transform of \\(P\\) is the function \\(U\\) from \\([0,\\infty)\\) to \\([0,1]\\) defined by\n\\[U(\\lambda) =  \\mathbb{E}\\left[\\mathrm{e}^{- \\lambda X}\\right] = \\int_{[0,\\infty)} \\mathrm{e}^{- \\lambda x} \\mathrm{d}F(x) \\,\\]\nwhere \\(X \\sim P\\).\n\n\nA probability distribution \\(P\\) over \\(\\mathbb{N}\\) is also a probability distribution over \\([0,\\infty)\\), as such it has both a probability generating function \\(G\\) and a Laplace transform \\(U\\). They are connected by\n\\[U(\\lambda) =  G(\\mathrm{e}^{-\\lambda}) \\, .\\]\nWhich properties of Probability Generating Functions are also satisfied by Laplace transforms?\n\n\nProposition 8.4 If \\(U: [0,\\infty) \\to [0,1]\\) is the Laplace transform of a probability distribution \\(P\\) over \\([0, \\infty)\\), then\n\n\\(U(0)=1\\);\n\\(U\\) is continuous;\n\\(U\\) is non-increasing.\n\\(U\\) is convex.\n\n\n\nExercise 8.1 Check the assertions in the proposition.\n\nCan we recognize Laplace transform of probability distributions over \\([0,\\infty)\\)? This is the content of the next Theorem (which proof is beyond the reach of this course).\n\nTheorem 8.3 (Bernstein’s Theorem) A function \\(U: (0, \\infty) \\to (0,\\infty)\\) is the Laplace transform of a probability distribution over \\([0,\\infty)\\) iff\n\n\\(U\\) is infinitely many times differentiable over \\((0, \\infty)\\)\n\\(U(0)=1\\)\n\\(U\\) is completely monotonous: \\((-1)^k U^{(k)} \\geq 0\\) over \\((0, \\infty)\\)\n\n\nUsing the connexion between Probability Generating Functions and Laplace transforms, we are in position to characterize those power series that are Probability Generating Functions.\n\nCorollary 8.2 A function \\(G: [0, 1] \\to [0,1]\\) is the Probability Generating Function of a probability distribution over \\(\\mathbb{N}\\) iff\n\n\\(G\\) is infinitely many times differentiable over \\((0,1)\\)\n\\(G(1)=1\\)\n\\(G\\) is completely monotonous: \\((-1)^k G^{(k)} \\geq 0\\) over \\((0, 1)\\)\n\n\n\nExample 8.4 Let \\(X\\) be \\(\\text{Gamma}(p, \\nu)\\)-distributed. The Laplace transform of (the distribution of) \\(X\\) is\n\\[\\begin{array}{rl}\nU(\\lambda)\n  & = \\int_0^\\infty \\nu \\mathrm{e}^{-\\lambda x} \\mathrm{e}^{-\\nu x} \\frac{(\\nu x)^{p-1}}{\\Gamma(p)} \\mathrm{d} x \\\\\n  & = \\frac{\\nu^p}{(\\lambda +\\nu)^p} \\int_0^\\infty (\\lambda +\\nu) \\mathrm{e}^{-(\\lambda +\\nu) x}  \\frac{((\\nu+\\lambda) x)^{p-1}}{\\Gamma(p)} \\mathrm{d} x \\\\\n  & = \\frac{\\nu^p}{(\\lambda +\\nu)^p} \\, .\n\\end{array}\\]\n\n\n\n8.3.2 Injectivity of Laplace transforms and an inversion formula\n\nTheorem 8.4 (Widder’s Theorem) A probability distribution on \\([0, \\infty)\\) is characterized by its Laplace transform.\n\nThe construction of the inversion formula relies on deviation inequalities for Poisson distribution. The next proposition is easily checked by using Markov’s inequality with exponential functions and optimization.\n\nTheorem 8.5 (Tail bounds for Poisson distribution) Let \\(Z\\) be Poisson distributed. Let \\(h(x) = \\mathrm{e}^x - x -1\\) and \\(h^*(x)= (x+1)\\log (x+1) -x, x\\geq -1\\) be its convex dual. Then for all \\(\\lambda \\in \\mathbb{R}\\)\n\\[\\log \\mathbb{E} \\mathrm{e}^{\\lambda (Z-\\mathbb{E}Z)} = \\mathbb{E}Z h(\\lambda) \\, .\\]\nFor \\(t\\geq 0\\) \\[\n\\Pr \\Big\\{ Z \\geq \\mathbb{E}Z + t \\Big\\} \\leq \\mathrm{e}^{-\\mathbb{E}Z h^*\\Big(\\frac{t}{\\mathbb{E}Z}\\Big)}\n\\] and for \\(0 \\leq t \\leq \\mathbb{E}Z\\) \\[\n\\Pr \\Big\\{ Z \\leq \\mathbb{E}Z -t \\Big\\} \\leq \\mathrm{e}^{-\\mathbb{E}Z h^*\\Big(\\frac{-t}{\\mathbb{E}Z}\\Big)} \\, .\n\\]\n\n\nRemark 8.1. \n\nSee Section 3.7) for the notion of convex duality.\nThe next bounds on \\(h^*\\) deliver looser but easier tail bounds\n\n\\[\\begin{array}{rll}\n  h^*(t) & \\geq \\frac{t^2}{2(1 + t/3)}    & \\text{for } t &gt;0 \\\\\n  h^*(t) & \\geq \\frac{t^2}{2}             & \\text{for } t &lt;0 \\, .\n\\end{array}\\]\n\n\nCorollary 8.3 For all positive \\(x, y, y \\neq x\\), \\[\n\\lim_{n \\to \\infty} \\sum_{k=0}^{nx} e^{-n y} \\frac{(ny)^k}{k!} = \\mathbb{I}_{y&lt;x} \\,.\n\\]\n\nWe shall check in one of the next lessons that for \\(x &gt;0\\): \\[\n\\lim_{n \\to \\infty} \\sum_{k=0}^{\\lfloor nx\\rfloor} e^{-n x} \\frac{(nx)^k}{k!} = \\frac{1}{2} \\, .\n\\]\n\n\nProof. Let \\(F\\) be the cumulative distribution function of \\(P\\) and \\(U\\) its Laplace transform. Let \\(X \\sim P\\).\nIt suffices to show that \\(F(x)\\) can be computed from \\(U\\) at any \\(x\\) where \\(F\\) is continuous.\nFunction \\(U\\) is infinitely many times differentiable on \\((0, \\infty)\\). For \\(k\\in  \\mathbb{N},\\) \\[\n    \\frac{\\mathrm{d}^kU}{\\mathrm{d}\\lambda^k}  = (-1)^k \\int_{[0,\\infty)} x^k e^{-\\lambda x} \\mathrm{d}F(x)  \\, .\n\\] and \\(U\\) has a power series expansion at every \\(\\lambda \\in (0,1)\\), for \\(\\lambda' \\in (0,1)\\):\n\\[\\begin{array}{rl}\nU(\\lambda')\n   & = \\sum_{k=0}^\\infty \\frac{(\\lambda' -\\lambda)^k}{k!} \\frac{\\mathrm{d}^kU}{\\mathrm{d}\\lambda^k}  \\, .\n\\end{array}\\]\nBy [Corollary 8.3), for all \\(0 &lt; y \\neq x\\), \\(\\lim_{n \\to \\infty} \\sum_{k=0}^{nx} e^{-n y} \\frac{(ny)^k}{k!} = \\mathbb{I}_{y&lt;x}\\).\n\\[\\begin{array}{rl}\nF(x)\n  & =  \\int_{\\mathbb{R_+}} \\mathbb{I}_{y\\leq x} \\mathrm{d}F(y) \\\\\n  & = \\int_{\\mathbb{R_+}} \\mathbb{I}_{y&lt; x} \\mathrm{d}F(y) \\\\\n  & = \\int_{(-\\infty, x)} \\mathbb{I}_{y&lt; x} \\mathrm{d}F(y) + \\int_{\\{x\\}} 1 \\mathrm{d}F(y) + \\int_{(x, \\infty)} \\mathbb{I}_{y&lt; x} \\mathrm{d}F(y) \\\\\n  & = \\int_{(-\\infty, x)} \\mathbb{I}_{y&lt; x} \\mathrm{d}F(y) + \\int_{\\{x\\}} 1 \\mathrm{d}F(y) + \\int_{(x, \\infty)} \\mathbb{I}_{y&lt; x} \\mathrm{d}F(y) \\\\\n  & =  \\int_{(-\\infty, x) \\cup (x, \\infty)} \\lim_{n \\to \\infty} \\sum_{k=0}^{nx} e^{-n y} \\frac{(ny)^k}{k!} \\mathrm{d}F(y) + \\int_{\\{x\\}} 1 \\mathrm{d}F(y) \\\\\n  & =  \\lim_{n \\to \\infty} \\sum_{k=0}^{nx} \\frac{(-n)^k}{k!}\\int_{(-\\infty, x) \\cup (x, \\infty)}  e^{-n y} {(-y)^k} \\mathrm{d}F(y) + \\int_{\\{x\\}} 1 \\mathrm{d}F(y)\\\\\n       &  \\text{by dominated convergence} \\\\\n  & =  \\lim_{n \\to \\infty} \\sum_{k=0}^{nx} \\frac{(-n)^k}{k!} \\frac{\\mathrm{d}^kU}{\\mathrm{d}\\lambda^k}_{\\mid \\lambda=n}  \\, .\n\\end{array}\\]\nIf \\(F\\) is continuous at \\(x\\), \\[\nF(x) = \\lim_{n \\to \\infty} \\sum_{k=0}^{nx} \\frac{(-n)^k}{k!} \\frac{\\mathrm{d}^kU}{\\mathrm{d}\\lambda^k}_{\\mid \\lambda=n} \\, .\n\\] If \\(F\\) jumps at \\(x\\), \\[\nF(x) - \\frac{P\\{X=x\\}}{2} =\\lim_{n \\to \\infty} \\sum_{k=0}^{nx} \\frac{(-n)^k}{k!} \\frac{\\mathrm{d}^kU}{\\mathrm{d}\\lambda^k}_{\\mid \\lambda=n} \\, .\n\\] This process shows that the Laplace transform contains enough information to reconstruct the distribution function which in turn characterizes the probability distribution.\n\n\nLaplace transforms of sums of independent non-negative random variables are easily obtained from the Laplace transforms of the summands.\n\n\nProposition 8.5 Let \\(X\\) and \\(Y\\) be two independent \\([0,\\infty)\\)-valued random variables, with Laplace transforms \\(U_X\\) and \\(U_Y\\). The Laplace transform of (the distribution of) \\(X+Y\\) is \\[\nG_{X+Y} = G_X \\times G_Y \\, .\n\\]\n\n\nProof. \\[\\begin{array}{rl}\nG_{X+Y}(\\lambda)\n  & = \\mathbb{E}\\Big[\\mathrm{e}^{\\lambda (X+Y)}\\Big] \\\\\n  & = \\mathbb{E}\\Big[\\mathrm{e}^{\\lambda X} \\times \\mathrm{e}^{\\lambda Y}\\Big] \\\\\n  & = \\mathbb{E}\\Big[\\mathrm{e}^{\\lambda X} \\Big] \\times \\mathbb{E}\\Big[\\mathrm{e}^{\\lambda Y}\\Big]\\\\\n  & \\text{independence}\\\\\n  & = G_X(\\lambda) \\times G_Y(\\lambda) \\, .\n\\end{array}\\]\n\n\n\n\nCombining the inversion theorem and the explicit formula for the Laplace transform of Gamma distributions, we recover the fact that sums of independent Gamma-distributed random variables with the same intensity parameter is also Gamma distributed.\n\nIf \\(X \\sim \\text{Gamma}(p, \\lambda)\\) is independent from \\(Y \\sim \\text{Gamma}(q, \\lambda)\\) then \\(X+Y\\) has Laplace transform \\(\\Big(\\frac{\\nu}{\\lambda+\\nu}\\Big)^{p+q}\\) and is \\(\\text{Gamma}(p+q, \\lambda)\\)-distributed.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Characterizations of probability distributions</span>"
    ]
  },
  {
    "objectID": "04-characterizations.html#sec-charfun",
    "href": "04-characterizations.html#sec-charfun",
    "title": "8  Characterizations of probability distributions",
    "section": "8.4 Characteristic functions and Fourier transforms",
    "text": "8.4 Characteristic functions and Fourier transforms\nThe Laplace transform characterizes probability distributions supported by \\([0, \\infty)\\). Characteristic functions deal with general probability distributions. They extend to multivariate distributions.\n\n8.4.1 Characteristic function\nThe next transform can be defined for all probability distributions over \\(\\mathbb{R}\\). And the definition can be extended to distributions on \\(\\mathbb{R}^k, k\\geq 1\\).\n\nLet the real-valued random variable \\(X\\) be distributed according to \\(P\\) with cumulative distribution function \\(F\\), the characteristic function of distribution \\(P\\) is the function from \\(\\mathbb{R}\\) to \\(\\mathbb{C}\\) defined by \\[\n    \\widehat{F}(t) = \\mathbb{E}\\left[\\mathrm{e}^{i t X}\\right]\n  = \\int_{\\mathbb{R}} \\mathrm{e}^{i t x} \\mathrm{d}F(x)\n  = \\int_{\\mathbb{R}} \\cos(t x) \\mathrm{d}F(x) + i \\int_{\\mathbb{R}} \\sin(t x) \\mathrm{d}F(x) \\, .\n\\]\n\n\nIf \\(F\\) is absolutely continuous with density \\(f\\) then \\(\\widehat{F}\\) is (up to a multiplicative constant) the Fourier transform of \\(f\\).\n\n\nLet the real-valued random variable \\(X\\) be distributed according to \\(P\\) with characteristic function \\(\\widehat{F}\\).\n\n\\(\\widehat{F}\\) is (uniformly) continuous over \\(\\mathbb{R}\\)\n\\(\\widehat{F}(0)=1\\)\nIf \\(X\\) is symmetric, \\(\\widehat{F}\\) is real-valued\nThe characteristic function of the distribution of \\(a X +b\\) is \\[\\mathrm{e}^{it b} \\widehat{F}(at) \\, .\\]\n\n\n\nProof. Let us check the continuity property. The three others are left as exercises.\nTrigonometric calculus leads to \\[\\begin{array}{rl}\n\\Big| \\mathrm{e}^{i(t+ \\delta)x} - \\mathrm{e}^{itx}\\Big|\n  & = \\Big| \\mathrm{e}^{itx}\\Big| \\times \\Big|\\mathrm{e}^{i\\delta x} - 1\\Big|\\\\\n  & \\leq \\Big|\\mathrm{e}^{i\\delta x} - 1\\Big| \\\\\n  & \\leq 2 \\Big( 1 \\wedge \\big| \\delta x \\big| \\Big)\n\\end{array}\\] for every \\(t\\in \\mathbb{R}, \\delta \\in \\mathbb{R}, x \\in \\mathbb{R}\\). Taking integration with respect to \\(F\\), \\[\\begin{array}{rl}\n  \\Big| \\widehat{F}(t+\\delta) - \\widehat{F}(t)\\Big|\n  & \\leq \\int 2 \\Big( 1 \\wedge \\big| \\delta x \\big| \\Big) \\mathrm{d}F(x) \\,.\n\\end{array}\\] Resorting to the dominated convergence theorem, we conclude \\[\n\\lim_{\\delta \\to 0} \\Big| \\widehat{F}(t+\\delta) - \\widehat{F}(t)\\Big| = 0\n\\] uniformly in \\(t\\).\n\n\n\n\n\nThe next properties are easily checked:\n\n\\(|\\widehat{F}(t)|\\leq 1\\) for every \\(t\\in \\mathbb{R}\\);\n\n\n\n\n\n\n\nCompute the characteristic function of:\n\nThe Poisson distribution with parameter \\(\\lambda&gt;0\\);\nThe uniform distribution on \\([-1,1]\\);\nThe triangle distribution on \\([-1,1]\\) (density: \\(1-|x|\\) on \\([-1,1]\\));\nThe exponential distribution with density \\(\\exp(-x)\\) on \\([0,+\\infty)\\);\nThe Laplace distribution, density \\(1/2 \\exp(-|x|)\\).\n\n\n\n\n\nJust as Probability Generating Functions and Laplace transforms, Characteristic functions of sums of independent random variables have a simple structure.\n\nProposition 8.6 Let \\(X\\) and \\(Y\\) be independent random variables with cumulative distribution functions \\(F_X\\) and \\(F_Y\\), then\n\\[\\widehat{F}_{X+Y}(t) =  \\widehat{F}_X(t) \\times \\widehat{F}_Y(t)\\]\nfor all \\(t \\in \\mathbb{R}\\).\n\n\nProof. The third equality is a consequence of the independence of \\(X\\) and \\(Y\\): \\[\\begin{array}{rl}\n\\widehat{F}_{X+Y}(t)\n  & = \\mathbb{E}\\Big[\\mathrm{e}^{it (X+Y)}\\big] \\\\\n  & = \\mathbb{E}\\Big[\\mathrm{e}^{it X} \\mathrm{e}^{it Y}\\big] \\\\\n  & = \\mathbb{E}\\Big[\\mathrm{e}^{it X} \\big] \\times \\mathbb{E}\\big[\\mathrm{e}^{it Y}\\big] \\\\\n  & =  \\widehat{F}_X(t) \\times \\widehat{F}_Y(t) \\, .\n\\end{array}\\]\n\n\nUse a counter-example to prove that \\[\n\\Big(\\forall t \\in \\mathbb{R}, \\quad \\widehat{F}_{X+Y}(t) =  \\widehat{F}_X(t) \\times \\widehat{F}_Y(t) \\Big)\n\\not\\Rightarrow X \\perp\\!\\!\\!\\perp Y \\, .\n\\]\n\n\n\n8.4.2 Characteristic function of a univariate Gaussian distribution\nIt is possible to compute characteristic functions by resorting to Complex Analysis. But we shall refrain from this when computing the most important characteristic function, the characteristic function of the standard Gaussian distribution.\n\nProposition 8.7 Let \\(\\widehat{\\Phi}\\) denote the characteristic function of the standard univariate Gaussian distribution \\(\\mathcal{N}(0,1)\\), the following holds\n\\[\\widehat{\\Phi}(t) = \\mathrm{e}^{-\\frac{t^2}{2}} \\, .\\]\n\n\nProof. Recall that as the standard Gaussian density is even, the characteristic function is real-valued and even.\nMoreover, \\(\\widehat{\\Phi}\\) is differentiable and the derivative can be computing by interverting expectation and derivation with respect to \\(t\\). \\[\\begin{array}{rl}\n\\widehat{\\Phi}'(t)\n& = - \\mathbb{E}\\left[X \\sin(t X) \\right] \\\\\n& =  - \\frac{1}{\\sqrt{2 \\pi}}\\int_{\\mathbb{R}} x \\sin(tx) \\mathrm{e}^{-\\frac{x^2}{2}} \\mathrm{d}x \\\\\n& = \\frac{1}{\\sqrt{2 \\pi}} \\Big[\\sin(tx) \\mathrm{e}^{-\\frac{x^2}{2}} \\Big]_{-\\infty}^{\\infty} - t \\frac{1}{\\sqrt{2 \\pi}}\\int_{\\mathbb{R}}  \\cos(tx) \\mathrm{e}^{-\\frac{x^2}{2}} \\mathrm{d}x \\\\\n& = - t \\widehat{\\Phi}(t) \\,.\n\\end{array}\\] Hence, \\(\\widehat{F}\\) is a solution of the differential equation: \\(g'(t) = -t g(t)\\) with \\(g(0)=1\\).\nThe differential equation is readily solved, and the solution is \\(g(t)= \\mathrm{e}^{- \\frac{t^2}{2}}\\).\n\n\n\n\n\nWhy is \\(\\widehat{\\Phi}\\) differentiable? Why are we allowed to interchange expectation and derivation?\n\n\n\n\nNote that a byproduct of Proposition @ref(prp:proCharFunGauss) is the following integral representation of the Gaussian density.\n\\[\\phi(x)  =  \\frac{1}{2 \\pi} \\int_{\\mathbb{R}} \\widehat{\\Phi}(t) \\mathrm{e}^{-itx} \\mathrm{d}t \\, .\\]\nIt does not look interesting, but it is a milestone for the derivation of the general inversion formula below.\n\n\n8.4.3 Sums of independent random variables and convolutions\nThe interplay between Characteristic functions/Fourier transforms and summation of independent random variables is one of the most attractive features of this transformation. In order to understand it, we shall need an operation stemming from analysis. Recall that if \\(f\\) and \\(g\\) are two integrable functions, the convolution of \\(f\\) and \\(g\\) is defined as \\[f \\star g (x)  = \\int_{\\mathbb{R}} f(x-y)g(y) \\mathrm{d}y = \\int_{\\mathbb{R}} g(x-y)f(y) \\mathrm{d}y \\, .\\] Note that \\(f \\star g\\) is also integrable. It is not too hard to check that if \\(f\\) and \\(g\\) are two probability densities then so is \\(f \\star g\\), moreover \\(f \\star g\\) is the density of the distribution of \\(X+Y\\) where \\(x \\sim f\\) is independent from \\(Y \\sim g\\). The next proposition extends this observation.\n\nProposition 8.8 Let \\(X,Y\\) be two independent random variables with distributions \\(P_X\\) and \\(P_Y\\). Assume that \\(P_X\\) is absolutely continuous with density \\(p_X\\). Then the distribution of \\(X+Y\\) is absolutely continuous and has density \\[\np_x \\star P_Y (z) = \\int_{\\mathbb{R}} p_X(z -y ) \\mathrm{d}P_Y(y) \\, .\n\\]\n\n\nProof. Let \\(B\\) be Borel subset of \\(\\mathbb{R}\\). \\[\\begin{array}{rl}\nP \\Big\\{ X+Y  \\in B\\Big\\}\n  & = \\int_{\\mathbb{R}} \\Big( \\int_{\\mathbb{R}} \\mathbb{I}_B(x+y) p_X(x)\\mathrm{d}x\\Big) \\mathrm{d}P_Y(y) \\\\\n  & = \\int_{\\mathbb{R}} \\Big(\\int_{\\mathbb{R}} \\mathbb{I}_B(z) p_X(z-y)\\mathrm{d}z\\Big) \\mathrm{d}P_Y(y) \\\\\n  & = \\int_{\\mathbb{R}} \\mathbb{I}_B(z) \\Big(\\int_{\\mathbb{R}}  p_X(z-y) \\mathrm{d}P_Y(y) \\Big) \\mathrm{d}z \\\\\n  & = \\int_{\\mathbb{R}} \\mathbb{I}_B(z) p_x \\star P_Y (z) \\mathrm{d}z\n\\end{array}\\] where the first equality follows from the Tonelli-Fubini Theorem, the second equality is obtained by change of variable \\(x \\mapsto z = x+y\\) for every \\(y\\), the third equality follows again from the Tonelli-Fubini Theorem.\n\n\n\nConvolution is not tied to Probability theory.\n\nIn Analysis, convolution is known to be a regularizing (smoothing) operation. This also holds in Probability theory: if the distribution of either \\(X\\) or \\(Y\\) has a density and \\(X \\perp\\!\\!\\!\\perp Y\\), then the distribution of \\(X+Y\\) has a density.\nConvolution with smooth distributions plays an important role in non-parametric statsitics, it is at the root of kernel density estimation.\nConvolution is an important tool in Signal Processing.\n\n\n\nCheck that if \\(X\\) and \\(Y\\) are independent with densities \\(f_X\\) and \\(f_Y\\), \\(f_X \\star f_Y\\) is a density of the distribution of \\(X+Y\\).\n\n\nIf \\(Y =0\\) almost surely (its distribution is \\(\\delta_0\\)), then \\(p_X \\star \\delta_0 = p_X\\).\nWhat happens in Proposition @ref(prp:convsum), if we consider the distributions of \\(\\sigma X +Y\\) and let \\(\\sigma\\) decrease to \\(0\\)?\n\nProposition 8.9 Let \\(X,Y\\) be two independent random variables with distributions \\(P_X\\) and \\(P_Y\\). Assume that \\(P_X\\) is absolutely continuous with density \\(p_X\\) and that \\(P_X(-\\infty, 0] = \\alpha \\in (0,1)\\). Then \\[\n\\lim_{\\sigma \\downarrow 0} \\mathbb{P}\\big\\{ Y + \\sigma X \\leq a \\Big\\} = P_Y(-\\infty, a) + \\alpha P_Y\\{a\\} \\, .\n\\]\n\n\nProof. \\[\\begin{array}{rl}\n\\mathbb{P}\\big\\{ Y + \\sigma X \\leq a \\Big\\}\n    & = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} \\mathbb{I}_{x \\leq  \\frac{a-y}{\\sigma}} p_X(x) \\mathrm{d}x \\mathrm{d}P_Y(y) \\\\\n    & = \\int_{(-\\infty,a)} \\int_{\\mathbb{R}} \\mathbb{I}_{x \\leq  \\frac{a-y}{\\sigma}} p_X(x) \\mathrm{d}x \\mathrm{d}P_Y(y) \\\\\n    & + \\int_{\\mathbb{R}} \\mathbb{I}_{x \\leq  \\frac{a-a}{\\sigma}} p_X(x) \\mathrm{d}x P_Y\\{a\\} \\\\\n    & + \\int_{(a, \\infty)} \\int_{\\mathbb{R}} \\mathbb{I}_{x \\leq  \\frac{a-y}{\\sigma}} p_X(x) \\mathrm{d}x \\mathrm{d}P_Y(y)\n\\end{array}\\] By monotone convergence, the first and third integrals converge respectively to \\(P_Y(-\\infty, a)\\) and \\(0\\) while the second term equals \\(\\alpha P_Y\\{a\\}\\).\n\n\n\n8.4.4 Injectivity Theorem and inversion formula\nThe characteristic function maps probability measures to \\(\\mathbb{C}\\)-valued functions. The main result of this section is that characteristic functions/Fourier transforms define is an injective operator on the set of Probability measures on the real line.\n\nTheorem 8.6 If two probability distribution \\(P\\) and \\(Q\\) have the same characteristic function, they are equal.\n\nThe injectivity property follows from an explicit inversion recipe. The characteristic function allows us to recover the cumulative distribution function at all its continuity points (just as the Laplace transform did). Again, as continuity points of cumulative distribution functions are dense on \\(\\mathbb{R}\\), this is enough.\n\nProposition 8.10 Let \\(X \\sim F\\) and \\(Z \\sim \\mathcal{N}(0,1)\\) be independent. Let \\(Y = X + \\sigma Z\\), then:\n\nthe distribution of \\(Y\\) has characteristic function \\[\n  \\widehat{F}_\\sigma(t) = \\widehat{\\Phi}(t\\sigma) \\times \\widehat{F}(t) = \\mathrm{e}^{- \\frac{t^2 \\sigma^2}{2}} \\widehat{F}(t)\n\\]\nthe distribution of \\(Y\\) is absolutely continuous with respect to Lebesgue measure\na version of the density of the distribution of \\(Y\\) is given by \\[\n  \\frac{1}{{2 \\pi}}\\int_{\\mathbb{R}} \\mathrm{e}^{- \\frac{t^2 \\sigma^2}{2}} \\widehat{F}(t)\\mathrm{e}^{-ity} \\mathrm{d}t\n= \\frac{1}{{2 \\pi}}\\int_{\\mathbb{R}}  \\widehat{F}_\\sigma(t)\\mathrm{e}^{-ity} \\mathrm{d}t \\,.\n\\]\n\n\n\nWhy can we take for granted the existence of a probability space with two independent random variables \\(X, Z\\) distributed as above?\n\nThe proposition states that a density of the distribution of \\(X + \\sigma Z\\) can be recovered from the characteristic function of the distribution of \\(X + \\sigma Z\\) by the Fourier inversion formula for functions with integrable Fourier transforms.\n\nProof. The fact that for any \\(\\sigma &gt;0\\), the distribution of \\(Y = X + \\sigma Z\\) is absolutely continuous with respect to Lebesgue measure comes from Proposition @ref(prp:convsum).\nA density of the distribution of \\(X + \\sigma Z\\) is given by \\[\n    \\int_{\\mathbb{R}} \\frac{1}{\\sigma} \\phi\\Big(\\frac{y -x}{\\sigma}\\Big) \\mathrm{d}F(x)\n\\] The characteristic function of \\(Y\\) at \\(t\\) is \\(\\mathrm{e}^{- \\frac{t^2 \\sigma^2}{2}} \\widehat{F}(t)\\).\n\\[\\begin{array}{rl}\n\\mathbb{P}\\Big\\{ Y \\leq u\\Big\\}\n& = \\int_{-\\infty}^u \\int_{\\mathbb{R}} \\frac{1}{\\sigma} \\phi\\Big(\\frac{y -x}{\\sigma}\\Big) \\mathrm{d}F(x)  \\mathrm{d}y \\\\\n& = \\int_{-\\infty}^u \\int_{\\mathbb{R}} \\frac{1}{\\sigma}\n\\left(\\frac{1}{{2 \\pi}} \\int_{\\mathbb{R}} \\mathrm{e}^{- \\frac{t^2}{2}} \\mathrm{e}^{-it  \\frac{y-x}{\\sigma}} \\mathrm{d}t\\right)\n\\mathrm{d}F(x)  \\mathrm{d}y \\\\\n& = \\int_{-\\infty}^u \\left(\\int_{\\mathbb{R}} \\frac{1}{\\sigma}\n\\frac{1}{{2 \\pi}} \\mathrm{e}^{- \\frac{t^2}{2}} \\mathrm{e}^{-\\frac{ity}{\\sigma}} \\left(\\int_{\\mathbb{R}}  \\mathrm{e}^{\\frac{itx}{\\sigma}} \\mathrm{d}F(x)\\right)  \\mathrm{d}t\\right)\n\\mathrm{d}y \\\\\n& = \\int_{-\\infty}^u \\left(\\int_{\\mathbb{R}} \\frac{1}{\\sigma}\n\\frac{1}{{2 \\pi}} \\mathrm{e}^{- \\frac{t^2}{2}} \\mathrm{e}^{-\\frac{ity}{\\sigma}} \\widehat{F}(t/\\sigma)  \\mathrm{d}t\\right)\n\\mathrm{d}y \\\\\n& = \\int_{-\\infty}^u  \\left( \\frac{1}{{2 \\pi}}\\int_{\\mathbb{R}} \\mathrm{e}^{- \\frac{t^2 \\sigma^2}{2}}\\mathrm{e}^{-ity} \\widehat{F}(t)\\mathrm{d}t   \\right) \\mathrm{d}y \\,.\n\\end{array}\\] The quantity \\(\\left( \\frac{1}{{2 \\pi}}\\int_{\\mathbb{R}} \\mathrm{e}^{- \\frac{t^2 \\sigma^2}{2}}\\mathrm{e}^{-ity} \\widehat{F}(t)\\mathrm{d}t   \\right)\\) is a version of the density of the distribution of \\(Y = X + \\sigma Z\\) (why?). Note that it is obtained from the same inversion formula that readily worked for the Gaussian density.\n\n\n\n\nNow we have to show that an inversion formula works for all probability distributions, not only for the smooth probability distributions obtained by adding Gaussian noise. We shall check that we can recover the distribution function from the Fourier transform.\n\nTheorem 8.7 Let \\(X\\) be distributed according to \\(P\\), with cumulative distribution function \\(F\\) and characteristic function \\(\\widehat{F}\\).\nThen: \\[\n\\lim_{\\sigma \\downarrow 0} \\int_{-\\infty}^u  \\left( \\frac{1}{{2 \\pi}}\\int_{\\mathbb{R}} \\mathrm{e}^{-ity}  \\mathrm{e}^{- \\frac{t^2 \\sigma^2}{2}}\\widehat{F}(t)\\mathrm{d}t   \\right) \\mathrm{d}y =  F(u_-) + \\frac{1}{2} P\\{u\\}\n\\] where \\[\nF(u_-) = \\lim_{v \\uparrow u} F(v) = P(-\\infty, u)\\, .\n\\]\n\n\nProof. The proof consists in combining Propositions @ref(prp:approxident) and @ref(prp:reginversion).\n\n\n\n\nNote that Theorem 8.7) does not deliver directly the distribution function \\(F\\). Indeed, if \\(F\\) is not continuous, \\(u \\mapsto \\widetilde{F}(u) = F(u_-) + \\frac{1}{2} P\\{u\\}\\), is not a distribution function. But the right-continuous modification of \\(\\widetilde{F}\\): \\(u \\mapsto \\lim_{v \\downarrow u} \\widetilde{F}(v)\\) coincides with \\(F\\). We have established Theorem 8.6).\n\n\n\nWhen the distribution function is absolutely continuous, Fourier inversion is simpler.\n\nLet \\(X\\) be distributed according to \\(P\\), with cumulative distribution function \\(F\\) and characteristic function \\(\\widehat{F}\\). Assume that \\(\\widehat{F}\\) is integrable (with respect to Lebesgue measure). Then:\n\n\\(P\\) is absolutely continuous with respect to Lebesgue measure;\n\\(y \\mapsto \\frac{1}{{2 \\pi}} \\int_{\\mathbb{R}} \\widehat{F}(t) \\mathrm{e}^{-ity} \\mathrm{d}t\\) is a uniformly continuous version of the density of \\(P\\).\n\n\n\nProof. Let \\(X\\) be distributed according to \\(P\\) with cumulative distribution function \\(F\\) and characteristic function \\(\\widehat{F}\\). Let \\(Z\\) be independent from \\(X\\) and \\(\\mathcal{N}(0,1)\\). Let \\(x\\) be a continuity point of \\(F\\).\n\\[\n\\lim_{\\sigma \\downarrow 0} P\\Big\\{ X + \\sigma Z  \\leq x \\Big\\} = F(x)\n\\]\n\\[\\begin{array}{rl}\n\\lim_{\\sigma \\downarrow 0} P\\Big\\{ X + \\sigma Z  \\leq x \\Big\\}\n& = \\lim_{\\sigma \\downarrow 0}\n\\int_{-\\infty}^x  \\left( \\frac{1}{{2 \\pi}}\\int_{\\mathbb{R}} \\mathrm{e}^{- \\frac{t^2 \\sigma^2}{2}}\\mathrm{e}^{-ity} \\widehat{F}(t)\\mathrm{d}t   \\right) \\mathrm{d}y \\\\\n& = \\int_{-\\infty}^x   \\frac{1}{{2 \\pi}}\\int_{\\mathbb{R}} \\lim_{\\sigma \\downarrow 0} \\mathrm{e}^{- \\frac{t^2 \\sigma^2}{2}}\\mathrm{e}^{-ity} \\widehat{F}(t)\\mathrm{d}t    \\mathrm{d}y \\\\\n& = \\int_{-\\infty}^x   \\frac{1}{{2 \\pi}}\\int_{\\mathbb{R}} \\mathrm{e}^{-ity} \\widehat{F}(t)\\mathrm{d}t    \\mathrm{d}y \\,\n\\end{array}\\] where interversion of limit and integration is justified by dominated convergence.\n\n\n\n\nWe close this section by an alternative inversion formula.\n\nTheorem 8.8 (Inversion formula) Let \\(P\\) be a probability distribution over \\(\\mathbb{R}\\) with cumulative distribution function \\(F\\), then \\[\n\\lim_{T \\to \\infty} \\frac{1}{2\\pi} \\int_{-T}^T \\frac{\\mathrm{e}^{-it a} - \\mathrm{e}^{-it b}}{it} \\widehat{F}(t) \\mathrm{d}t\n= F(b_-) - F(a) + \\frac{1}{2} \\Big(P\\{b\\} + P\\{a\\}\\Big) \\, .\n\\]\n\n\n\n\nThe proof of Theorem 8.8) can be found in textbooks like (Durrett, 2010) or (Billingsley, 2012).\n\nLet \\(\\widehat{F}\\) denote the characteristic function of the probability distribution \\(P\\), if \\(\\widehat{F}(t) = \\mathrm{e}^{-\\frac{t^2}{2}}\\), then \\(P\\) is the standard univariate Gaussian distribution (\\(\\mathcal{N}(0,1)\\)).\n\n\nLet \\(\\widehat{F}\\) denote the characteristic function of probability distribution \\(P\\), if \\(\\widehat{F}(t) = \\mathrm{e}^{i\\mu t -\\frac{\\sigma^2 t^2}{2}}\\), then \\(P\\) is the Gaussian distribution ( \\(\\mathcal{N}(\\mu,\\sigma^2)\\) ).\n\nAnother important byproduct of the proof of injectivity of the characteristic function is Stein’s identity, an important property of the standard Gaussian distribution.\n\nTheorem 8.9 (Stein’s identity) Let \\(X \\sim \\mathcal{N}(0,1)\\), and \\(g\\) be a differentiable function such that \\(\\mathbb{E}|g'(X)|&lt; \\infty\\), then\\[\n\\mathbb{E}[g'(X)] = \\mathbb{E}[Xg(X)] \\, .\n\\] Conversely, if \\(X\\) is a random variable such that \\[\n\\mathbb{E}[g'(X)] = \\mathbb{E}[X g(X)]\n\\] holds for any differentiable funtion \\(g\\) such that \\(g'\\) is integrable, then \\(X \\sim \\mathcal{N}(0,1)\\).\n\n\nProof. The direct part follows by integration by parts.\nTo check the converse, note that if \\(X\\) satisfies the identity in the Theorem, then for all \\(t \\in \\mathbb{R}\\), the functions \\(t \\mapsto \\mathbb{E} \\cos(tX)\\) and \\(t \\mapsto \\mathbb{E} \\sin(tX)\\) satisfy the differential equation \\(g'(t) = t g(t)\\) with conditions \\(\\mathbb{E} \\cos(0X)=1\\) and \\(\\mathbb{E} \\sin(0X) =0\\). This entails \\(\\mathbb{E} \\mathrm{e}^{itX} = \\exp\\Big(-\\frac{t^2}{2}\\Big)\\), that is \\(X \\sim \\mathcal{N}(0,1)\\)\n\n\n\n\n\n\n8.4.5 Differentiability and integrability\nDifferentiability of the Fourier transform at \\(0\\) and integrability are intimately related.\n\nTheorem 8.10 If \\(X\\) is \\(p\\)-integrable for some \\(p \\in \\mathbb{N}\\) then the Fourier transform of the distribution of \\(X\\) is \\(p\\)-times differentiable at \\(0\\) and the \\(p^{\\text{th}}\\) derivative equals \\(i^k \\mathbb{E}X^k\\).\n\n\nProof. The proof relies on a Taylor expansion with remainder of \\(x \\mapsto \\mathrm{e}^{ix}\\) at \\(x=0\\): \\[\n\\mathrm{e}^{ix} - \\sum_{k=0}^n \\frac{(ix)^k}{k!} = \\frac{i^{n+1}}{n!} \\int_0^x (x-s)^n \\mathrm{e}^{is} \\mathrm{d}s \\, .\n\\] The modulus of the right hand side can be upper-bounded in two different ways. \\[\n\\frac{1}{n+!}\\Big| \\int_0^x (x-s)^n \\mathrm{e}^{is} \\mathrm{d}s \\Big|\n  \\leq \\frac{|x|^{n+1}}{(n+1)!}\n\\] which is good when \\(|x|\\) is small. To handle large values of \\(|x|\\), integration by parts leads to \\[\n\\frac{i^{n+1}}{n!} \\int_0^x (x-s)^n \\mathrm{e}^{is} \\mathrm{d}s =  \\frac{i^{n}}{(n-1)!} \\int_0^x (x-s)^{n-1} \\left(\\mathrm{e}^{is}-1\\right) \\mathrm{d}s \\,.\n\\] The modulus of the right hand side can be upper-bounded by \\(2|x|^n/n!\\).\nApplying this Taylor expansion to \\(x=t X\\), using the pointwise upper bounds and taking expectations leads to \\[\\begin{array}{rl}\n  \\Big| \\widehat{F}(t) - \\sum_{k=0}^n \\mathbb{E}\\frac{(itX)^k}{k!}  \\Big|\n    & \\leq \\mathbb{E} \\Big[\\min\\Big( \\frac{|tX|^{n+1}}{(n+1)!} ,2 \\frac{|tX|^n}{n!}\\Big)\\Big] \\\\\n    & = \\frac{|t|^n}{(n+1)!} \\mathbb{E} \\Big[\\min\\Big(|tX|^{n+1} ,2 (n+1) |X|^n \\Big)\\Big] \\, .\n\\end{array}\\] Note that the right hand side is well defined as soon as \\(\\mathbb{E}|X|^n &lt; \\infty\\). Now, by dominated convergence, \\[\n\\lim_{t \\to 0} \\mathbb{E} \\Big[\\min\\Big(|tX|^{n+1} ,2 (n+1) |X|^n \\Big)\\Big] = 0\\,\n\\] Hence we have established that if \\(\\mathbb{E}|X|^n &lt; \\infty\\), \\[\n\\widehat{F}(t) = \\sum_{k=0}^n i^k \\mathbb{E}X^k \\frac{t^k}{k!} + o(|t|^n) \\, .\n\\]\n\n\n\n\nIn the other direction, the connection is not as simple: differentiability of the Fourier transform does not imply integrability. But the following holds.\n\nTheorem 8.11 If the Fourier transform \\(\\widehat{F}\\) of the distribution of \\(X\\) satisfies \\[\n\\lim_{h \\downarrow 0} \\frac{2 - \\widehat{F}(h) - \\widehat{F}(-h)}{h^2} = \\sigma^2 &lt; \\infty\n\\] then \\(\\mathbb{E}X^2 = \\sigma^2\\), .\n\n\nProof. Note that \\[\n2 - \\widehat{F}(h) - \\widehat{F}(-h) =  2\\mathbb{E}\\Big[1 - \\cos(hX)\\Big] \\, ,\n\\] and using Taylor with remainder formula for \\(\\cos\\) at \\(0\\): \\[\n1 - \\cos x = \\int_0^x \\cos(s) (x-s) \\mathrm{d}s = x^ 2 \\int_0^1 \\cos(sx) (1-s) \\mathrm{d}s\n\\] Note that \\(\\int_0^1 \\cos(sx) (1-s) \\mathrm{d}s\\geq 0\\) for all \\(x \\in \\mathbb{R}\\). \\[\\begin{array}{rl}\n  \\frac{2\\mathbb{E}\\Big[1 - \\cos(hX)\\Big]}{h^2}\n    & = 2 \\frac{\\mathbb{E}\\Big[ h^2 X^2 \\int_0^1 \\cos(shX) (1-s) \\mathrm{d}s\\Big]}{h^2} \\\\\n    & = 2 \\mathbb{E}\\Big[ X^2 \\int_0^1 \\cos(shX) (1-s) \\mathrm{d}s\\Big] \\, .\n\\end{array}\\] By Fatou’s Lemma: \\[\n\\sigma^2 = \\lim_{h \\downarrow 0} 2\\mathbb{E}\\Big[ X^2 \\int_0^1 \\cos(shX) (1-s) \\mathrm{d}s\\Big]\n\\geq 2\\mathbb{E}\\Big[\\liminf_{h \\downarrow 0} X^2 \\int_0^1 \\cos(shX) (1-s) \\mathrm{d}s \\Big]\n\\] but for all \\(x \\in \\mathbb{R}\\), by dominated convergence, \\[\n\\liminf_{h \\downarrow 0} x^ 2 \\int_0^1 \\cos(shx) (1-s) \\mathrm{d}s = \\frac{x^2}{2} \\, .\n\\] Hence \\[\n\\sigma^2 \\geq \\mathbb{E} X^2 \\, .\n\\] The proof is completed by invoking Theorem 8.10).\n\n\n\n8.4.6 Another application: understanding Cauchy distribution\nAssume \\(U\\) is uniformly distributed over \\(]0,1[\\), let the real valued random variable \\(X\\) be defined by \\[X = \\tan\\left(\\frac{\\pi}{2} (2 \\times U -1)\\right)\\].\nAs \\(\\tan\\) is continuously increasing from \\(-\\pi/2\\) to \\(\\pi/2\\), the cumulative distribution function of the distribution of \\(X\\) is \\[\\begin{array}{rl}\n\\mathbb{P}\\{ X \\leq x\\} & = \\mathbb{P}\\left\\{\\tan\\left(\\frac{\\pi}{2}(2U-1)\\right) \\leq x\\right\\} \\\\\n& = \\mathbb{P}\\left\\{U \\leq \\frac{1}{2} + \\frac{1}{\\pi}\\arctan(x) \\right\\} \\\\\n& = \\frac{1}{2} + \\frac{1}{\\pi}\\arctan(x)\n\\end{array}\\] for \\(x \\in \\mathbb{R}\\).\nAs \\(\\arctan\\) has derivative \\(x \\mapsto \\frac{1}{1+x^2}\\), the cumulative distribution function is absolutely continuous with density: \\[\\frac{1}{\\pi} \\frac{1}{1  + x^2}\\] This is the density of the Cauchy distribution.\nNote that \\(\\mathbb{E}(X)_+ = \\mathbb{E} (X)_- = \\mathbb{E}|X| =\\infty\\). The Cauchy distribution is not integrable.\nNow, assume \\(X_1, X_2,, \\ldots, X_n\\) are i.i.d. and Cauchy distributed. Let \\(Z = \\sum_{i=1}^n X_i/n\\). How is \\(Z\\) distributed? We might compute the convolution power of the Cauchy density. It turns out that starting from the characteristic function is much more simple.\nWe refrain from computing directly the characteristic function of the Cauchy distribution. We take a roundabout.\nLet \\(Y\\) be distributed according to Laplace distribution, that is with density \\(y \\mapsto  \\frac{1}{2} \\exp(-|y|)\\) for \\(y \\in \\mathbb{R}\\). The random variable \\(Y\\) is symmetric (\\(Y \\sim -Y\\)). Let \\(\\widehat{F}_Y\\) denote the characteristic function of (the distribution of) \\(Y\\).\n\\[\\begin{array}{rl}\n  \\widehat{F}_Y(t) & = \\mathbb{E}\\mathrm{e}^{tY} \\\\\n  & = \\mathbb{E}\\cos(tY) \\\\\n  & =  \\int_0^{\\infty} \\mathrm{e}^{-y} \\cos(ty) \\mathrm{d}y \\\\\n  & = \\left[- \\mathrm{e}^{-y} \\cos(ty)\\right]_0^\\infty - t \\int_{0}^\\infty \\mathrm{e}^{-ty} \\sin(ty) \\mathrm{d}y \\\\\n  & = 1 - t \\int_{0}^\\infty \\mathrm{e}^{-y} \\sin(ty) \\mathrm{d}y \\\\\n  & = 1 -t \\left[- \\mathrm{e}^{-y} \\sin(ty)\\right]_0^\\infty  - t^2 \\int_0^\\infty \\mathrm{e}^{-y} \\cos(ty) \\mathrm{d}y \\\\\n  & = 1 - t^2 \\widehat{F}_Y(t)\n\\end{array}\\] where we have performed integration by parts twice.\nThe characteristic function \\(\\widehat{F}_Y\\) satisfies \\[\\widehat{F}_Y(t) = \\frac{1}{1+ t^2}\\, ,\\] up to \\(\\frac{1}{\\pi}\\), this is the density of the Cauchy distribution.\n\\[\\begin{array}{rl}\n\\widehat{F}_X(t) & = \\mathbb{E}\\mathrm{e}^{itX}\\\\\n  & =  \\int_{-\\infty}^{\\infty} \\frac{1}{\\pi} \\frac{1}{1+x^2}  \\cos(tx)\\mathrm{d}x \\\\\n  & = \\frac{2}{\\pi} \\int_0^\\infty \\cos(tx) \\widehat{F}_Y(x) \\mathrm{d}x \\\\\n  & = 2 \\times \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\mathrm{e}^{-itx} \\widehat{F}_Y(x) \\mathrm{d}x \\\\\n  & = 2 \\times \\frac{1}{2} \\mathrm{e}^{-|t|} = \\mathrm{e}^{-|t|}\n\\end{array}\\] where we have used the inversion formula.\nNow, the characteristic function of the distribution of \\(Z\\) is \\[\\widehat{F}_Z(t) =\\left(\\mathrm{e}^{-\\frac{|t|}{n}}\\right)^n=  \\widehat{F}_X(t)\\] which means \\(Z \\sim X\\).\nThe basic tools of characteristic functions theory allow us to - compute the characteristic function of the Laplace distribution - compute the characteristic function of the Cauchy distribution by inversion - compute the characteristic function of sums of independant Cauchy random variables - show that the Cauchy distribution is \\(1\\)-stable.\n\nThe density of the Laplace distribution is not differentiable at \\(0\\), this is reflected in the fact that its Fourier transform (the characteristic function of the Laplace distribution) is not integrable.\nConversely the lack of integrability of the Cauchy distribution is reflected in the non-differentiability of its characteristic function at \\(0\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Characterizations of probability distributions</span>"
    ]
  },
  {
    "objectID": "04-characterizations.html#quantiles",
    "href": "04-characterizations.html#quantiles",
    "title": "8  Characterizations of probability distributions",
    "section": "8.5 Quantile functions",
    "text": "8.5 Quantile functions\nSo far we have seen several characterizations of probability distributions: cumulative distribution functions, Laplace transform for distributions supported on \\([0, \\infty)\\), characteristic functions. The last characterization is praised for its behavior with respect to sums of independent random variables.\nFor univariate distributions, a companion to the cumulative distribution function is the quantile function. It plays a significant role in simulations, statistics and risk theory.\nA cumulative distribution function \\(F\\) is non-negative, \\([0,1]\\)-valued, non-decreasing, right-continuous, with left-limit at any point. The cumulative distribution function of a diffuse probability measure is continuous at any point.\nThe quantile function \\(F^{\\leftarrow}\\) is defined as an extended inverse of the cumulative distribution function \\(F\\).\n\nDefinition 8.3 (Quantile function) The quantile function \\(F^{\\leftarrow}\\) of random variable \\(X\\) distributed according to \\(P\\) (with cumulative distribution function \\(F\\)) is defined as \\[\\begin{array}{rl}\nF^{\\leftarrow} (p)\n   & = \\inf \\Big\\{ x : P\\{X \\leq x\\} \\geq p \\Big\\} \\\\\n   & = \\inf \\Big\\{ x : F (x) \\geq p \\Big\\} \\qquad \\text{for } p \\in (0,1).\n\\end{array}\\]\n\nThe quantile function is non-decreasing and left-continuous. The interplay between the quantile and cumulative distribution functions is summarized in the next proposition.\n\nProposition 8.11 If \\(F\\) and \\(F^\\leftarrow\\) are the cumulative distribution function and the quantile function of (the distribution of) \\(X\\), the following statements hold for \\(p \\in] 0, 1 [\\):\n\n\\(p \\leq  F (x)\\) iff \\(F^\\leftarrow (p) \\leq  x\\).\n\\(F \\circ F^\\leftarrow (p) \\geq p\\) .\n\\(F^\\leftarrow \\circ F (x) \\leq  x\\) .\nIf \\(F\\) is absolutely continuous, then \\(F \\circ F^\\leftarrow (p) = p\\)\n\n\n\nProof. According to the definition of \\(F^\\leftarrow\\) if \\(F (x) \\geq p\\) then \\(F^\\leftarrow\n(p) \\leq  x.\\)\nTo prove the converse, it suffices to check that \\(F \\circ F^\\leftarrow (p) \\geq p\\).\nIndeed, if \\(x \\geq F^\\leftarrow (p),\\) as \\(F\\) is non-decreasing \\(F (x) \\geq F \\circ F^\\leftarrow (p)\\). Si \\(y = F^\\leftarrow\n(p),\\) par definition de \\(y = F^\\leftarrow (p),\\) il existe une non-increasing sequence \\((z_n)_{n \\in \\mathbb{N}}\\) which converges to \\(y\\) such that \\(F (z_n) \\geq p .\\) Mais as \\(F\\) is right-continuous \\(\\lim_n F\n(z_n) = F (\\lim_n z_n) = F (y) .\\) Hence \\(F (y) \\geq p\\).\nWe just proved 1. and 2.\n3.) is an immediate consequence de 1). Let \\(p = F (x) .\\) Hence \\(p \\leq  F (x),\\) according to 1.) this is equivalent to \\(F^\\leftarrow (p)\n\\leq  x,\\) that is \\(F^\\leftarrow \\circ F (x) \\leq  x.\\)\n4.) For every \\(p\\) in \\(] 0, 1 [,\\) \\(\\{ x : p = F (x)\\}\\) is non-empty (Mean value Theorem). Let \\(y = \\inf \\{ x : p = F (x)\\} =F^\\leftarrow (p)\\). According to \\(1)\\), \\(F (y) \\geq p\\). Now, if \\((z_n)_{n \\in \\mathbb{N}}\\) is an increasing sequence converging to \\(y\\), for every \\(n\\), \\(F (z_n) &lt; p,\\) and, by left-continuity, \\(F (y) = F (\\lim_n z_n) = \\lim_n F (z_n) \\leq\np.\\) Hence \\(F (y) = p,\\) that is \\(F \\circ F^\\leftarrow (p) = p.\\)\n\n\n\nProposition 8.12 (Quantile transformation) If \\(U\\) is uniformly distributed on \\((0,1)\\), and \\(F\\) is a cumulative distribution over \\(\\mathbb{R}\\), \\(F^{\\leftarrow}(U)\\) has cumulative distribution \\(F\\).\n\n\nProof. \\[\\begin{array}{rl}\n  P\\Big\\{ F^\\leftarrow(U) \\leq x \\Big\\}\n  & = P\\Big\\{ U \\leq F(x) \\Big\\} \\\\\n  & = F(x) \\, .\n\\end{array}\\]\n\\(\\Box\\)\n\n\nRemark 8.2. The quantile transformation works whatever the continuity properties of \\(F\\).\n\nThe quantile transformation has many applications. It can be used to show stochastic domination properties.\n\nExample 8.5 In Figures 8.1 up to Figure 8.4, we illustrate quantile functions for discrete (binomial) distributions and for distributions that are neither discrete nor continuous. The quantile function of a discrete distribution is step function that jumps at the cumulative probability of every possible outcome. If a probability distribution is a mixture of a discrete distribution and a continuous distribution, the quantile function jumps at the cumulative probability of every possible outcome of the discrete component.\n\n\n\n\n\n\n\n\n\n\nFigure 8.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Quantile functions \\(\\max(X, \\tau)\\) where \\(X \\sim \\mathcal{N}(0,1)\\) for \\(\\tau \\in \\{0, 2\\}\\). Let \\(\\Phi^\\leftarrow\\) denote the quantile function of \\(\\mathcal{N}(0,1)\\). The quantile function of \\(\\max(X,\\tau)\\) is \\(\\mathbb{I}_{(0,\\Phi(\\tau)]}(p) \\times \\tau + \\Phi^{\\leftarrow}(p) \\times \\mathbb{I}_{(\\Phi(\\tau), 1)}(p)= \\Phi^{\\leftarrow}(p \\vee \\Phi(\\tau))\\). The two distributions are neither absolutely continuous nor discrete.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: Cumulative distribution functions for the probability distributions illustrated in Figure 8.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Representation of \\(F \\circ F^{\\leftarrow}\\) for the probability distributions illustrated in Figures 8.2 and Figure 8.3. The function \\(F \\circ F^{\\leftarrow}\\) always lies above the line \\(y=x\\) (dotted line) as prescribed in Proposition 8.11. Plateaux that lie strictly above the dotted line are in correspondence with jumps of the quantile function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.5\n\n\n\n\n\n\n\nLet us conclude this section with an important observation. concerning the behavior of \\(F(X)\\) when \\(X \\sim P\\) with cumulative distribution function \\(F\\). \n\nCorollary 8.4 If \\(X \\sim P\\) with continuous cumulative distribution function \\(F\\), then \\(F (X)\\) and \\(1 - F (X)\\) are uniformly distributed on \\([0, 1]\\).\n\n\nExercise 8.2 Prove Corollary 8.4",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Characterizations of probability distributions</span>"
    ]
  },
  {
    "objectID": "04-characterizations.html#sec-bibcharac",
    "href": "04-characterizations.html#sec-bibcharac",
    "title": "8  Characterizations of probability distributions",
    "section": "8.6 Bibliographic remarks",
    "text": "8.6 Bibliographic remarks\nWilf (2005) explores the interplay between combinatorics, algorithm analysis and generating function theory.\nWidder (2015) is a classic reference on Laplace transforms. Laplace transforms play an important role in Point Process Theory, and Extreme Value Theory, to name a few fields of application.\nThe first part of Chapter 9 from Pollard (2002) describes characteristic functions as Fourier transforms. Properties and applications of characteristic functions are thoroughly discussed in (Durrett, 2010), (Billingsley, 2012).\n\n\n\n\nBillingsley, P. (2012). Probability and measure. John Wiley & Sons, Inc., Hoboken, NJ.\n\n\nDurrett, R. (2010). Probability: Theory and examples. Cambridge University Press.\n\n\nPollard, D. (2002). A user’s guide to measure theoretic probability (Vol. 8, p. xiv+351). Cambridge University Press, Cambridge.\n\n\nWidder, D. V. (2015). Laplace transform (PMS-6). Princeton university press.\n\n\nWilf, H. S. (2005). Generatingfunctionology. AK Peters/CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Characterizations of probability distributions</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html",
    "href": "06-conditioning.html",
    "title": "9  Conditioning",
    "section": "",
    "text": "9.1 Defining conditional expectation\nIn this and the following sections, \\((\\Omega,\\mathcal{F},P)\\) is a probability space, and \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) a sub-\\(\\sigma\\)-algebra. The sub-\\(\\sigma\\)-algebra need not be atomic as in Chapter 7. We cannot define conditional probabilities by conditioning with respect to atoms generating \\(\\mathcal{G}\\). Our objective is nervertheless to define conditional expectations with respect to sub-\\(\\sigma\\)-algebra \\(\\mathcal{G}\\), while retaining the nice properties surveyed in Chapter 7.\nThe general definition of conditional expectation starts from the property described in Proposition 7.4.\nLeaving aside the question of the existence of a version of conditional expectation of \\(X,\\) we first check that if there exist different versions, they differ only up to a negligible event.\nStill postponing the existence question, let us check now a few properties versions of conditional expectation of \\(X\\) should satisfy.\nThe proof reproduces the argument used to established that different versions of the conditional expectation are almost surely equal.\nThe next corollary is a consequence of Proposition 9.1.\nFor nested sub-\\(\\sigma\\)-algebras, conditional expectations satisfy the tower property:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#sec-motivcondexp",
    "href": "06-conditioning.html#sec-motivcondexp",
    "title": "9  Conditioning",
    "section": "",
    "text": "Definition 9.1 (Conditional expectation) Let \\(X \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\) and \\(\\mathcal{G}\\) be a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\), then a random variable \\(Y\\) is a version of the conditional expectation of \\(X\\) with respect to \\(\\mathcal{G}\\) iff\n\n\\(Y\\) is \\(\\mathcal{G}\\)-measurable.\nFor every event \\(B\\) in \\(\\mathcal{G}\\):\n\n\\[\\mathbb{E} \\left[\\mathbb{I}_B X \\right] = \\mathbb{E} \\left[ \\mathbb{I}_B Y \\right]\\,.\\]\n\n\n\nLet \\(X \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\) and \\(\\mathcal{G}\\) a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\), then if \\(Y'\\) and \\(Y\\) are two versions of the conditional expectation of \\(X\\) with respect to \\(\\mathcal{G}\\):\n\\[P \\left\\{ Y = Y' \\right\\} = 1.\\]\n\n\nProof. As \\(Y\\) and \\(Y'\\) are \\(\\mathcal{G}\\)-measurable, the event \\[\nB = \\left\\{ \\omega~:~Y(\\omega) &gt;Y'(\\omega)\\right\\}\n\\] belongs to \\(\\mathcal{G}.\\) Moreover, \\[\\begin{align*}\n\\mathbb{E}\\left[\\mathbb{I}_B\\, X\\right]\n& =  \\mathbb{E}\\left[\\mathbb{I}_B \\, Y\\right] \\\\\n& =  \\mathbb{E}\\left[\\mathbb{I}_B \\, Y'\\right]  \\, .\n\\end{align*}\\] Thus \\[\n\\mathbb{E}\\left[\\mathbb{I}_B (Y-Y') \\right] = 0 \\, .\n\\] As random variable \\(\\mathbb{I}_B (Y-Y')\\) is non-negative, its expectation is zero, it is null with probability \\(1\\). Thus \\[\nP\\{Y&gt;Y'\\}=0 \\, .\n\\] We can conclude by proceeding in a similar way for event \\(\\{Y&lt;Y'\\}\\).\n\\(\\square\\)\n\n\n\nLet \\(X_1, X_2 \\in \\mathcal{L}_1 (\\Omega,\n\\mathcal{F}, P)\\), \\(\\mathcal{G}\\) a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\), \\(a_1, a_2\\) two real numbers, then if \\(Y_1\\) \\(Y_2\\) and \\(Z\\) are respectively versions versions of conditional expectation of \\(X_1, X_2\\) and \\(a_1 X_1\n+ a_2 X_2\\) with respect to \\(\\mathcal{G}\\), we have \\[\nP\\{a_1 Y_1 + a_2 Y_2 = Z\\} = 1 \\, .\n\\]\n\n\nProof. Let \\(B\\) be the event of \\(\\mathcal{G}\\) defined by \\[\n\\{ a_1 Y_1 + a_2 Y_2 &gt; Z\\}\\, .\n\\] We get \\[\\begin{eqnarray*}\n\\mathbb{E}\n[\\mathbb{I}_B Z] & = &  \\mathbb{E} [\\mathbb{I}_B (a_1 X_1 + a_2 X_2)]  \\\\\n& = & a _1 \\mathbb{E} [\\mathbb{I}_B  X_1 ]+a_2 \\mathbb{E} [\\mathbb{I}_B X_2] \\\\\n& = & a_1  \\mathbb{E} [\\mathbb{I}_B  Y_1 ]+a_2 \\mathbb{E}\n[\\mathbb{I}_B Y_2] \\\\\n& = & \\mathbb{E} [\\mathbb{I}_B (a_1 Y_1 + a_2 Y_2)]  \\, ,\n\\end{eqnarray*}\\] and thus \\[\n\\mathbb{E}\n[\\mathbb{I}_B (Z-(a_1 Y_1 + a_2 Y_2))]=  0 \\, .\n\\] We conclude as in the proceeding proof that \\(P\\{B\\}=0.\\)\nThe proof is completed by handling in a similar way the event \\(\\{ a_1 Y_1 + a_2 Y_2 &lt; Z\\}\\, .\\)\n\\(\\square\\)\n\n\nProposition 9.1 If \\(X \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\), \\(\\mathcal{G}\\) a sub-\\(\\sigma\\) algebra of \\(\\mathcal{F}\\). If \\(Z\\) is a version the conditional expectation of \\(X\\) with respect to \\(\\mathcal{G}\\) and if \\(X\\) is \\(P\\)-a.s. non-negative, then \\[\nP\\{Z \\geq 0\\} =1 \\, .\n\\]\n\n\n\nProof. For \\(n \\in \\mathbb{N}\\), let \\(B_n\\) denote the event (from \\(\\mathcal{G}\\)) defined by \\[ B_n = \\left\\{ \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] &lt; -\n\\frac{1}{n} \\right\\} . \\] To prove the proposition, it is enough to check \\[ P \\left\\{ \\cup_n B_n \\right\\} = 0 . \\] As \\(P \\left\\{ \\cup_n B_n \\right\\} = \\lim_n P\\{B_n \\}\\), it suffices to check \\(P \\left\\{ B_n \\right\\} = 0\\), for all \\(n\\), \\(P \\left\\{ B_n\n\\right\\} = 0.\\) For all \\(n\\), \\[\\begin{align*}\n  0\n  & \\leq \\mathbb{E}\\big[\\mathbb{I}_{B_n} X\\big] \\\\\n  & = \\mathbb{E} \\left[ \\mathbb{I}_{B_n} X \\right] \\\\\n  & = \\mathbb{E} \\left[ \\mathbb{I}_{B_n}  \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\right] \\\\\n  & \\leq  - \\frac{P\\{B_n \\}}{n} \\, .\n\\end{align*}\\] Hence, for all \\(n\\), \\(P\\{B_n \\}= 0\\).\n\\(\\square\\)\n\n\n\nCorollary 9.1 If \\((X_n)_{n \\in \\mathbb{N}}\\) is a sequence of random variables from \\(\\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\) satisfying \\(X_{n + 1} \\geq X_n\\) \\(P\\)-a.s. then there exists an \\(P\\)-a.s. non-decreasing sequence of versions of conditional expectations \\[\n\\forall n \\in \\mathbb{N},\\quad\\mathbb{E} \\left[ X_{n + 1} \\mid \\mathcal{F} \\right] \\geq   \\mathbb{E} \\left[ X_n \\mid \\mathcal{F} \\right] .\n\\]\n\n\nLet \\(\\mathcal{E}\\) be a \\(\\pi\\)-system generating \\(\\mathcal{G}\\) and containing \\(\\Omega\\). Check that \\(\\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]\\) is the unique element from \\(\\mathcal{L}_1 \\left( \\Omega, \\mathcal{G}, P\n\\right)\\) which satisfies \\[\n\\forall B \\in \\mathcal{E}, \\quad \\mathbb{E} \\left[\n\\mathbb{I}_B X \\right] = \\mathbb{E} \\left[ \\mathbb{I}_B\n\\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\right] .\n\\]\n\n\n\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space, and \\(\\mathcal{G} \\subseteq \\mathcal{H} \\subseteq \\mathcal{F}\\) be two nested sub-\\(\\sigma\\)-algebras. Then for every \\(X \\in \\mathcal{L}_1(\\Omega, \\mathcal{F}, P)\\): \\[\n\\mathbb{E} \\Big[ \\mathbb{E}\\left[ X \\mid \\mathcal{G} \\right] \\mid \\mathcal{H} \\Big]\n= \\mathbb{E} \\Big[ \\mathbb{E} \\, \\left[ X \\mid \\mathcal{H} \\right] \\mid \\mathcal{G} \\Big]\n= \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]\\qquad \\text{a.s.}\n\\]\n\n\nProof. Almost sure equality \\(\\mathbb{E} \\Big[ \\mathbb{E}\\left[ X \\mid \\mathcal{G} \\right] \\mid \\mathcal{H} \\Big]=\\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]\\) is trivial: any \\(\\mathcal{G}\\)-measurable random variable is also \\(\\mathcal{H}\\)-measurable.\nLet us now check the second equality.\nFor every \\(B \\in \\mathcal{G}\\), \\[\\begin{eqnarray*}\n\\mathbb{E} \\left[ \\mathbb{I}_B \\mathbb{E} \\left[ \\mathbb{E}\n\\left[ X \\mid \\mathcal{H} \\right] \\mid \\mathcal{G} \\right] \\right] & = &\n\\mathbb{E} \\left[ \\mathbb{I}_B \\mathbb{E} \\left[ X \\mid\n\\mathcal{H} \\right] \\right]\\\\\n&  & \\text{comme } B \\in \\mathcal{H}\\\\\n& = & \\mathbb{E} \\left[ \\mathbb{I}_B X \\right] .\n\\end{eqnarray*}\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#predictpoint",
    "href": "06-conditioning.html#predictpoint",
    "title": "9  Conditioning",
    "section": "9.2 Conditional expectation in \\(\\mathcal{L}_2(\\Omega, \\mathcal{F}, P)\\)",
    "text": "9.2 Conditional expectation in \\(\\mathcal{L}_2(\\Omega, \\mathcal{F}, P)\\)\nIf we focus on square-integrable random variables, building versions of conditional expectation turn out to be easy. Recall that when the conditioning sub-\\(\\sigma\\)-algebra \\(\\mathcal{G}\\) is atomic, according to Proposition 7.5, the condition expectation \\(\\mathbb{E}[X \\mid \\mathcal{G}]\\) defines an optimal predictor of \\(X\\) with respect to quadratic error amongst \\(\\mathcal{G}\\)-measurable random variables. This characterization remains valid for square integrable random variables even when the conditioning sub-\\(\\sigma\\)-algebra is no more atomic. This is the content of the next theorem.\n\nTheorem 9.1 (Conditional expectation for square integrable random variables) Let be \\(X \\in \\mathcal{L}_2 (\\Omega, \\mathcal{F}, P)\\) and \\(\\mathcal{G}\\) a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\).\nThere exists \\(Y \\in \\mathcal{L}_2(\\Omega, \\mathcal{G}, P)\\) that minimizes the \\(L_2\\) distance to \\(X\\):\n\\[\\exists Y \\in \\mathcal{L}_2(\\Omega, \\mathcal{G}, P) \\qquad \\mathbb{E}(Y-X)^2 =  \\inf \\Big\\{ \\mathbb{E}(Z-X)^2 : Z \\in \\mathcal{L}_2(\\Omega, \\mathcal{G}, P) \\Big\\}\\, ,\\]\nthat is, \\(Y\\) represents a version of the of \\(X\\) on \\(\\mathcal{L}_2(\\Omega, \\mathcal{G}, P)\\).\nA version \\(Y\\) of the of \\(X\\) on \\(\\mathcal{L}_2(\\Omega, \\mathcal{G}, P)\\) is also a version of the conditional expectation of \\(X\\) with respect to \\(\\mathcal{G}\\):\n\\[\\forall B \\in \\mathcal{G}, \\quad\n\\mathbb{E} \\left[\n\\mathbb{I}_B X \\right] = \\mathbb{E} \\left[ \\mathbb{I}_B\\, Y \\right] \\, .\\]\n\nNote that theorem contains two statements: first, there exists a minimizer of \\(\\mathbb{E}(X-Z)^2\\) in \\(\\mathcal{L}_2(\\omega, \\mathcal{F}, P)\\), second, such a minimizer is a version of condition expectation defined according to Definition 9.1. Checking the first statement amounts to invoke the right arguments from Hilbert spaces theory.\nFor the sake of self-reference, we recall basics if Hilbert spaces theory.\n\nDefinition 9.2 (Hilbert’s space) A real vector space \\(E\\) equipped with a norm \\(\\|\\cdot\\|\\) is a Hilbert space iff \\(\\langle \\cdot, \\cdot \\rangle\\) defined by\n\\[\\forall x, y \\in E, \\langle x, y \\rangle = \\frac{1}{4} \\Big(\\Vert x+y \\Vert^2 + \\Vert x-y \\Vert^2\\Big)\\]\nis an inner product and \\(E\\) is complete for the topology induced by the norm.\n\n\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space, then the set \\(L_2(\\Omega, \\mathcal{F}, P)\\) of equivalence classes of square integrable variables, equipped with \\(\\Vert X\\Vert^2=  (\\mathbb{E} X^2)^{1/2}\\) is a Hilbert space.\n\n\nRemark 9.4. In this context,\n\\[\\langle X, Y \\rangle  = \\mathbb{E}\\left[ XY \\right] \\, .\\]\n\nFrom Hilbert space theory, the essential tool we shall use is the projection Theorem below. Our starting point is the next observation (that follows from results in Chapter 3).\n\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space, let \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) be a sub-\\(\\sigma\\)-algebra, then \\(L_2(\\Omega, \\mathcal{G}, P)\\) is a closed convex subset (subspace) of \\(L_2(\\Omega, \\mathcal{F}, P)\\).\n\nWe look for the element from \\(L_2(\\Omega, \\mathcal{G}, P)\\) that is closest (in the \\(L_2\\) sense) to a random variable from \\(L_2(\\Omega, \\mathcal{F}, P)\\). The existence and unicity of this closest \\(\\mathcal{G}\\)-measurable random variable are warranted by the Projection Theorem.\n\nTheorem 9.2 (Projection Theorem) Let \\(E\\) be a Hilbert space and \\(F\\) a closed convex subset of \\(F\\). For every \\(x \\in E\\), there exists a unique \\(y \\in F\\), such that\n\\[\\|x - y\\|= \\inf_{z \\in F} \\|x - z\\|.\\]\nThis unique closest point in \\(F\\) is called the (orthogonal) projection of \\(x\\) over \\(F\\). For any \\(z \\in F\\), \\[\\langle x-y, z-y\\rangle \\leq 0 \\, .\\] If \\(F\\) is a linear subspace of \\(E\\), the Pythagorean relationship holds: \\[\\|x\\|^2 =  \\|y\\|^2 + \\|x - y\\|^2 \\,\\] and for any \\(z \\in F\\), \\(\\langle x - y, z\\rangle =0\\).\n\n\nProof. Let \\(d = \\inf_{z \\in F} \\|x - z\\|\\). Let \\((z_n)_n\\) be a sequence of elements from \\(F\\) such that \\[\\lim_n \\|x - z_n \\|= d .\\] According to the parallelogram law, \\[2 \\left( \\|x - z_n \\|^2 +\\|x - z_m \\|^2 \\right)  = \\|2 x - (z_n + z_m)\\|^2 + \\| z_n - z_m \\|^2 .\\] Since \\(F\\) is convex, \\((z_n + z_m) / 2 \\in F\\), so \\[\\|x - (z_n + z_m) / 2\\| \\geq d \\,.\\] Let \\(\\epsilon \\in (0, 1]\\) and \\(n_0\\) be such that for \\(n \\geq n_0\\), \\(\\|x - z_n \\| \\leq d + \\epsilon .\\) For \\(n, m \\geq n_0\\) \\[4 (d + \\epsilon)^2 \\geq 4 d^2 +\\|z_n - z_m \\|^2\\] or equivalently \\[\\|z_n - z_m \\|^2 \\leq 4 (2 d + 1) \\epsilon \\,.\\] Hence, the minimizing sequence \\((z_n)_n\\) has the Cauchy property. As \\(F\\) is closed, it has a unique limit \\(y \\in F\\) and \\(d  = \\|x - y\\|\\).\nTo verify uniqueness, suppose there exists \\(y' \\in F\\), such as \\(\\|x - y' \\|= d\\). Now, let us build a new sequence \\((z'_n)_{n \\in\n\\mathbb{N}}\\) such that \\(z'_{2 n} = z_n\\) and \\(z'_{2 n + 1} = y'\\). This \\(F\\)-valued sequence satisfies \\(\\lim_n \\|z'_n - x\\|= d.\\) By the argument above, it admits a limit \\(y^{\\prime\\prime}\\) in \\(F\\). The limit \\(y^{\\prime\\prime}\\) coincides with the limit of any sub-sequence, so it equals \\(y\\) and \\(y'.\\)\nFix \\(z \\in F \\setminus \\{y\\}\\), for any \\(u \\in (0,1]\\), let \\(z_u = y + u (z-y)\\), then \\(z_u \\in F\\) and \\[\\Vert  x - z_u\\Vert^2 - \\Vert x -y \\Vert^2 = -2 u \\langle x-y, z-y \\rangle +  u^2 \\Vert z - y \\Vert^2 \\, .\\] As this quantity is non-negative for \\(u \\in [0,1]\\), \\(\\langle x-y, z-y \\rangle\\) has to be non-positive.\nNow suppose that \\(F\\) is a subspace of \\(E.\\)\nIf there is \\(y \\in F\\) such as \\(\\langle x - y, z \\rangle = 0\\) for any \\(z\n\\in F\\), then \\(y\\) is the orthogonal projection of \\(x\\) on \\(F\\) since for all \\(z \\in F\\): \\[\\begin{align*}\n\\|x - z\\|^2\n  & =  \\|x - y\\|^2 - 2 \\langle x - y, z \\rangle +\\|z\\|^2\\\\\n  & \\geq  \\|x - y\\||^2 .\n\\end{align*}\\]\nConversely, if \\(y\\) is the orthogonal projection of \\(x\\) on \\(F\\), for all \\(z\\) of \\(F\\) and all \\(\\lambda \\in \\in \\mathbb{R}\\): \\[\\begin{align*}\n\\|x - y\\||^2\n  & \\leq  \\|x - (y + \\lambda z)\\|^2 \\\\\n  & =  \\|x - y\\|^2 - 2 \\lambda \\langle x - y, z \\rangle + \\lambda^2 \\|z\\|^2,\n\\end{align*}\\] so \\(0 \\leq 2 \\lambda \\langle x - y, z \\rangle + \\lambda^2 \\|z\\|^2\\). For this polynomial in \\(\\lambda\\) to be of constant sign, it is necessary that \\(\\langle x - y, z \\rangle = 0.\\)\n\\(\\square\\)\n\nAs \\(\\mathcal{L}_2 (\\Omega, \\mathcal{G}, P)\\) is a convex part of \\(\\mathcal{L}_2 (\\Omega, \\mathcal{F}, P)\\), the existence and uniqueness of the projection on a closed convex part of a Hilbert space gives the following corollary which matches the first statement in Theorem 9.1).\n\nGiven \\(X \\in \\mathcal{L}_2 (\\Omega, \\mathcal{F}, P)\\) and \\(\\mathcal{G}\\) a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\), there exists \\(Y \\in \\mathcal{L}_2 (\\Omega, \\mathcal{G}, P)\\) that minimizes \\[\n\\mathbb{E} \\left[ \\left( X - Z \\right)^2 \\right] \\qquad \\text{ for } Z \\in\n\\mathcal{L}_2 (\\Omega, \\mathcal{G}, P) .\n\\] Any other minimizer in \\(\\mathcal{L}_2 (\\Omega, \\mathcal{G},\nP)\\) is \\(P\\)-almost surely equal to~\\(Y.\\)\n\n\nProof. Let \\(Y\\) be a version of the orthogonal projection of \\(X\\) on \\(L_2(\\Omega,\\mathcal{G},P)\\) and \\(B\\) an element of \\(\\mathcal{G}.\\)\nThe inner product of \\(\\mathbb{I}_B \\in \\mathcal{L}_2(\\Omega,\\mathcal{G},P)\\)) and \\(X-Y\\) is \\[\n\\langle X-Y, \\mathbb{I}_B \\rangle = \\mathbb{E}\\left[(X-Y)\\mathbb{I}_B\\right] \\, .\n\\] By Theorem 9.2, \\(\\mathbb{E}\\left[(X-Y)\\mathbb{I}_B\\right]=0\\).\n\\(\\square\\)\n\nWe conclude this section with a Pythagorean theorem for the variance.\n\nDefinition 9.3 (Conditional variance) Let \\(X \\in \\mathcal{L}_2(\\Omega, \\mathcal{F}, P)\\) and \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) a sub-\\(\\sigma\\)-algebra. The conditional variance of \\(X\\) with respect to \\(\\mathcal{G}\\) is defined by\n\\[ \\operatorname{Var} \\left[ X \\mid \\mathcal{G} \\right] = \\mathbb{E} \\left[\\left( X - \\mathbb{E} [X \\mid \\mathcal{G}] \\right)^2 \\mid \\mathcal{G}\\right] .\\]\n\nThe conditional variance is a (\\(\\mathcal{G}\\)-measurable) random variable, just as the conditional expectation. It is the conditional expectation of the prediction error that is incurred when trying to predict \\(X\\) using \\(\\mathbb{E}[X \\mid \\mathcal{G}]\\).\n\nProposition 9.2 Let \\(X \\in \\mathcal{L}_2(\\Omega, \\mathcal{F}, P)\\) and \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) a sub-\\(\\sigma\\)-algebra. Then\n\\[\\operatorname{Var} [X] = \\operatorname{Var} \\Big[ \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\Big] + \\mathbb{E} \\Big[ \\operatorname{Var} \\left[ X \\mid \\mathcal{G} \\right] \\Big] \\, .\\]\n\n\nProof. Recall that \\(\\mathbb{E} \\left[ \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\right] = \\mathbb{E} \\left[ X \\right]\\).\n\\[\\begin{align*}\n\\operatorname{Var} \\left[ X \\right] & =  \\mathbb{E} \\left[ \\left( X -\n\\mathbb{E} \\left[ X \\right] \\right)^2 \\right]\\\\\n& =  \\mathbb{E} \\left[ \\left( X - \\mathbb{E} \\left[ X \\mid\n\\mathcal{G} \\right] + \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] -\n\\mathbb{E} \\left[ X \\right] \\right)^2 \\right]\\\\\n& =  \\mathbb{E} \\left[ \\left( X - \\mathbb{E} \\left[ X \\mid\n\\mathcal{G} \\right] \\right)^2 \\right]\\\\\n&  \\qquad + 2 \\mathbb{E} \\left[ \\left( X - \\mathbb{E} \\left[ X \\mid\n\\mathcal{G} \\right] \\right) \\left( \\mathbb{E} \\left[ X \\mid \\mathcal{G}\n\\right] - \\mathbb{E} \\left[ X \\right] \\right) \\right]\\\\\n&  \\qquad + \\mathbb{E} \\left[ \\left( \\mathbb{E} \\left[ X \\mid\n\\mathcal{G} \\right] - \\mathbb{E} \\left[ X \\right] \\right)^2 \\right]\\\\\n& =  \\mathbb{E} \\left[ \\mathbb{E} \\left[ \\left( X - \\mathbb{E}\n\\left[ X \\mid \\mathcal{G} \\right] \\right)^2 \\mid \\mathcal{G} \\right]\n\\right]\\\\\n&  \\qquad + 2 \\mathbb{E} \\Big[ \\mathbb{E} \\left[ \\left( X -\n\\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\right) \\mid \\mathcal{G}\n\\right]  \\left( \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] -\n\\mathbb{E} \\left[ X \\right] \\right) \\Big]\\\\\n&  \\qquad + \\operatorname{Var} \\left[ \\mathbb{E} \\left[ X \\mid \\mathcal{G}\n\\right] \\right]\\\\\n& =  \\mathbb{E} \\left[ \\operatorname{Var} \\left[ X \\mid \\mathcal{G} \\right]\n\\right] + \\operatorname{Var} \\left[ \\mathbb{E} \\left[ X \\mid \\mathcal{G}\n\\right] \\right] .\n\\end{align*}\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#conditional-expectation-in-mathcall_1-omega-mathcalf-p",
    "href": "06-conditioning.html#conditional-expectation-in-mathcall_1-omega-mathcalf-p",
    "title": "9  Conditioning",
    "section": "9.3 Conditional expectation in \\(\\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\)",
    "text": "9.3 Conditional expectation in \\(\\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\)\nTo construct the conditional expectation of a random variable, square-integrability is not necessary. This is the meaning of the next theorem.\n\nTheorem 9.3 If \\(Y \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F},\nP)\\), then there exists an integrable \\(\\mathcal{G}\\)-measurable random variable, denoted by \\(\\mathbb{E} \\left[ Y \\mid \\mathcal{G} \\right]\\) such that\n\\[\\forall B \\in \\mathcal{G}, \\mathbb{E} \\left[ \\mathbb{I}_B Y \\right] = \\mathbb{E} \\left[ \\mathbb{I}_B \\mathbb{E} \\left[ Y \\mid \\mathcal{G}\\right] \\right]  .\\]\n\nIn words, conditional expectations according to Definition 9.1 exist for all integrable random variables and all sub-\\(\\sigma\\)-algebras.\n\nExercise 9.1 Let \\(\\mathcal{G}'\\) be a \\(\\pi\\)-system that contains \\(\\Omega\\) and generates \\(\\mathcal{G}\\). If \\(Z\\) is an integrable \\(\\mathcal{G}\\)-measurable variable that satisfies\n\\[\\forall B \\in \\mathcal{G}', \\mathbb{E} \\left[ \\mathbb{I}_B Y \\right] = \\mathbb{E} \\left[ \\mathbb{I}_B \\mathbb{E} \\left[ Y \\mid \\mathcal{G} \\right] \\right]\\]\nthen \\(Z = \\mathbb{E} \\left[ Y \\mid \\mathcal{G} \\right] .\\)\n\nTo establish the Theorem 9.3, we use the usual machinery of limiting arguments.\n\nProposition 9.3 If \\((Y_n)_n\\) is a non-decreasing sequence of non-negative square-integrable random variables such as \\(Y_n \\uparrow Y\\) a.s. then there exists a \\(\\mathcal{G}\\)-measurable random variable \\(Z\\) such that\n\\[\\mathbb{E} \\left[ Y_n \\mid \\mathcal{G} \\right] \\uparrow Z \\qquad \\text{a.s.}\\]\n\n\nProof. As \\((Y_n)_n\\) is non-decreasing, according to Proposition 9.1 \\(\\left( \\mathbb{E} \\left[ Y_n \\mid \\mathcal{G} \\right] \\right)_n\\) is an (a.s.) non-decreasing sequence of \\(\\mathcal{G}\\)-measurable random variables, it admits a \\(\\mathcal{G}\\)-measurable limit (finite or not).\n\\(\\square\\)\n\nWe now proceed to the proof of Theorem 9.3.\n\nProof. Without losing in generality, we assume \\(Y \\geq 0\\) (if this is not the case, let \\(Y = (Y)_+ - (Y)_-\\) with \\((Y)_+ = |Y| \\mathbb{I}_{Y\n\\geq 0}\\) and \\((Y)_- = |Y| \\mathbb{I}_{Y &lt; 0}\\), handle \\((Y)_+\\) and \\((Y)_-\\) separately and use the linearity of conditional expectation).\nLet \\[ Y_n = Y \\mathbb{I}_{|Y| \\leq n}\\] so that \\(Y_n \\nearrow Y\\) everywhere. The random variable \\(Y_n\\) is bounded and thus square-integrable. The random variable\n\\(\\mathbb{E}\\left[ Y_n \\mid \\mathcal{G} \\right]\\) is therefore well defined for each \\(n\\).\nThe sequence \\(\\mathbb{E} \\left[ Y_n \\mid \\mathcal{G} \\right]\\) is \\(P\\)-a.s. monotonous. It converges monotonously towards a \\(\\mathcal{G}\\)-measurable random variable \\(Z\\) which takes values in \\(\\mathbb{R}_+ \\cup \\{\\infty\\}\\). We need to check that this random variable \\(Z \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\).\nBy monotonous convergence: \\[\\begin{align*}\n\\mathbb{E} Y\n& = \\mathbb{E}\\big[ \\lim_n  \\uparrow Y_n\\big] \\\\\n& = \\lim_n  \\uparrow \\mathbb{E}\\big[  Y_n\\big] \\\\\n& = \\lim_n  \\uparrow \\mathbb{E} \\Big[ \\mathbb{E} \\left[ Y_n \\mid \\mathcal{G} \\right] \\Big]  \\\\\n& = \\mathbb{E} \\Big[ \\lim_n \\uparrow  \\mathbb{E} \\left[ Y_n \\mid \\mathcal{G} \\right] \\Big] \\\\\n& = \\mathbb{E} Z  \\, .\n\\end{align*}\\] If \\(A \\in \\mathcal{G}\\), by monotonous convergence, \\[\\lim_n \\uparrow \\mathbb{E} \\left[ \\mathbb{I}_A Y_n \\right] = \\mathbb{E}  \\left[ \\mathbb{I}_A Y \\right]\\] and so \\[\\lim_n \\uparrow  \\mathbb{E} \\left[ \\mathbb{I}_A \\mathbb{E} \\left[ Y_n \\mid\n\\mathcal{G} \\right] \\right] = \\mathbb{E} \\left[ \\mathbb{I}_A\nY \\right].\\] By monotonous convergence again: \\[\\lim_n \\uparrow  \\mathbb{E} \\left[ \\mathbb{I}_A \\lim_n \\mathbb{E} \\left[ Y_n \\mid\n\\mathcal{G}\\right] \\right] = \\mathbb{E} \\left[ \\mathbb{I}_A Z\n\\right]\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#properties-of-general-conditional-expectation",
    "href": "06-conditioning.html#properties-of-general-conditional-expectation",
    "title": "9  Conditioning",
    "section": "9.4 Properties of (general) conditional expectation",
    "text": "9.4 Properties of (general) conditional expectation\n\nRemark 9.2. In this Section \\((\\Omega, \\mathcal{F}, P)\\) is a probability space, \\(\\mathcal{G}\\) is a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}\\). Random variables \\((X_n)_n, (Y_n)_n, X, Y, Z\\) are meant to be integrables, and a.s. means \\(P\\)-a.s.\n\nThe easiest property is:\n\nIf \\(X \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\) then\n\\[\\mathbb{E} \\left[ X \\right] = \\mathbb{E} \\left[ \\mathbb{E}   \\left[ X \\mid \\mathcal{G} \\right] \\right].\\]\n\n\nExercise 9.2 Prove it.\n\n\nIf \\(X \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\) and \\(X\\) is \\(\\mathcal{G}\\)-measurable then\n\\[X = \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]  \\hspace{1em} P\\text{-a.s.}\\]\n\n\nExercise 9.3 Prove it.\n\nUsing the definition of conditional expectation and monotone approximation by simple functions (see Section 3.2)), we obtain an alternative characterization of conditional expectation.\n\nLet \\(X \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\) and \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) be a sub-\\(\\sigma\\)-algebra, then for every \\(Y \\in\\mathcal{L}_1 (\\Omega, \\mathcal{G}, P)\\), such that \\(\\mathbb{E} \\left[ |X Y| \\right] &lt; \\infty\\)\n\\[\\mathbb{E} \\left[ XY \\right] = \\mathbb{E} \\left[ Y \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\right] .\\]\n\n\nExercise 9.4 Prove it.\n\nWe pocket the next proposition for future and frequent use. We could go ahead with listing many other useful properties of conditional expectation. They are best discovered and established when needed.\n\nIf \\(X, Y \\in \\mathcal{L}_1 (\\Omega, \\mathcal{F}, P)\\) and \\(Y\\) is \\(\\mathcal{G}\\)-measurable then \\[ \\mathbb{E} \\left[ XY \\mid \\mathcal{G} \\right] = Y \\mathbb{E} \\left[X \\mid \\mathcal{G} \\right]  \\hspace{1em} P \\text{-a.s.}\\]\n\n\nProof. As \\(Y \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]\\) is \\(\\mathcal{G}\\)-measurable, it suffices to check that for every \\(B \\in\\mathcal{G}\\),\n\\[ \\mathbb{E} \\left[ \\mathbb{I}_B XY \\right] = \\mathbb{E} \\left[\\mathbb{I}_B \\left( Y \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]\\right) \\right] .\\]\nBut\n\\[\\begin{eqnarray*}\n\\mathbb{E} \\left[ \\mathbb{I}_B XY \\right] & = & \\mathbb{E} \\left[\n( \\mathbb{I}_B Y) X \\right]\\\\\n& = & \\mathbb{E} \\left[ ( \\mathbb{I}_B Y) \\mathbb{E} \\left[ X\n\\mid \\mathcal{G} \\right] \\right]\\\\\n& = & \\mathbb{E} \\left[ \\mathbb{I}_B \\left( Y \\mathbb{E} \\left[ X\n\\mid \\mathcal{G} \\right] \\right) \\right] .\n\\end{eqnarray*}\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#sec-condconvtheorems",
    "href": "06-conditioning.html#sec-condconvtheorems",
    "title": "9  Conditioning",
    "section": "9.5 Conditional convergence theorems",
    "text": "9.5 Conditional convergence theorems\nLimit theorems from integration theory (monotone convergence theorem, Fatou’s Lemma, Dominated convergence theorem) can be adapted to the conditional expectation setting.\n\nTheorem 9.4 (Monotone convergence) Let the sequence \\((X_n)_n\\) of non-negative random variables converge monotonously to \\(X\\) (\\(X_n \\uparrow X\\) a.s.), with \\(X\\) integrable, then for every sequence of versions of conditional expectations:\n\\[\\lim_n \\uparrow \\mathbb{E} \\left[ X_n \\mid \\mathcal{G} \\right] = \\mathbb{E}  \\left[ X \\mid \\mathcal{G} \\right]  \\text{ a.s.}\\]\n\n\nProof. The sequence \\(X - X_n\\) is non-negative and decreases to \\(0\\) a.s. It suffices to show that \\(\\lim_n \\downarrow \\mathbb{E} \\left[ X - X_n \\mid\n\\mathcal{G} \\right] = 0\\) a.s. Note first that the sequence \\(\\mathbb{E} \\left[ X - X_n \\mid \\mathcal{G} \\right]\\) converges a.s. toward a non-negative limit. We need to check that this limit is a.s. zero.\nFor \\(A \\in \\mathcal{G}\\) : \\[\\begin{align*}\n\\mathbb{E} \\left[ \\mathbb{I}_A \\lim_n \\mathbb{E} \\left[ X - X_n\n\\mid \\mathcal{G} \\right] \\right] & =  \\lim_n \\mathbb{E} \\left[\n\\mathbb{I}_A  \\mathbb{E} \\left[ X - X_n \\mid \\mathcal{G} \\right]\n\\right]\\\\\n&  \\qquad \\text{ monotone convergence theorem}\\\\\n& = \\lim_n \\mathbb{E} \\left[ \\mathbb{I}_A \\left( X_n - X \\right)\n\\right]\\\\\n&  \\qquad \\text{ monotone convergence theorem}\\\\\n& =  0 \\, .\n\\end{align*}\\]\n\\(\\square\\)\n\n\nTheorem 9.5 (Conditional Fatou’s Lemma) Let \\((X_n)_n\\) be a sequence of non-negative random variables, then\n\\[\\mathbb{E} \\left[ \\liminf_n X_n \\mid \\mathcal{G} \\right] \\leq \\liminf_n \\mathbb{E} \\left[ X_n \\mid \\mathcal{G} \\right]  \\hspace{1em} \\text{a.s.}\\]\n\nAs for the proof of Fatou’s Lemma, the argument boils down to monotone convergence arguments.\n\nProof. Let \\(B \\in \\mathcal{G}\\). Let \\(X = \\liminf_n X_n\\), \\(X\\) is a non-negative random variable. Let \\(Y = \\liminf_n \\mathbb{E} \\left[\nX_n \\mid \\mathcal{G} \\right]\\), \\(Y\\) is a \\(\\mathcal{G}\\)-measurable integrable random variable. The theorem compares \\(\\mathbb{E} \\left[ X\n\\mid \\mathcal{G} \\right]\\) and \\(Y.\\)\nLet \\(Z_k = \\inf_{n \\geq k} X_n\\). Thus \\(\\lim_k \\uparrow Z_k =  \\liminf_n X_n = X\\). According to Theorem 9.4, \\[\n\\mathbb{E} \\left[ Z_k \\mid \\mathcal{G} \\right] \\uparrow_k\n\\mathbb{E} \\left[ \\liminf_n X_n \\mid \\mathcal{G} \\right]\n\\text{ a.s.} \\] For every \\(n \\geq k\\), \\(X_n \\geq Z_k\\) a.s. Hence by the comparison Theorem (Corollary 9.1)),\n\\[\\forall n \\geq k \\hspace{1em} \\mathbb{E} \\left[ Z_k \\mid \\mathcal{G} \\right] \\leq \\mathbb{E} \\left[ X_n \\mid \\mathcal{G}\\right]   \\text{ a.s.}\\]\nas a countable union of \\(P\\)-negligible events is \\(P\\)-negligible. Hence for every \\(k\\), \\[\\mathbb{E} \\left[ Z_k \\mid \\mathcal{G} \\right] \\leq \\liminf_n \\mathbb{E} \\left[ X_n \\mid \\mathcal{G} \\right]  \\hspace{1em} \\text{a.s.}\\] This entails \\[\\lim_k \\uparrow \\mathbb{E} \\left[ Z_k \\mid \\mathcal{G} \\right] \\leq\\liminf_n  \\mathbb{E} \\left[ X_n \\mid \\mathcal{G} \\right]\\quad  \\text{ a.s.}\\]\n\\(\\square\\)\n\n\n\n9.5.1 Dominated convergence\nLet \\(V \\in \\mathcal{L}_1(\\Omega, \\mathcal{F}, P)\\). Let sequence \\((X_n)_n\\) satisfy \\(|X_n | \\leq V\\) for every \\(n\\) and \\(X_n \\rightarrow X \\text{a.s.}\\), then for any sequence of versions of conditional expectations of \\((X_n)_n\\) and \\(X\\) \\[\\mathbb{E} \\left[ X_n \\mid \\mathcal{G} \\right] \\rightarrow \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right]  \\hspace{1em} \\text{a.s.}\\]\n\n\n\nProof. Let \\(Y_n = \\inf_{m \\geq n} X_m\\) and \\(Z_n = \\sup_{m \\geq n} X_m\\). Hence \\(-V \\leq Y_n \\leq Z_n \\leq V\\). As \\(Y_n \\uparrow X\\) and \\(Z_n \\downarrow X\\). By the conditional monotone convergence Theorem (Theorem 9.4)), \\(\\mathbb{E} \\left[ Y_n \\mid \\mathcal{G} \\right] \\uparrow\n\\mathbb{E} [X \\mid \\mathcal{G}]\\) and  \\(\\mathbb{E} \\left[ Z_n \\mid\n\\mathcal{G} \\right] \\downarrow \\mathbb{E} [X \\mid \\mathcal{G}] \\text{p.s}\n.\\) Observe that for every \\(n\\)\n\\[\\mathbb{E} \\left[ Y_n \\mid \\mathcal{G} \\right] \\leq \\mathbb{E}\n\\left[ X_n \\mid \\mathcal{G} \\right] \\leq \\mathbb{E} \\left[ Z_n\n\\mid \\mathcal{G} \\right]  \\hspace{1em} \\text{a.s.}\\]\n\\(\\square\\)\n\nJensen’s inequality also has a conditional version. The proof relies again on the variational representation of convex lower semi-comntinuous functions and on the monotonicity property of conditional expectation (Corollary 9.1)).\n\n\n9.5.2 Jensen’s inequality\nIf \\(g\\) is a lower semi-continuous convex function on \\(\\mathbb{R}\\), with \\(\\mathbb{E} \\left[ | g (X) | \\right] &lt; \\infty\\) then\n\\[g \\left( \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\right) \\leq\n\\mathbb{E} \\left[ g (X) \\mid \\mathcal{G} \\right]  \\text{a.s.} .\\]\n\n\n\nProof. A lower semi-continuous convex function is a countable supremum of affine functions: there exists a countable collection \\((a_n, b_n)_n\\) such that for every \\(x\\): \\[g (x) = \\sup_n  \\left[ a_n x + b_n \\right] .\\]\n\\[\\begin{eqnarray*}\ng \\left( \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\right) & = &\n\\sup_n \\left[ a_n \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] + b_n\n\\right] \\\\\n& = & \\sup_n \\left[ \\mathbb{E} \\left[ a_n X + b_n \\mid \\mathcal{G}\n\\right] \\right]\\\\\n& \\leq & \\mathbb{E} \\left[ \\sup_n \\left( a_n X + b_n \\right) \\mid\n\\mathcal{G} \\right] P \\text{-a.s.}\\\\\n\\end{eqnarray*}\\]\n\\(\\square\\)\n\n\n9.5.3 Independence\nWhen the conditioning \\(\\sigma\\)-algebra \\(\\mathcal{G}\\) is atomic, if the conditioned random variable \\(X\\) is independent from the conditioning \\(\\sigma\\)-algebra, it is obvious that the conditional expectation is an a.s. constant random variable which value equals \\(\\mathbb{E}X\\). This remains true in the general framework. It deserves a proof.\n\nProposition 9.4 If \\(X\\) is independent from \\(\\mathcal{G}\\), then\n\\[\\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] = \\mathbb{E} \\left[ X\\right] .\\]\n\n\nProof. Note that \\(\\mathbb{E} \\left[ X \\right]\\) is \\(\\mathcal{G}\\)-measurable. Let \\(B \\in \\mathcal{G}\\),\n\\[\\begin{align*}\n\\mathbb{E} \\left[ \\mathbb{I}_B X \\right]\n& =  \\mathbb{E} \\left[ \\mathbb{I}_B \\right]  \\mathbb{E} \\left[ X \\right]\\\\\n&  \\qquad \\text{by  independence} \\\\\n& =  \\mathbb{E} \\left[ \\mathbb{I}_B \\times \\mathbb{E} \\left[ X\n\\right] \\right] .\n\\end{align*}\\]\nHence \\(\\mathbb{E} \\left[ X \\right] = \\mathbb{E} \\left[ X \\mid\\mathcal{G} \\right]\\).\n\\(\\square\\)\n\nProposition 9.4 can be generalized to a more general setting.\n\nIf sub-\\(\\sigma\\)-algebra \\(\\mathcal{H}\\) is independent from \\(\\sigma (\\mathcal{G}, \\sigma (X))\\) then\n\\[\\mathbb{E} \\left[ X \\mid \\sigma ( \\mathcal{G}, \\mathcal{H}) \\right] = \\mathbb{E}\\left[ X \\mid \\mathcal{G} \\right] \\hspace{1em} \\text{a.s.}\\]\n\n\nProof. Recall that conditional expectation with respect to \\(\\sigma(\n\\mathcal{G}, \\mathcal{H})\\) can be characterized using a \\(\\pi\\)-system containing \\(\\Omega\\) and generating \\(\\sigma \\left( \\mathcal{G,\nH} \\right)\\), for example \\(\\mathcal{G} \\times \\mathcal{H}\\). Let \\(B \\in\n\\mathcal{G}\\) and \\(C \\in \\mathcal{H}\\),\n\\[\\begin{align*}\n\\mathbb{E} \\left[ \\mathbb{I}_B  \\mathbb{I}_C  \\mathbb{E} \\left[\nX \\mid \\mathcal{G} \\right] \\right]\n& =  \\mathbb{E} \\left[\\mathbb{I}_B  \\mathbb{E} \\left[ X \\mid \\mathcal{G} \\right] \\right]\n      \\times \\mathbb{E} \\left[ \\mathbb{I}_C  \\right]\\\\\n&   \\qquad C \\text{ is independent from }     \\sigma ( \\mathcal{G}, \\sigma (X))\\\\\n& =  \\mathbb{E} \\left[ \\mathbb{I}_B X \\right] \\times \\mathbb{E}\\left[ \\mathbb{I}_C  \\right]\\\\\n& =  \\mathbb{E} \\left[ \\mathbb{I}_C  \\mathbb{I}_B X \\right] \\\\\n&  \\qquad C  \\text{ is independent from }  \\sigma ( \\mathcal{G}, \\sigma (X))\\,  .\n\\end{align*}\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#sec-condProbDistrib",
    "href": "06-conditioning.html#sec-condProbDistrib",
    "title": "9  Conditioning",
    "section": "9.6 Conditional probability distributions",
    "text": "9.6 Conditional probability distributions\n\n9.6.1 Easy case: conditioning with respect to a discrete \\(\\sigma\\)-algebra\nWe come back to the basic setting: \\((\\Omega,\\mathcal{F},P)\\) refers to a probability space while \\(\\mathcal{G}\\subseteq \\mathcal{F}\\) denotes an atomic sub-\\(\\sigma\\)-algebra generated by a countable partition \\((A_n)_n\\) of \\(\\Omega.\\)\nEither from conditional expectations with respect to \\(\\mathcal{G}\\), or from conditional probabilities knowing the events \\(A_n,\\) we can define a \\(N\\) function of \\(\\Omega \\times \\mathcal{F}\\) per\n\\[N(\\omega, B) = \\mathbb{E}_{P}[\\mathbb{I}_B\\mid \\mathcal{G}](\\omega) =\nP\\{B\\mid A_n\\}\\text{  when } \\omega \\in A_n \\, .\\]\nThe \\(N\\) function has two remarkable properties:\n\nFor every \\(\\omega \\in \\Omega,\\) \\(N(\\omega,\\cdot)\\) defines a probability on \\((\\Omega,\\mathcal{F}).\\)\nFor every event \\(B\\in \\mathcal{F},\\) the function \\(N(\\cdot,B)\\) is a \\(\\mathcal{G}\\)-measurable function.\n\nIn this simple atomic setting, we observe that while it is intuitive to define conditional expectation starting from conditional probabilities, we can also proceed the other way around: we can build conditional probabilities starting from conditional expectations.\n\n\n9.6.2 Impediments\nIn the general case, we attempt to construct conditional probabilities when the conditioning \\(\\sigma\\)-algebra is not atomic.\nFor each \\(B \\in \\mathcal{F}\\), we can rely on the existence of random variable \\(\\sigma (X)\\)-measurable which is \\(P\\)-a.s. a version of the conditional expectation of \\(\\mathbb{I}_B\\) with respect to \\(X\\). Indeed, for any kind of countable collection of events \\((B_n)_n\\) of \\(\\mathcal{F}\\), we can take for granted that there exists a collection of random variables which, almost surely, form a consistent collection of versions of the expectation of \\((\\mathbb{I}_{B_n})_n\\) with respect to \\(X\\). If \\((B_n)_n\\) is non-decreasing tending towards \\(B\\), by Theorem 9.4), we are confident in the fact that the following holds\n\\[\\lim_n \\uparrow \\mathbb{E} \\left[ \\mathbb{I}_{B_n} \\mid X \\right]  = \\mathbb{E}\\left[ \\mathbb{I}_B \\mid X \\right] \\qquad \\text{a.s.}\\]\nIt is therefore tempting to define a conditional probability with respect to \\(\\sigma(X)\\) as a function\n\\[\\begin{align*}\n\\Omega \\times \\mathcal{F} &\n\\to [0, 1] \\\\\n(\\omega, B) & \\mapsto \\mathbb{E} \\left[ \\mathbb{I}_B \\mid\n\\sigma(X) \\right](\\omega) \\, .\n\\end{align*}\\]\nUnfortunately, we cannnot guarantee that \\(P\\)-a.s., this object has the properties of a probability distribution \\((\\Omega, \\mathcal{F})\\). The problem does not arise from the diffuse nature of the distribution of \\(X\\) but from the size of \\(\\mathcal{F}\\). As \\(\\mathcal{F}\\) may not be countable, it is possible to build an uncountable non-decreasing sequence of events. Checking the a.s. monotonicity of the sequence of corresponding conditional probabilities looks beyond our reach (an uncountable union of \\(P\\)-negligible events is not necessarily \\(P\\)-negligible).\nFortunately, the situation is not desperate. In most settings envisioned in an introductory course on Probability, we can take the existence of condition probabilities for granted.\nIn Section 9.6.3), we first review the easy case, where we can define conditional probabilities that even have a density with respect to a reference measure. In Section 9.6.4) we shall see that if \\(\\Omega\\) is not too large, we can rely on the existence of conditional probabilities.\n\n\n9.6.3 Joint density setting\nIf \\(\\Omega = \\mathbb{R}^k\\), \\(\\mathcal{F} = \\mathcal{B}(\\mathbb{R}^k)\\) and the probability distribution \\(P\\) is absolutely continuous with respect to Lebesgue measure (has a density denoted by \\(p\\)), defining conditional density with respect to coordinate projections is almost as simple as conditioning with respect to an atomic \\(\\sigma\\)-algebra.\nFor the sake of simplicity, we stick to the case \\(k=2\\). A generic outcome is denoted by \\(\\omega = (x, y)\\) and the coordinated projections define two random variables \\(X(x, y) = x\\) and \\(Y (x, y) = y\\). We denote by \\(p_X\\) the marginal density of the distribution of \\(X\\):\n\\[p_X (x) = \\int_{\\mathcal{\\mathbb{R}}} p (x, y) \\mathrm{d} y .\\]\nAnd we agree on \\(D =\\{x : p_X (x) &gt; 0\\}\\). This is the support of the density \\(p_X\\) (beware, this may be different from the support of distribution \\(P \\circ X^{- 1}\\)).\n\nExercise 9.5 Check that \\(p_X\\) is the density of \\(P \\circ X^{- 1}\\).\n\nHaving a density allows us to calculate conditional expectation and to define just as easily what we will call a conditional probability of \\(Y\\) knowing \\(X\\).\n\nTheorem 9.6 (Conditional density) Let be \\(X, Y\\) be the projection coordinates on \\(\\mathbb{R}^2\\). Let \\(P\\) be an absolutely continuous distribution on \\((\\mathbb{R}^2, \\mathcal{B}(\\mathbb{R}^2))\\) with density \\(p (., .)\\) with respect to Lebesgue’s measure. Let the first marginal density (density of \\(P \\circ X^{-1}\\) be denoted by \\(p_X\\).\nThe function\n\\[\\begin{align*}\nN: \\qquad\\mathbb{R}^2 &  \\to  [0,\\infty)  \\\\\n    (x, y) &  \\mapsto N (x, y) =\n\\begin{cases}\n\\frac{p (x, y)}{p_X (x)} & \\text{if } p_X (x) &gt; 0\\\\\n0 &\\text{ otherwise,}\n\\end{cases}\n\\end{align*}\\]\nsatisfies the following properties.\n\nFor each \\(x\\) such that \\(p_X (x) &gt; 0\\), the set function \\(P_{\\cdot \\mid X=x}\\) defined by \\[\\begin{align*}\n\\mathcal{B}(\\mathbb{R}^2) & \\to [0, 1]\\\\\nB & \\mapsto P_{\\cdot \\mid X=x} \\{B\\} = \\int_{\\mathbb{R}} \\mathbb{I}_B(x,y) N (x, y) \\mathrm{d} y\n\\end{align*}\\] is a probability measure on \\((\\mathbb{R}^2, \\mathcal{B} ( \\mathbb{R}^2))\\). This probability distribution is supported by \\(\\{x\\} \\times \\mathbb{R}\\).\nFor every \\(B \\in \\mathcal{B} ( \\mathbb{R}^2)\\), the function \\[\n\\omega \\mapsto \\int_{\\mathbb{R}} \\mathbb{I}_B(X(\\omega),y) N (X(\\omega), y) \\mathrm{d} y\n= \\mathbb{E}_{P_{\\cdot \\mid X=X(\\omega)}} \\mathbb{I}_B\n\\] for \\(\\Omega = \\mathbb{R}^2\\), is \\(\\sigma(X)\\)-measurable and may be called a version of \\(\\mathbb{E}\\big[\\mathbb{I}_B \\mid \\sigma(X)\\big]\\).\nFor every \\(B \\in \\mathcal{B} ( \\mathbb{R}^2)\\) \\[\nP(B) = \\int \\left( \\int \\mathbb{I}_B (s,y) N (s,y) \\mathrm{d} y\n\\right) p_X (s) \\mathrm{d} s =  \\int P_{\\cdot \\mid X=s}(B) p_X(s) \\mathrm{d} s .\n\\]\nFor any \\(P\\)-integrable function \\(f\\) on \\(\\mathbb{R}^2\\), the random variable defined by applying \\[\nx \\mapsto \\int_{\\mathbb{R}} f (x, y) N (x, y) \\mathrm{d} y\n\\] to \\(X\\) is a version of the conditional expectation of \\(f(X, Y)\\) with respect to \\(\\sigma (X).\\)\n\n\n\nRemark 9.3. For each \\(x\\) such that \\(p_X(x)&gt;0\\), \\(P_{\\cdot \\mid X=x}\\) is a probability on \\(\\mathbb{R}^2\\). But this probability measure is supported by \\(\\{x\\} \\times \\mathbb{R}\\), it is the product of the Dirac mass in \\(\\{x\\}\\) times the probability distribution on \\(\\mathbb{R}\\) defined by the density \\(N(x, \\cdot)\\). This is why \\(N(x, \\cdot)\\) is often called the conditional density of \\(Y\\) given \\(X=x\\), and the distribution over \\(\\mathbb{R}\\) defined by this density is often called the conditional distribution of \\(Y\\) given \\(X\\).\n\n\nExercise 9.6 Is \\(N(x,y)\\) a probability density? If yes, with respect to which \\(\\sigma\\)-finite measure?\n\nThe proof of Theorem 9.6) consists of milking the Tonelli-Fubini Theorem.\n\nProof. Proof of (i). Let us agree on notation: \\[ P_x \\{B\\}= \\int_{\\mathbb{R}} \\mathbb{I}_B(x,y) N (x, y) \\mathrm{d} y . \\] The fact that the \\(P_x\\) is \\([0, 1]\\)-valued is immediate. Same for the fact that \\(P_x (\\{x\\} \\times \\{\\emptyset\\}) = 0\\) and \\(P_x (\\{x\\} \\times \\{\\mathbb{R}\\}) = 1\\). The same applies to additivity.\nIt remains to check that if \\((B_n)\\) is a non-decreasing sequence of Borelians from \\(\\mathbb{R}^2\\) that tends to to a limit \\(B\\) then \\[\n\\lim_n \\uparrow P_x (B_n) = P_x (B) \\, .\n\\] This is an immediate consequence of the monotonous convergence theorem, for each \\((x',y')\\) \\(\\lim_n \\uparrow \\mathbb{I}_{B_n} (x', y') N (x', y') =  \\mathbb{I}_{B} (x', y') N (x', y')\\).\nProof of ii) As the function \\((x, y) \\mapsto p (x, y) \\mathbb{I}_B (x,y)\\) is \\(\\mathcal{B} ( \\mathbb{R}^2)\\)-measurable and integrable, by the Tonelli-Fubini Theorem, \\[ x \\mapsto \\int_B p (x, y) \\mathbb{I}_B (x,y) \\mathrm{d} y \\] is defined almost everywhere and Borel-measurable.\nProof of iii) This is also an immediate consequence of the Tonelli-Fubini Theorem.\nProof of iv), It follows from i.), using the usual approximation by simple functions argument.\n\\(\\square\\)\n\n\nExercise 9.7 We consider the uniform law on the surface of \\(\\mathbb{R}^2\\) defined by \\(0 \\leq x \\leq y \\leq y \\leq 1\\). Give the attached density \\(p()\\), the marginal density \\(p_X\\) and the kernel \\(N (,) .\\)\n\n\n\n9.6.4 Regular conditional probabilities, kernels\nWe will outline some results that allow us to work within a more general framework. We introduce two new notions.\n\n\n\n9.6.5 Conditional probability kernel\nLet \\((\\Omega, \\mathcal{F})\\) be a measurable space, and \\(\\mathcal{G}\\) a sub-\\(\\sigma\\)-algebra of \\(\\mathcal{F}.\\)\nWe call conditional probability kernel with respect to \\(\\mathcal{G}\\) a function \\(N : \\Omega \\times \\mathcal{F} \\rightarrow\n\\mathbb{R}_+\\) that satisfies:\n\nFor any \\(\\omega \\in \\Omega\\), \\(N (\\omega, \\cdot)\\) defines a probability on \\((\\Omega, \\mathcal{F})\\).\nFor any \\(A \\in \\mathcal{F}\\), \\(N (\\cdot, A)\\) is \\(\\mathcal{G}\\)-measurable\n\n\n\nIf the measurable space is endowed with a probability distribution \\(P\\), we are interested in conditional probability kernels with respect to \\(\\mathcal{G}\\) that are compliant with \\(P\\). We call them regular conditional probability kernels.\n\n\n9.6.6 Regular conditional probability\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) a sub-\\(\\sigma\\)-algebra. A kernel \\(N : \\Omega \\times \\mathcal{F} \\to \\mathbb{R}_+\\) is a regular conditional probability with respect \\(\\mathcal{G}\\) if and only if\n\nFor any \\(B \\in \\mathcal{F}\\), \\(\\omega \\mapsto N (\\omega, B)\\) is a version of the conditional expectation of \\(\\mathbb{I}_B\\) knowing \\(\\mathcal{G}\\) (\\(N (\\cdot, B)\\) is therefore \\(\\mathcal{G}\\)-measurable): \\[\nN (\\cdot, B) = \\mathbb{E}[\\mathbb{I}_B \\mid \\mathcal{G}]\\quad P-\\text{a.s.}\n\\]\nFor \\(P\\)-almost all \\(\\omega \\in \\Omega\\), \\(B \\mapsto N\n(\\omega, B)\\) defines a probability on \\((\\Omega, \\mathcal{F})\\).\n\n\n\nA regular conditional probability (whenever it exists) is defined from versions of conditional expectations. Conversely, a regular conditional probability provides us with a way to to compute conditional expectations.\n\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\(\\mathcal{G} \\subseteq \\mathcal{F}\\) a sub-\\(\\sigma\\)-algebra. Let \\(N\\) be a probability kernel on \\((\\Omega,\\mathcal{F})\\) with respect to \\(\\mathcal{G}\\).\nThe following properties are equivalent\n\n\\(N(\\cdot,\\cdot)\\) defines a regular conditional probability kernel with respect to \\(\\mathcal{G}\\) for \\((\\Omega, \\mathcal{G}, P)\\).\n\\(P\\)-almost surely, for any \\(P\\)-integrable function \\(f\\) on \\((\\Omega, \\mathcal{F})\\): \\[\n\\mathbb{E} \\left[ f \\mid \\mathcal{G} \\right](\\omega) =\n\\mathbb{E}_{N(\\omega,\\cdot)}[f] \\, .\n\\]\nFor any \\(P\\)-integrable random variable \\(X\\) on \\((\\Omega, \\mathcal{F})\\) \\[ \\mathbb{E} \\left[ X \\right] =\n\\mathbb{E}\\left[ \\mathbb{E}_{N(\\omega,\\cdot)}[X]\\right] \\, .\n\\]\n\n\n\nRemark 9.4. The proof of \\(1) \\Rightarrow 2)\\) relies on the usual machinery: approximation of positive integrable functions by an increasing sequences of simple functions, monotone convergence of expectation and conditional expectation.\n\\(2) \\Rightarrow 3)\\) is trivial.\n\\(3) \\Rightarrow 1)\\) is more interesting.\n\n\n9.6.7 Existence of regular conditional probability distributions when \\(\\Omega =\\mathbb{R}\\)\nWe shall check the existence of conditional probabilities in at least one non-trivial case.\n\nLet \\(P\\) be a probability on \\(( \\mathbb{R}, \\mathcal{B} (\n\\mathbb{R}))\\) and \\(\\mathcal{G} \\subseteq \\mathcal{B} (\n\\mathbb{R})\\), then there exists a regular conditional probability kernel with respect to \\(\\mathcal{G}.\\)\n\n\nWe take advantage of the fact that \\(\\mathcal{B} (\\mathbb{R})\\) is countably generated.\n\n\nProof. Let \\(\\mathcal{C}\\) be the set formed by half-lines with rational endpoint, the empty set, and \\(\\mathbb{R}\\): \\[\n  \\mathcal{C}  = \\big\\{ (-\\infty, q]:  q \\in \\mathbb{Q} \\big\\} \\cup \\{\\emptyset, \\mathbb{R} \\}\\, .\n\\] This countable collection of half-lines is a \\(\\pi\\)-system (See Section 2.6)) that generates \\(\\mathcal{B}(\\mathbb{R})\\).\nFor \\(q &lt; q' \\in \\mathbb{Q},\\) we can choose versions of \\(Y_q\\) and \\(Y_{q'}\\) of the conditional expectations of \\(\\mathbb{I}_{(-\n\\infty, q]}\\) and \\(\\mathbb{I}_{(- \\infty, q']}\\) such that \\[\n  Y_q &lt; Y_{q'} \\qquad P\\text{-a.s.}\n\\] Observe that \\(Y_{q'} - Y_q\\) is also a version of the conditional expectation of \\(\\mathbb{I}_{(q, q']}\\).\nA countable union of \\(P\\)-negligible events is \\(P\\)-negligible, so, as \\(\\mathbb{Q}^2\\) is countable, we can choose versions \\(\\left( Y_q \\right)_{q \\in\n\\mathbb{Q}}\\) of the conditional expectations of \\(\\mathbb{I}_{(-\n\\infty, q]}\\) such that \\[\nP\\text{-a.s.} \\qquad\\forall q,q' \\in \\mathbb{Q}, \\quad q &lt; q' \\Rightarrow Y_q &lt; Y_{q'},\n\\] Let \\(\\Omega_0\\) be the \\(P\\)-almost sure event on which all \\(Y_q, q \\in \\mathbb{Q}\\) satisfy the good properties.\nFor each \\(x \\in \\mathbb{R}\\), we can define \\(Z_x\\) for each \\(\\omega \\in \\mathbb{R}\\) by \\[ Z_x (\\omega) = \\inf \\left\\{ Y_q (\\omega) : q \\in \\mathbb{Q}, x &lt; q\n\\right\\}\\] On \\(\\Omega_0\\), the function \\(x \\mapsto Z_x (\\omega)\\) is increasing, it has a limit on the left at each point and it is right-continuous. The function \\(x \\mapsto Z_x(\\omega)\\) tends to \\(0\\) when \\(x\\) tends to \\(- \\infty\\), to \\(1\\) when \\(x\\) tends towards \\(+ \\infty\\). In words, on \\(\\Omega_0\\), \\(x\n\\mapsto Z_x (\\omega)\\) is a cumulative distribution function, it defines so (uniquely) a unique probability measure on \\(\\mathbb{R}\\). We will denote it by \\(\\nu (\\omega, .)\\).\nIn addition, for each \\(x\\), \\(Z_x\\) is defined as a countable infimum of \\(\\mathcal{G}\\)-measurable random variables, \\(Z_x\\) is therefore \\(\\mathcal{G}\\)-measurable.\nIt remains to check that for every \\(B \\in \\mathcal{F}\\), \\(\\omega\n\\mapsto \\nu (\\omega, B)\\) for \\(\\omega \\in \\Omega_0\\), \\(0\\) elsewhere, defines a version of the conditional expectation of \\(\\mathbb{I}_B\\) with respect to \\(\\mathcal{G}\\).\nThis property is satisfied for \\(B \\in \\mathcal{C}\\).\nLet us call \\(\\mathcal{D}\\) the set of all the events for which \\(\\omega \\mapsto \\nu (\\omega, B)\\) (on \\(\\Omega_0\\), \\(0\\) elsewhere) defines a version of the conditional expectation of \\(\\mathbb{I}_B\\) with respect to \\(\\mathcal{G}\\). We shall show that \\(\\mathcal{D}\\) is a \\(\\lambda\\)-system, that is\n\n\\(\\mathcal{D}\\) contains \\(\\emptyset\\) and \\(\\mathbb{R} = \\Omega\\).\nIf \\(B, B'\\) belong to \\(\\mathcal{D},\\) and \\(B \\subseteq B'\\) then \\(B' \\setminus B \\in \\mathcal{D} .\\)\nIf \\((B_n)_n\\) is a growing sequence of events from \\(\\mathcal{D},\\) limit \\(B\\) then \\(B \\in \\mathcal{D} .\\)\n\nClause i.) is guaranteed by construction.\nClause ii.) If \\(B' \\subseteq B\\) belong to \\(\\mathcal{D}\\), then by linearity of conditional expectations, if \\(\\mathbb{E}\n\\left[ \\mathbb{I}_{B' \\setminus B} \\mid \\mathcal{G} \\right]\\) is a version of the conditional expectation of \\(\\mathbb{I}_{B' \\setminus\nB}\\) with respect to \\(\\mathcal{G}\\), on an almost-sure event \\(\\Omega_1 \\subseteq\n\\Omega_0\\): \\[\\begin{align*}\n\\mathbb{E} \\left[ \\mathbb{I}_{B' \\setminus B} \\mid \\mathcal{G} \\right]\n  & = \\mathbb{E} \\left[ \\mathbb{I}_{B'} - \\mathbb{I}_B \\mid \\mathcal{G} \\right] \\\\\n  & = \\mathbb{E} \\left[ \\mathbb{I}_{B'} \\mid \\mathcal{G} \\right] - \\mathbb{E} \\left[ \\mathbb{I}_B \\mid \\mathcal{G} \\right] \\\\\n  & = \\nu (\\omega, B') - \\nu (\\omega, B) \\\\\n  & = \\nu (\\omega, B' \\setminus B) \\, .\n\\end{align*}\\] Clause iii.). If \\((B_n)_n\\) is a non-decreasing sequence of events from \\(\\mathcal{D},\\) with \\(B_n \\uparrow B\\), if \\(\\mathbb{E} \\left[ \\mathbb{I}_B \\mid \\mathcal{G} \\right]\\) is a version of the conditional expectation of \\(\\mathbb{I}_B\\) with respect to \\(\\mathcal{G}\\), on an event \\(\\Omega_1 \\subseteq \\Omega_0\\) with probability \\(1\\): \\[\n\\mathbb{E} \\left[ \\mathbb{I}_B \\mid \\mathcal{G} \\right] = \\lim_n\n\\mathbb{E} \\left[ \\mathbb{I}_{B_n} \\mid \\mathcal{G} \\right] =\n\\lim_n \\nu (\\omega, B_n) = \\nu (\\omega, B) \\, .\n\\] So \\(B \\in \\mathcal{D}\\).\nThe Monotone class Theorem (Section 2.6)) tells us that \\(\\mathcal{F \\subseteq \\mathcal{D}}\\).\n\\(\\square\\)\n\n\nWorking harder would allow us to show that the existence of regular conditional probabilities is guaranteed as soon as \\(\\Omega\\) can be endowed with a complete and separable metric space structure and that the \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) is the Borelian \\(\\sigma\\)-algebra induced by this metric.\n\nWe often define a probability distribution starting from a marginal distribution and a kernel.\n\nLet \\((\\Omega, \\mathcal{F})\\) be a measurable space, \\(X\\) a random variable from \\((\\Omega, \\mathcal{F})\\), and \\(N\\) a conditional probability kernel with respect to \\(\\sigma(X)\\). Let \\(P_X\\) be a probability measure on \\((\\Omega \\sigma(X))\\).\nThen there exists a unique probability measure \\(P\\) on \\((\\Omega, \\mathcal{F})\\) such that \\(P_X = P \\circ X^{- 1}\\) and \\(N\\) is a regualr conditional probability kernel with respect to \\(\\sigma(X)\\), we have for every \\(B \\in \\mathcal{F}\\): \\[\nP(B) = \\int_{X(\\Omega)} N(x, B) \\mathrm{d}P_x(x)\n\\]\n\nThe following theorem guarantees the existence of a regular conditional probability in all the scenarios we are interested in.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#sec-ess",
    "href": "06-conditioning.html#sec-ess",
    "title": "9  Conditioning",
    "section": "9.7 Efron-Stein-Steele inequalities",
    "text": "9.7 Efron-Stein-Steele inequalities\nIn this section, \\(X_1, \\ldots, X_n\\) denote independent random variables on some probability space with values in \\(\\mathcal{X}_1, \\ldots, \\mathcal{X}_n\\), and \\(f\\) denote a measurable function from \\(\\mathcal{X}_1 \\times  \\ldots \\times \\mathcal{X}_n\\) to \\(\\mathbb{R}\\). Let \\(Z=f(X_1, \\ldots, X_n)\\). The random variable \\(Z\\) is a general function of independent random variables. We assume \\(Z\\) is integrable.\nIf we had \\(Z = \\sum_{i=1}^n X_i\\), we could write \\[\n\\operatorname{var}(Z)\n= \\sum_{i=1}^n \\operatorname{var}(X_i)\n= \\sum_{i=1}^n \\mathbb{E}\\Big[\\operatorname{var}( Z \\mid X_1, \\ldots, X_{i-1}, X_{i+1}, \\ldots X_n)\\Big]\n\\] even though the last expression looks pedantic. The aim of this section is to show that even if \\(f\\) is not as simple as the sum of its arguments, the last expression can still serve as an upper bound on the variance.\nIt is a natural idea to bound the variance of such a general function by expressing \\(Z-\\mathbb{E} Z\\) as a sum of differences and to use the orthogonality of these differences.\nMore precisely, if we denote by \\(\\mathbb{E}_i\\) the conditional expectation operator, conditioned on \\(\\left(X_{1},\\ldots,X_{i}\\right)\\), and use the convention \\(\\mathbb{E}_0=\\mathbb{E}\\), then we may define \\[\n\\Delta_{i}=\\mathbb{E}_i Z  -\\mathbb{E}_{i-1} Z\n\\] for every \\(i=1,\\ldots,n\\).\n\nExercise 9.8 Check that \\(\\mathbb{E} \\Delta_i=0\\) and that for \\(j&gt;i\\), \\(\\mathbb{E}_i \\Delta_j=0\\) a.s.\n\nStarting from the decomposition \\[\nZ-\\mathbb{E} Z  =\\sum_{i=1}^{n}\\Delta_{i}\n\\] one has \\[\n\\operatorname{var}\\left(  Z\\right)  =\\mathbb{E}\\left[  \\left(  \\sum_{i=1}\n^{n}\\Delta_{i}\\right)  ^{2}\\right]  =\\sum_{i=1}^{n}\\mathbb{E}\\left[\n\\Delta_{i}^{2}\\right]  +2\\sum_{j&gt;i}\\mathbb{E}\\left[  \\Delta_{i}\\Delta\n_{j}\\right]  \\text{.}\n\\] Now if \\(j&gt;i\\), \\(\\mathbb{E}_i \\Delta_{j}  =0\\) implies that \\[\n\\mathbb{E}_i\\left[  \\Delta_{j}\\Delta_{i}\\right]  =\\Delta_{i}\n\\mathbb{E}_{i} \\Delta_{j}  =0~,\n\\] and, a fortiori, \\(\\mathbb{E}\\left[  \\Delta_{j}\\Delta_{i}\\right]  =0\\). Thus, we obtain the following analog of the additivity formula of the variance: \\[\n\\operatorname{var}\\left(  Z\\right)  =\\mathbb{E}\\left[  \\left(  \\sum_{i=1}\n^{n}\\Delta_{i}\\right)  ^{2}\\right]  =\\sum_{i=1}^{n}\\mathbb{E}\\left[\n\\Delta_{i}^{2}\\right]~.\n\\] Observe that up to now, we have not made any use of the fact that \\(Z\\) is a function of independent variables \\(X_{1},\\ldots,X_{n}\\).\nIndependence may be used as in the following argument: for any integrable function \\(Z= f\\left(  X_{1},\\ldots,X_{n}\\right)\\) one may write, by the Tonelli-Fubini theorem, \\[\n\\mathbb{E}_i Z  =\\int\n_{\\mathcal{X}^{n-i}}f\\left(  X_{1},\\ldots,X_{i},x_{i+1},\\ldots,x_{n}\\right)\nd\\mu_{i+1}\\left(  x_{i+1}\\right)  \\ldots d\\mu_{n}\\left(  x_{n}\\right)  \\text{,}\n\\] where, for every \\(j= 1,\\ldots,n\\), \\(\\mu_{j}\\) denotes the probability distribution of \\(X_{j}\\). Also, if we denote by \\(\\mathbb{E}^{(i)}\\) the conditional expectation operator conditioned on \\(X^{(i)}=(X_{1},\\ldots,X_{i-1},X_{i+1},\\ldots,X_{n})\\), we have \\[\n\\mathbb{E}^{(i)}Z %%%%%\\left[  Z\\right]\n=\\int_{\\mathcal{X}}\nf\\left(  X_{1},\\ldots,X_{i-1},x_{i},X_{i+1},\\ldots,X_{n}\\right)  d\\mu_{i}\\left(\nx_{i}\\right)~.\n\\] Then, again by the Tonelli-Fubini theorem, \\[\\begin{equation}\n\\mathbb{E}_i\\left[  \\mathbb{E}^{\\left(  i\\right)  } Z \\right]\n=\\mathbb{E}_{i-1} Z  \\text{.}\n  (\\#eq:efundind)\n\\end{equation}\\] This observation is the key in the proof of the main result of this section which we state next:\n\nTheorem 9.7 (Efron-Stein-Steele’s inequalities) Let \\(X_1,\\ldots,X_n\\) be independent random variables and let \\(Z=f(X)\\) be a square-integrable function of \\(X=\\left(  X_{1},\\ldots,X_{n}\\right)\\). Then \\[\n\\operatorname{var}\\left(  Z\\right)  \\leq\\sum_{i=1}^n\\mathbb{E}\\left[\n\\left(  Z-\\mathbb{E}^{(i)} Z  \\right)^2\\right]  = v~.\n\\] Moreover if \\(X_1',\\ldots,X_n'\\) are independent copies of \\(X_1,\\ldots,X_n\\) and if we define, for every \\(i=1,\\ldots,n\\), \\[\nZ_i'=  f\\left(X_1,\\ldots,X_{i-1},X_i',X_{i+1},\\ldots,X_n\\right)~,\n\\] then \\[\nv=\\frac{1}{2}\\sum_{i=1}^n\\mathbb{E}\\left[  \\left(  Z-Z_i'\\right)^2\\right]\n=\\sum_{i=1}^n\\mathbb{E}\\left[  \\left(  Z-Z_i'\\right)_+^2\\right]\n=\\sum_{i=1}^n\\mathbb{E}\\left[  \\left(  Z-Z_i'\\right)_-^2\\right]\n\\] where \\(x_+=\\max(x,0)\\) and \\(x_-=\\max(-x,0)\\) denote the positive and negative parts of a real number \\(x\\). Also, \\[\nv=\\inf_{Z_{i}}\\sum_{i=1}^{n}\\mathbb{E}\\left[  \\left(  Z-Z_{i}\\right)^2\\right]~,\n\\] where the infimum is taken over the class of all \\(X^{(i)}\\)-measurable and square-integrable variables \\(Z_{i}\\), \\(i=1,\\ldots,n\\).\n\n\nProof. We begin with the proof of the first statement. Note that, using @ref(eq:efundind), we may write \\[\n\\Delta_{i}=\\mathbb{E}_i\\left[  Z-\\mathbb{E}^{\\left(  i\\right)  } Z  \\right]  \\text{.}\n\\] By the conditional Jensen inequality, \\[\n\\Delta_{i}^{2}\\leq\\mathbb{E}_i\\left[  \\left(  Z-\\mathbb{E}^{\\left(\ni\\right)  }Z  \\right)  ^{2}\\right]~.\n\\] Using \\(\\operatorname{var}(Z) = \\sum_{i=1}^n \\mathbb{E}\\left[ \\Delta_i^2\\right]\\), we obtain the desired inequality. To prove the identities for \\(v\\), denote by \\(\\operatorname{var}^{\\left(i\\right) }\\) the conditional variance operator conditioned on \\(X^{\\left(  i\\right)  }\\). Then we may write \\(v\\) as \\[\nv=\\sum_{i=1}^{n}\\mathbb{E}\\left[  \\operatorname{var}^{\\left(  i\\right)  }\\left(\nZ\\right)  \\right]  \\text{.}\n\\] Now note that one may simply use (conditionally) the elementary fact that if \\(X\\) and \\(Y\\) are independent and identically distributed real-valued random variables, then \\(\\operatorname{var}(X)=(1/2) \\mathbb{E}[(X-Y)^2]\\). Since conditionally on \\(X^{\\left(  i\\right)  }\\), \\(Z_i'\\) is an independent copy of \\(Z\\), we may write \\[\n\\operatorname{var}^{\\left(  i\\right)  }\\left(  Z\\right)  =\\frac{1}{2}\\mathbb{E}\n^{\\left(  i\\right)  }\\left[  \\left(  Z-Z_i'\\right)^2\\right]\n=\\mathbb{E}^{\\left(  i\\right)  }\\left[  \\left(  Z-Z_i'\\right)_+^2\\right]\n=\\mathbb{E}^{\\left(  i\\right)  }\\left[  \\left(  Z-Z_i'\\right)_-^2\\right]\n\\] where we used the fact that the conditional distributions of \\(Z\\) and \\(Z_i'\\) are identical. The last identity is obtained by recalling that, for any real-valued random variable \\(X\\), \\(\\operatorname{var}(X) = \\inf_{a\\in \\mathbb{R}} \\mathbb{E}[(X-a)^2]\\). Using this fact conditionally, we have, for every \\(i=1,\\ldots,n\\), \\[\n\\operatorname{var}^{\\left(  i\\right)  }\\left(  Z\\right)  =\\inf_{Z_{i}}\n\\mathbb{E}^{\\left(  i\\right)  }\\left[  \\left(  Z-Z_{i}\\right)^2\\right]~.\n\\] Note that this infimum is achieved whenever \\(Z_{i}=\\mathbb{E}^{\\left(  i\\right)  } Z\\).\n\\(\\square\\)\n\nObserve that in the case when \\(Z=\\sum_{i=1}^n X_i\\) is a sum of independent random variables (with finite variance) then the Efron-Stein-Steele inequality becomes an equality. Thus, the bound in the Efron-Stein-Steele inequality is, in a sense, not improvable.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#sec-mcdiarmid",
    "href": "06-conditioning.html#sec-mcdiarmid",
    "title": "9  Conditioning",
    "section": "9.8 Bounded differences inequalities",
    "text": "9.8 Bounded differences inequalities\nIn this section we combine Hoeffding’s inequality and conditioning to establish the so-called Bounded differences inequality (also known as McDiarmid’s inequality). This inequality is a first example of the concentration of measure phenomenon. This phenomenon is best portrayed by the following say:\n\nA function of many independent random variables that does not depend too much on any of them is concentrated around its mean or median value.\n\n\n\n9.8.1 Bounded differences inequalities\nLet \\(X_1, \\ldots, X_n\\) be independent random variables on some probability space with values in \\(\\mathcal{X}_1, \\mathcal{X}_2, \\ldots, \\mathcal{X}_n\\). Let \\(f : \\mathcal{X}_1 \\times \\mathcal{X}_2 \\times  \\ldots \\times  \\mathcal{X}_n \\to \\mathbb{R}\\) be a measurable function such that there exists non-negative constants \\(c_1, \\ldots, c_n\\) satisfying \\(\\forall x_1, \\ldots, x_n \\in \\prod_{i=1}^n \\mathcal{X}_i\\), \\(\\forall y_1, \\ldots, y_n \\in \\prod_{i=1}^n \\mathcal{X}_i\\), \\[\n\\Big| f(x_1, \\ldots, x_n) - f(y_1, \\ldots, y_n)\\Big|  \\leq \\sum_{i=1}^n c_i \\mathbb{I}_{x_i\\neq y_i} \\,.\n\\] Let \\(Z =  f(X_1, \\ldots, X_n)\\) and \\(v = \\sum_{i=1}^n \\frac{c_i^2}{4}\\). Then \\[\n\\operatorname{var}(Z)  \\leq v \\, ,\n\\] \\[\n\\log \\mathbb{E} \\mathrm{e}^{\\lambda (Z -\\mathbb{E}Z)} \\leq \\frac{\\lambda^2 v}{2}\n\\] and \\[\nP \\Big\\{ Z \\geq \\mathbb{E}Z + t \\Big\\} \\leq \\mathrm{e}^{-\\frac{t^2}{2v}} \\,.\n\\]\n\n\n\nProof. The variance bound is an immediate consequence of the Efron-Stein-Steele inequalities.\nThe tail bound follows from the upper bound on the logarithmic moment generating function by Cramer-Chernoff bounding.\nLet us now check the upper-bound on the logarithmic moment generating function.\nWe proceed by inudction on the number of arguments \\(n\\).\nIf \\(n=1\\), the upper-bound on the logarithmic moment generating function is just Hoeffing’s Lemma (see Section 10.7)).\nAssume the upper-bound is valid up to \\(n-1\\).\nWe adopt the same notation as in Section 9.7). \\[\\begin{align*}\n\\mathbb{E} \\mathrm{e}^{\\lambda (Z - \\mathbb{E}Z)}\n  & = \\mathbb{E}\\Big[ \\mathbb{E}_{n-1}\\mathrm{e}^{\\lambda (Z - \\mathbb{E}Z)} \\Big] \\\\\n  & = \\mathbb{E}\\Big[ \\mathbb{E}_{n-1}\\big[\\mathrm{e}^{\\lambda (Z - \\mathbb{E}_{n-1}Z)}\\big] \\times \\mathrm{e}^{\\lambda (\\mathbb{E}_{n-1}Z - \\mathbb{E}Z)}  \\Big] \\,.\n\\end{align*}\\] Now, \\[\n\\mathbb{E}_{n-1}Z = \\int_{\\mathcal{X}_n} f(x_1,\\ldots,x_{n-1}, u) \\mathrm{d}P_{X_n}(u) \\qquad\\text{a.s.}\n\\] and \\[\\begin{align*}\n& \\mathbb{E}_{n-1}\\big[\\mathrm{e}^{\\lambda (Z - \\mathbb{E}_{n-1}Z)}\\big] \\\\\n  & = \\int_{\\mathcal{X}_n}  \\exp\\Big(\\lambda  \\int_{\\mathcal{X}_n} f(x_1,\\ldots,x_{n-1}, v) -f(x_1,\\ldots,x_{n-1}, u) \\mathrm{d}P_{X_n}(u)  \\Big) \\mathrm{d}P_{X_n}(v) \\, .\n\\end{align*}\\] For every \\(x_1, \\ldots, x_{n-1} \\in \\mathcal{X_1} \\times \\ldots \\times \\mathcal{X}_{n-1}\\), for every \\(v, v' \\in \\mathcal{X}_n\\), \\[\\begin{align*}\n& \\Big| \\int_{\\mathcal{X}_n} f(x_1,\\ldots,x_{n-1}, v) -f(x_1,\\ldots,x_{n-1}, u) \\mathrm{d}P_{X_n}(u) \\\\\n& - \\int_{\\mathcal{X}_n} f(x_1,\\ldots,x_{n-1}, v') -f(x_1,\\ldots,x_{n-1}, u) \\mathrm{d}P_{X_n}(u)\\Big| \\leq c_n\n\\end{align*}\\] hence, by Hoeffding’s Lemma \\[\n\\mathbb{E}_{n-1}\\big[\\mathrm{e}^{\\lambda (Z - \\mathbb{E}_{n-1}Z)}\\big] \\leq \\mathrm{e}^{\\frac{\\lambda^2 c_n^2}{8}} \\, .\n\\] \\[\\begin{align*}\n  \\mathbb{E} \\mathrm{e}^{\\lambda (Z - \\mathbb{E}Z)}\n  & \\leq \\mathbb{E}\\Big[  \\mathrm{e}^{\\lambda (\\mathbb{E}_{n-1}Z - \\mathbb{E}Z)}  \\Big] \\times \\mathrm{e}^{\\frac{\\lambda^2 c_n^2}{8}} \\, .\n\\end{align*}\\] But, if \\(X_1=x_1, \\ldots X_{n-1}=x_{n-1}\\), \\[\n\\mathrm{e}^{\\lambda (\\mathbb{E}_{n-1}Z - \\mathbb{E}Z)}\n= \\int_{\\mathcal{X}_n} f(x_1,\\ldots,x_{n-1}, v) \\mathrm{d}P_{X_n}(v) - \\mathbb{E}Z \\,,\n\\] it is a function of \\(n-1\\) independent random variables that satisfies the bounded differences conditions with constants \\(c_1, \\ldots, c_{n-1}\\). By the induction hypothesis: \\[\n\\mathbb{E}\\Big[  \\mathrm{e}^{\\lambda (\\mathbb{E}_{n-1}Z - \\mathbb{E}Z)}  \\Big]\n\\leq \\mathrm{e}^{\\frac{\\lambda^2}{2} \\sum_{i=1}^{n-1} \\frac{c_i^2}{4}} \\, .\n\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "06-conditioning.html#sec-bibconditionning",
    "href": "06-conditioning.html#sec-bibconditionning",
    "title": "9  Conditioning",
    "section": "9.9 Bibliographic remarks",
    "text": "9.9 Bibliographic remarks\nConditional expectations can be constructed from the Radon-Nikodym Theorem, see (Dudley, 2002).\nIt is also possible to prove the Radon-Nikodym Theorem starting from the construction of conditional expectation in \\(\\mathcal{L}_2\\), see (Williams, 1991).\nThe Section on Efron-Stein-Steele’s inequalities is from (Boucheron, Lugosi, & Massart, 2013)\nBounded difference inequality is due to C. McDiarmid. It became popular in (Theoretical) computer science during the 1990’s. See (McDiarmid, 1998)\n\n\n\n\nBoucheron, S., Lugosi, G., & Massart, P. (2013). Concentration inequalities. Oxford University Press.\n\n\nDudley, R. M. (2002). Real analysis and probability (Vol. 74, p. x+555). Cambridge: Cambridge University Press.\n\n\nMcDiarmid, C. (1998). Concentration. In M. Habib, C. McDiarmid, J. Ramirez-Alfonsin, & B. Reed (Eds.), Probabilistic methods for algorithmic discrete mathematics (pp. 195–248). Springer, New York.\n\n\nWilliams, D. (1991). Probability with martingales (p. xvi+251). Cambridge University Press, Cambridge. https://doi.org/10.1017/CBO9780511813658",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "05-convergences-1.html",
    "href": "05-convergences-1.html",
    "title": "10  Convergences I : almost sure, \\(L_2\\), \\(L_1\\), in Probability",
    "section": "",
    "text": "10.1 Motivations\nWe need to put topological structures in the world of random variables living on some probability space. As random variables are (measurable) functions, we shall borrow and adapt the notions used in Analysis: pointwise convergence (Section 10.2)), convergence in \\(L_p, 1 \\leq p &lt;\\infty\\) (Section 10.3)).\nFinally, we define and investigate convergence in probability. This notion weakens both \\(L_p\\) and almost sure (pointwise) convergence. Just as \\(L_p\\) convergences, it can be metrized.\nConvergence in probability and almost sure convergence are illustrated by weak and strong law of large numbers (Sections Section 10.5 and Section 10.6). Laws of large numbers assert that empirical means converge towards expectations (under mild conditions), they are the workhorses of statistical learning theory.\nIn Section 10.7), we look at non-asymptotic counterparts of the weak law of large numbers. We establish exponential tail bounds for sums of independent random variables (under stringent integrability assumptions).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convergences I : almost sure, $L_2$, $L_1$, in Probability</span>"
    ]
  },
  {
    "objectID": "05-convergences-1.html#sec-asconvergence",
    "href": "05-convergences-1.html#sec-asconvergence",
    "title": "10  Convergences I : almost sure, \\(L_2\\), \\(L_1\\), in Probability",
    "section": "10.2 Almost sure convergence",
    "text": "10.2 Almost sure convergence\nThe notion of almost sure convergence mirrors the notion of pointwise convergence in probabilistic settings.\nRecall that a sequence of real-valued functions \\((f_n)_n\\) mapping some space \\(\\Omega\\) to \\(\\mathbb{R}\\) converges pointwise to \\(f: \\Omega \\to \\mathbb{R}\\), if for each \\(\\omega \\in \\Omega\\), \\(f_n(\\omega) \\to f(\\omega)\\). There is no uniformity condition.\nIn the next definition, we assume that random variables are real-valued. The definition is easily extended to multivariate settings.\n\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space, a sequence \\((X_n)_n\\) of random variables converges almost surely (a.s.) towards a random variable \\(X\\) if the event \\[E = \\left\\{ \\omega : \\lim_n X_n(\\omega) = X(\\omega)\\right\\}\\] has \\(P\\)-probability \\(1\\).\n\nAlmost sure convergence, is (just) pointwise convergence with probability \\(1\\). Almost sure convergence is not tied to integrability. Note that all random variables involved in the above statements live on the same probability space. We may wonder whether we can design a metric for almost-sure convergence? The answer is no, as for pointwise convergence, in general.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convergences I : almost sure, $L_2$, $L_1$, in Probability</span>"
    ]
  },
  {
    "objectID": "05-convergences-1.html#sec-Lpconvergence",
    "href": "05-convergences-1.html#sec-Lpconvergence",
    "title": "10  Convergences I : almost sure, \\(L_2\\), \\(L_1\\), in Probability",
    "section": "10.3 Convergence in \\(L_p\\)",
    "text": "10.3 Convergence in \\(L_p\\)\nIn this section, we consider random variables that satisfy integrability assumptions. The scope of \\(L_p\\) convergences is narrower than the scope of \\(L_p\\) convergences.\nWe already introduced \\(L_p\\) convergences in Lesson Chapter 3. We recall it for the sake of readibility.\n\nFor \\(p \\in [1, \\infty)\\), \\(L_p\\) is the set of random variables over \\((\\Omega, \\mathcal{F}, P)\\) that satisfy \\(\\mathbb{E} |X|^p &lt;\\infty\\). The \\(p\\)-pseudo-norm is defined by \\(\\|X\\|_p = \\big(\\mathbb{E} |X|^p \\big)^{1/p}\\). Convergence in \\(L_p\\) means convergence for this pseudo-norm.\n\nRecall that \\(L_p\\) spaces are nested (by Holder’s inequality) and complete.\n\nConvergence in \\(L_q, q\\geq 1\\) implies convergence in \\(L_p, 1\\leq p \\leq q\\).\n\nAlmost sure convergence is not tied to integrability. We cannot ask whether almost sure convergence implies \\(L_p\\) convergence. But we can ask whether \\(L_p\\) convergence implies almost sure convergence. The next statement is a by-product of the proof of the completeness of \\(L_p\\) spaces, see Section 3.11).\n\nConvergence in \\(L_p\\) implies almost sure convergence along a subsequence.\n\nA counter-example given in Section 3.11) shows that convergence in \\(L_p\\) does not imply almost-sure convergence.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convergences I : almost sure, $L_2$, $L_1$, in Probability</span>"
    ]
  },
  {
    "objectID": "05-convergences-1.html#sec-probaconvergence",
    "href": "05-convergences-1.html#sec-probaconvergence",
    "title": "10  Convergences I : almost sure, \\(L_2\\), \\(L_1\\), in Probability",
    "section": "10.4 Convergence in probability",
    "text": "10.4 Convergence in probability\nIf we denote by \\(L_0=L_0(\\Omega, \\mathcal{F}, P)\\) the set of real-valued random variables, the notion of convergence in probability is relevant to all sequences in \\(L_0\\) like almost sure convergence. And like convergence in \\(L_p, p\\geq 1\\), convergence in probability can be metrized.\n\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space.\nA sequence \\((X_n)_n\\) of random variables converges in probability towards a random variable \\(X\\) if for any \\(\\epsilon&gt;0\\) \\[\\lim_n P \\{ |X_n -X| \\geq \\epsilon \\}  = 0\\, .\\]\n\n\nConvergence in \\(L_p, p\\geq 1\\) implies convergence in probability.\n\nThis is an immediate consequence of Markov’s inequality.\n\nThe sequence \\((X_n)_n\\) converges in probability towards \\(X\\) iff \\[\\lim_n \\mathbb{E} \\Big[ 1 \\wedge |X_n -X|\\Big] = 0\\]\n\n\nProof. Assuming convergence in probability, \\[\\begin{array}{rl}\n\\mathbb{E} \\Big[ 1 \\wedge |X_n -X|\\Big]  & \\leq \\mathbb{E} \\Big[ (1 \\wedge |X_n -X|)\\mathbb{I}_{|X-X_n| \\geq \\epsilon}\\Big]  + \\mathbb{E} \\Big[ (1 \\wedge |X_n -X|)\\mathbb{I}_{|X-X_n| &lt; \\epsilon}\\Big] \\\\  & \\leq P \\Big\\{|X-X_n| \\geq \\epsilon \\Big\\} + \\epsilon\\end{array}\\] the limit of the right-hand side is not larger than \\(\\epsilon\\). As we can take \\(\\epsilon\\) arbitrarily small, this entails that the limit of the left-hand side is zero.\nConversely, for all \\(0&lt; \\epsilon&lt; 1\\) \\[\\begin{array}{rl}\nP \\Big\\{|X-X_n| \\geq \\epsilon \\Big\\}\n  & \\leq \\frac{1}{\\epsilon} \\mathbb{E}\\Big[ 1 \\wedge |X-X_n|\\Big] \\,.\n\\end{array}\\] Hence \\(\\lim_n \\mathbb{E} \\Big[ 1 \\wedge |X_n -X|\\Big] = 0\\) entails \\(\\lim_n P \\big\\{|X-X_n| \\geq \\epsilon \\big\\} =0\\). As this holds for all \\(\\epsilon&gt;0\\), \\(\\lim_n \\mathbb{E} \\Big[ 1 \\wedge |X_n -X|\\Big] = 0\\) entails convergence in Probability.\n\n\n\n\n\nAlmost sure convergence implies convergence in probability.\n\n\nProof. Assume \\(X_n \\to X\\) a.s., that is \\(|X_n -X| \\to 0\\). Then by dominated convergence, \\[\\lim_n \\mathbb{E}\\Big[ |X_n -X| \\wedge 1\\Big] = 0\\] which entails convergence in probability of \\((X_n)_n\\) towards \\(X\\).\n\n\n\n\nNow, we come to a metric which fits perfectly with convergence in probability.\n\nDefinition 10.1 (Ky-Fan distance) The Ky-Fan distance is defined as \\[\\mathrm{d}_{\\mathrm{KF}}(X, Y) = \\inf_{\\epsilon\\geq 0} P\\Big\\{ |X-Y| &gt;\\epsilon\\Big\\}  \\leq \\epsilon \\,.\\]\n\nNote that we have to check that \\(\\mathrm{d}_{\\mathrm{KF}}\\) is indeed a distance. This is the content of Proposition @ref(prp:kyfanprop) below.\n\nIn the definition of the Ky-Fan distance, the infimum is attained.\n\n\nProof. Let \\(a &gt; \\mathrm{d}_{\\mathrm{KF}}(X, Y)\\) the event \\(A_a = \\Big\\{ |X-Y| &gt; a \\Big\\}\\) has probability smaller than \\(\\epsilon\\). And if \\(\\epsilon &lt; a &lt; b\\), \\(A_b \\subseteq  A_a\\). By monotone converence, \\(P\\Big(\\cap_n A_{\\epsilon + 1/n}\\Big)=  \\lim_{n} \\uparrow P\\Big(A_{\\epsilon + 1/n}\\Big) = \\epsilon\\).\n\n\n\n\n\nProposition 10.1 Ky-Fan distance satisfies:\n\n\\(\\mathrm{d}_{\\mathrm{KF}}(X, Y)=0 \\Rightarrow X=Y \\qquad \\text{a.s.}\\)\n\\(\\mathrm{d}_{\\mathrm{KF}}(X, Y) = \\mathrm{d}_{\\mathrm{KF}}(Y, X)\\)\n\\(\\mathrm{d}_{\\mathrm{KF}}(X, Z) \\leq  \\mathrm{d}_{\\mathrm{KF}}(X, Y) + \\mathrm{d}_{\\mathrm{KF}}(Y, Z)\\)\n\n\n\nProof. We check that \\(\\mathrm{d}_{\\mathrm{KF}}\\) satisfies the triangle inequality. There exists two events \\(B\\) and \\(C\\) with respective probabilities \\(\\mathrm{d}_{\\mathrm{KF}}(X, Y)\\) and \\(\\mathrm{d}_{\\mathrm{KF}}(Y, Z)\\) such that \\[|X(\\omega) -Y(\\omega)| \\leq \\mathrm{d}_{\\mathrm{KF}}(X, Y) \\qquad \\text{on } B^c\\] and \\[|Z(\\omega) -Y(\\omega)| \\leq \\mathrm{d}_{\\mathrm{KF}}(Z, Y) \\qquad \\text{on } C^c\\,.\\]\nOn \\(B^c \\cap C^c\\), by the triangle inequality on \\(\\mathbb{R}\\): \\[|X(\\omega) - Z(\\omega)|  \\leq \\mathrm{d}_{\\mathrm{KF}}(X, Y) + \\mathrm{d}_{\\mathrm{KF}}(Y, Z)\\, .\\]\nWe conclude by observing \\[\\begin{array}{rl}\nP \\Big( |X(\\omega) - Z(\\omega)| &gt; \\mathrm{d}_{\\mathrm{KF}}(X, Y) + \\mathrm{d}_{\\mathrm{KF}}(Y, Z)\n\\Big)\n& \\leq P\\Big( (B^c \\cap C^c)^c\\Big)\\\\\n& =  P(B \\cup C) \\\\\n& \\leq P(B) + P(C) \\\\\n& = \\mathrm{d}_{\\mathrm{KF}}(X, Y) + \\mathrm{d}_{\\mathrm{KF}}(Y, Z) \\, .\n\\end{array}\\]\n\n\n\n\n\nThe two statements are equivalent:\n\n\\((X_n)_n\\) converges in probability towards \\(X\\)\n\\(\\mathrm{d}_{\\mathrm{KF}}(X_n, X)\\) tends to \\(0\\) as \\(n\\) tends to infinity.\n\n\n\nExercise 10.1 Check the proposition.\n\n\n\n\nWe leave the following questions as exercises:\n\nIs \\(\\mathcal{L}_0(\\Omega, \\mathcal{F}, P)\\) complete under the Ky-Fan metric?\nDoes convergence in probability imply almost sure convergence?\nDoes convergence in probability imply convergence in \\(L_p, p\\geq 1\\)?\n\nFinally, we state a more gemeral definition of convergence in probability. The notion can be tailored to random variables that map some universe to some metric space. The connections with almost-sure convergence and \\(L_p\\) convergences remain unchanged.\n\nDefinition 10.2 (Convergence in probability, multivariate setting) A sequence \\((X_n)_{n \\in \\mathbb{N}}\\) of \\(\\mathbb{R}^k\\)-valued random variables living on the same probability space \\((\\Omega, \\mathcal{F}, P)\\) converges in probability (in \\(\\mathbb{P}\\)-probability) towards a \\(\\mathbb{R}^k\\)-valued random variable \\(X\\) iff for every \\(\\epsilon &gt;0\\)\n\\[\\lim_{n \\to \\infty} \\mathbb{P} \\{ \\Vert X_n -X\\Vert &gt; \\epsilon \\} = 0 \\,.\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convergences I : almost sure, $L_2$, $L_1$, in Probability</span>"
    ]
  },
  {
    "objectID": "05-convergences-1.html#sec-wlln",
    "href": "05-convergences-1.html#sec-wlln",
    "title": "10  Convergences I : almost sure, \\(L_2\\), \\(L_1\\), in Probability",
    "section": "10.5 Weak law of large numbers",
    "text": "10.5 Weak law of large numbers\nThe weak and the strong law of large numbers are concerned with the convergence of empirical means of independent, identically distributed, integrable random variables towards their common expectation.\n\nTheorem 10.1 (Weak law of large numbers) If \\(X_1, \\ldots, X_n, \\ldots\\) are independently, identically distributed, integrable \\(\\mathbb{R}^k\\)-valued random variables over \\((\\Omega, \\mathcal{F}, P)\\) with expectation \\(\\mu\\) then the sequence \\((\\overline{X}_n)\\) defined by \\(\\overline{X}_n := \\frac{1}{n} \\sum_{i=1}^n X_i\\) converges in \\(P\\)-probability towards \\(\\mu\\).\n\n\nProof. Assume first that \\(\\mathbb{E}\\Big[\\Big(X_i-\\mu\\Big)^2\\Big] = \\sigma^2 &lt; \\infty.\\) Then, for all \\(\\epsilon&gt;0\\), by the Markov-Chebychev inequality: \\[\\begin{array}{rl}\nP\\Big\\{ \\Big|\\frac{1}{n}\\sum_{i=1}^n X_i - \\mu\\Big| &gt; \\epsilon\\Big\\}\n& \\leq \\frac{\\mathbb{E} \\Big|\\frac{1}{n}\\sum_{i=1}^n X_i - \\mu\\Big|^2 }{\\epsilon^2} \\\\\n& =  \\frac{\\mathbb{E}\\Big[\\Big(X_i-\\mu\\Big)^2\\Big] }{n \\epsilon^2} \\\\\n& = \\frac{\\sigma^2}{n \\epsilon^2}\n\\end{array}\\] because the variance of a sum of independent random variables equals the sum of the variances of the summands.\nThe right-hand side converges to \\(0\\) for all \\(\\epsilon &gt;0\\). The WLLN holds for square-integrable random variables.\nLet us turn to the general case. Without loss of generality, assume all \\(X_n\\) are centered. Let \\(\\tau &gt;0\\) be a truncation threshold (which value will be tuned later). For each \\(i \\in \\mathbb{N}\\), \\(X_i\\) is decomposed into a sum: \\[X_i = X^\\tau_i + Y^\\tau_i\\]\nwith \\(X^\\tau_i =  \\mathbb{I}_{|X_i|\\leq \\tau} X_i\\) and \\(Y^\\tau_i =  \\mathbb{I}_{|X_i|&gt;\\tau} X_i\\). For every \\(\\epsilon &gt;0\\),\n\\[\\Big\\{ \\Big|\\frac{1}{n}\\sum_{i=1}^n X_i \\Big| &gt;\\epsilon\\Big\\}\n\\subseteq \\Big\\{ \\Big|\\frac{1}{n}\\sum_{i=1}^n X^\\tau_i \\Big| &gt; \\frac{\\epsilon}{2}\\Big\\} \\cup\n\\Big\\{ \\Big|\\frac{1}{n}\\sum_{i=1}^n Y^\\tau_i \\Big| &gt;\\frac{\\epsilon}{2}\\Big\\} \\, .\\]\nInvoking the union bound, Markov’s inequality (twice), the boundedness of the variances of the \\(X^\\tau_i\\)’s leads to:\n\\[\\begin{array}{rl}   P\\Big\\{ \\Big|\\frac{1}{n}\\sum_{i=1}^n X_i - \\mu\\Big| &gt; \\epsilon\\Big\\} & \\leq P \\Big\\{ \\Big|\\frac{1}{n}\\sum_{i=1}^n X^\\tau_i \\Big| &gt; \\frac{\\epsilon}{2}\\Big\\} + P \\Big\\{ \\Big|\\frac{1}{n}\\sum_{i=1}^n Y^\\tau_i \\Big| &gt;\\frac{\\epsilon}{2}\\Big\\} \\\\ & \\leq 4 \\frac{\\mathbb{E}\\Big|\\frac{1}{n}\\sum_{i=1}^n X^\\tau_i \\Big|^2}{\\epsilon^2}  + 2 \\frac{\\mathbb{E}\\Big|\\frac{1}{n}\\sum_{i=1}^n Y^\\tau_i \\Big|}{\\epsilon} \\\\ & \\leq  \\frac{4 \\tau^2}{n\\epsilon^2} + 2 \\frac{\\mathbb{E}\\Big|\\frac{1}{n}\\sum_{i=1}^n Y^\\tau_i \\Big|}{\\epsilon} \\\\\n& \\leq  \\frac{4 \\tau^2}{n\\epsilon^2} + 2 \\frac{1}{n}\\sum_{i=1}^n  \\frac{\\mathbb{E}\\Big|Y^\\tau_i \\Big|}{\\epsilon} \\\\\n& \\leq  \\frac{4 \\tau^2}{n\\epsilon^2} + 2 \\frac{\\mathbb{E} \\Big|Y^\\tau_1 \\Big|}{\\epsilon}\n\\, .\n\\end{array}\\]\nTaking \\(n\\) to infinity leads to \\[\\limsup_n P\\Big\\{ \\Big|\\frac{1}{n}\\sum_{i=1}^n X_i - \\mu\\Big| &gt; \\epsilon\\Big\\} \\leq\n2  \\frac{\\mathbb{E}\\Big|Y^\\tau_1 \\Big|}{\\epsilon} \\, .\\]\nNow as \\({\\tau \\uparrow \\infty}\\) \\(|Y^\\tau_1|  \\downarrow 0\\) while \\(|Y^\\tau_1| \\leq |X_1|\\), dominated convergence (here a special case of monotone convergence) warrants that \\(\\lim_{\\tau \\uparrow \\infty}  \\frac{\\mathbb{E}\\Big|Y^\\tau_1 \\Big|}{\\epsilon}=0\\).\nThis completes the proof of the WLLN.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convergences I : almost sure, $L_2$, $L_1$, in Probability</span>"
    ]
  },
  {
    "objectID": "05-convergences-1.html#sec-slln",
    "href": "05-convergences-1.html#sec-slln",
    "title": "10  Convergences I : almost sure, \\(L_2\\), \\(L_1\\), in Probability",
    "section": "10.6 Strong law of large numbers",
    "text": "10.6 Strong law of large numbers\nInfinite product space endowed with cylinders \\(\\sigma\\)-algebra, and infinite product distribution.\n\nTheorem 10.2 (Strong law of large numbers, direct part) If \\(X_1, \\ldots, X_n, \\ldots\\) are independently, identically distributed, integrable \\(\\mathbb{R}\\)-valued random variables over \\((\\Omega, \\mathcal{F}, P)\\) with expectation \\(\\mu\\) then \\(P\\)-a.s. \\[\\lim_{n \\to \\infty}    \\overline{X}_n =   \\mu \\qquad\\text{with} \\quad \\overline{X}_n := \\frac{1}{n} \\sum_{i=1}^n X_i  .\\]\n\nRecall\n\nLemma 10.1 (Borel-Cantelli I) Let \\(A_1, A_2, \\ldots, A_n\\) be events from probability space \\((\\Omega, \\mathcal{F}, P)\\).\nIf \\[\\sum_{n} P(A_n) &lt; \\infty\\] then:\nwith probability \\(1\\), only finitely many events among \\(A_1, A_2, \\ldots, A_n\\) occur: \\[P \\Big\\{ \\omega : \\sum_{n} \\mathbb{I}_{A_n}(\\omega) &lt; \\infty\\Big\\} = 1 \\,.\\]\n\n\nProof. An outcome \\(\\omega\\) belongs to infinitely many events \\(A_k\\), iff \\(\\omega \\in \\cap_{n} \\cup_{k\\geq n} A_k\\). By monotone convergence, \\[\\begin{array}{rl}\n  P \\Big\\{ \\omega : \\omega \\text{ belongs to infinitely many events } A_k\\Big\\}\n  & = P \\Big\\{ \\cap_{n} \\cup_{k\\geq n} A_k \\Big\\} \\\\\n  & = \\lim_n \\downarrow P \\Big\\{ \\cup_{k\\geq n} A_k \\Big\\} \\\\\n  & \\leq \\lim_n \\downarrow \\sum_{k \\geq n} P \\Big\\{ A_k \\Big\\} \\\\\n  & =  0 \\, .\n\\end{array}\\]\n\n\nProof (Proof of SLLN (direct part)). The event \\(\\Big\\{ \\omega : \\lim_n \\sum_{i=1}^n \\frac{X_i}{n} = \\mu \\Big\\}\\) belongs to the tail \\(\\sigma\\)-algebra. To check the Strong Law of Large Numbers, it suffices to check that this event has non-zero probability.\nMoreover, using the usual decomposition \\(X = (X)_+ - (X)_-\\) where \\((X)_+\\) and \\((X)_-\\) are the positive and negative parts of \\(X\\), we observe that we can assume without loss of generality that \\(X_i\\)’s are non-negative.\nRecall the definition of truncated variables \\(X_i^i = \\mathbb{I}_{X_i \\leq i}X_i\\) for \\(i \\in \\mathbb{N}\\). Let \\(S_n = \\sum_{i=1}^n X_i\\) and \\(T_n = \\sum_{i=1}^n X_i^i\\).\nThe difference \\(S_n - T_n = \\sum_{i=1}^n (X_i - X^i_i)\\) is a sum of non-negative random variables. As \\[P \\{ X_i - X^i_i &gt;0 \\} = P\\{ X_i &gt;i \\} = P\\{ X_1 &gt; i\\} \\, ,\\]\nthanks to \\(\\mathbb{E} X_1 &lt; \\infty\\), \\[\\sum_{i \\in \\mathbb{N}} P \\{ X_i - X^i_i &gt;0 \\} &lt; \\infty \\, .\\]\nBy the first Borel-Cantelli Lemma, this implies that almost surely, only finitely many events \\(\\{ X_i - X^i_i &gt;0 \\}\\) are realized. Hence almost surely, \\(T_n\\) and \\(S_n\\) differ by at most a bounded number of summands, and \\(\\lim_n \\uparrow (S_n - T_n)\\) is finite.\nNow \\[\\lim_n \\uparrow \\mathbb{E} \\frac{T_n}{n} = \\mathbb{E} X_1 \\, .\\]\nWe shall first check that \\(T_{n(k)}/n(k)\\) converges almost surely towards \\(\\mathbb{E} X_1\\) for some (almost) geometrically increasing subsequence \\((n(k))_{k \\in \\mathbb{N}}\\).\nFix \\(\\alpha&gt;1\\) and let \\(n(k) = \\lfloor \\alpha^k \\rfloor\\). If for all \\(\\epsilon&gt;0\\), almost surely, only finitely many events \\[\\Big\\{ \\Big|T_{n(k)} - \\mathbb{E}T_{n(k)} \\Big| \\geq n(k) &gt; \\epsilon \\Big\\}\\] occur, then \\(\\Big|T_{n(k)} - \\mathbb{E}T_{n(k)} \\Big|/n(k)\\) converges almost surely to \\(0\\) and thus \\(T_{n(k)}/n(k)\\) converges almost surely to \\(\\mathbb{E}X_1\\).\nLet \\[\\Theta = \\sum_{k\\in \\mathbb{N}}  P\\Big\\{ \\Big|T_{n(k)} - \\mathbb{E}T_{n(k)} \\Big| \\geq n(k) &gt; \\epsilon \\Big\\} \\, .\\]\nThanks to truncation, each \\(T_{n(k)}\\) is square-integrable. By Chebychev’s inequality: \\[P\\Big\\{ \\Big|T_{n(k)} - \\mathbb{E}T_{n(k)} \\Big| \\geq n(k) &gt; \\epsilon \\Big\\}\n\\leq \\frac{\\operatorname{var}(T_{n(k)})}{\\epsilon^2 n(k)^2} \\, .\\]\nAs \\(X_i^i\\)’s are independent, \\[\\begin{array}{rl}\n\\operatorname{var}(T_{n(k)})\n& = \\sum_{i \\leq n(k)} \\operatorname{var}(X_i^i)  \\\\\n& \\leq \\sum_{i \\leq n(k)} \\mathbb{E}\\Big[(X_i^i)^2\\Big] \\\\\n& =  \\sum_{i \\leq n(k)} \\int_0^\\infty 2 t P \\{ X^i_i &gt;t \\} \\mathrm{d}t \\\\\n& \\leq \\sum_{i \\leq n(k)} \\int_0^i 2 t P \\{ X_1 &gt;t \\} \\mathrm{d}t\n\\, .\n\\end{array}\\]\n\\[\\begin{array}{rl}\n\\Theta\n  & \\leq \\sum_{k\\in \\mathbb{N}} \\frac{1}{\\epsilon^2 n(k)^2}\\sum_{i \\leq n(k)} \\int_0^i 2 t P \\{ X_1 &gt;t \\} \\mathrm{d}t \\\\\n  & = \\frac{1}{\\epsilon^2} \\sum_{i \\in \\mathbb{N}} \\int_0^i 2 t P \\{ X_1 &gt;t \\} \\mathrm{d}t \\sum_{k: n(k)\\geq i} \\frac{1}{n(k)^2} \\, .\n\\end{array}\\] Thanks to the fact that \\(\\alpha^k &gt;1\\) for \\(k\\geq 1\\), the following holds: \\[ \\sum_{k: n(k)\\geq i} \\frac{1}{n(k)^2} =  \\sum_{k: \\lfloor \\alpha^k \\rfloor \\geq i} \\frac{1}{\\lfloor \\alpha^k \\rfloor^2}\n\\leq \\frac{4}{i^2} \\frac{\\alpha^2}{\\alpha^2- 1} \\, .\\]\n\\[\\begin{array}{rl}\n\\Theta\n  & \\leq \\frac{4\\alpha^2}{\\epsilon^2(\\alpha^2-1)} \\sum_{i \\in \\mathbb{N}} \\frac{1}{i^2}  \\int_0^i 2 t P \\{ X_1 &gt;t \\} \\mathrm{d}t \\\\\n  & \\leq \\frac{4\\alpha^2}{\\epsilon^2(\\alpha^2-1)} \\sum_{i \\in \\mathbb{N}} \\frac{1}{i^2} \\sum_{j&lt;i} \\int_{j}^{j+1} 2P \\{ X_1 &gt;t \\} \\mathrm{d}t \\\\\n  & \\leq \\frac{4\\alpha^2}{\\epsilon^2(\\alpha^2-1)} \\sum_{j=0}^\\infty  \\int_{j}^{j+1} 2t P \\{ X_1 &gt;t \\} \\mathrm{d}t \\sum_{i &gt;j} \\frac{1}{i^2} \\\\\n  & \\leq \\frac{4\\alpha^2}{\\epsilon^2(\\alpha^2-1)} \\sum_{j=0}^\\infty  \\int_{j}^{j+1} 2t P \\{ X_1 &gt;t \\} \\mathrm{d}t\n   \\frac{2}{j\\vee 1} \\\\\n   & \\leq 8\\frac{4\\alpha^2}{\\epsilon^2(\\alpha^2-1)} \\sum_{j=0}^\\infty  \\int_{j}^{j+1}  P \\{ X_1 &gt;t \\} \\mathrm{d}t \\\\\n   & \\leq 8\\frac{4\\alpha^2}{\\epsilon^2(\\alpha^2-1)} \\mathbb{E} X_1 \\\\\n   & &lt; \\infty \\, .\n\\end{array}\\] By the first Borell-Cantelli Lemma, with probability \\(1\\), only finitely many events \\[\\Big\\{ \\Big|T_{n(k)} - \\mathbb{E}T_{n(k)} \\Big| \\geq n(k) &gt; \\epsilon \\Big\\}\\] occur. As this holds for each \\(\\epsilon&gt;0\\), it holds simultaneously for all \\(\\epsilon= 1/n\\), which implies that \\(\\Big|T_{n(k)} - \\mathbb{E}T_{n(k)} \\Big|/n(k)\\) converges almost surely to \\(0\\). This also implies that \\(S_{n(k)}/n(k)\\) converges almost surely to \\(\\mathbb{E}X_1\\).\nTo complete the proof, we need to check that this holds for \\(S_n/n\\).\nIf \\(n(k) \\leq n &lt; n(k+1)\\), as \\((S_n)_n\\) is non-decreasing, \\[\\frac{n(k)}{n(k+1)}\\frac{S_{n(k)}}{n(k)}\\leq \\frac{S_n}{n}\\leq \\frac{n(k+1)}{n(k)}\\frac{S_{n(k+1)}}{n(k+1)}\\] with \\[\\frac{1}{\\alpha} \\Big(1 - \\frac{1}{\\alpha^k} \\Big)\\leq \\frac{n(k+1)}{n(k)}  \\leq \\alpha \\left(1 + \\frac{1}{\\lfloor \\alpha^k\\rfloor}\\right) \\, .\\]\nTaking \\(k\\) to infinty, almost surely \\[\\frac{1}{\\alpha} \\mathbb{E} X_1 \\leq \\liminf_n \\frac{S_n}{n} \\leq \\limsup_n \\frac{S_n}{n} \\leq \\alpha \\mathbb{E}X_1 \\, .\\]\nFinally, we may choose \\(\\alpha\\) arbitrarily close to \\(1\\), to establish the desired result.\n\\(\\square\\)\n\n\nRemark 10.1. In the statement of the Theorem, we can replace the independence assumption by a pairwise independence assumption.\n\nTheorem 10.3) shows that, under independence assumption, the conditions in Theorem 10.2) are tight. Before proceeding to the proof of Theorem 10.3), we state and prove the second Borel-Cantelli Lemma.\n\n\n\n\nLemma 10.2 (Borel-Cantelli II) Let \\(A_1, A_2, \\ldots, A_n\\) be independent events from probability space \\((\\Omega, \\mathcal{F}, P)\\).\nIf \\[\\sum_{n} P(A_n) = \\infty\\]\nthen\nwith probability \\(1\\), infinitely many events among \\(A_1, A_2, \\ldots, A_n\\) occur: \\[P \\Big\\{ \\omega : \\sum_{n} \\mathbb{I}_{A_n}(\\omega) = \\infty \\Big\\} = 1 \\,.\\]\n\n\nProof. An outcome \\(\\omega\\) does not belong to infinitely many events \\(A_k\\), iff \\(\\omega \\in \\cup_{n} \\cap_{k\\geq n} A^c_k\\). By monotone convergence, \\[\\begin{array}{rl}\n  P \\Big\\{ \\omega : \\omega \\text{ does not belong to infinitely many events } A_k\\Big\\}\n  & = P \\Big\\{ \\omega \\in \\cup_{n} \\cap_{k\\geq n} A^c_k  \\Big\\} \\\\\n  & = \\lim_n \\uparrow P \\Big\\{ \\cap_{k\\geq n} A^c_k  \\Big\\} \\\\\n  & = \\lim_n \\uparrow \\lim_{m \\uparrow \\infty } \\downarrow P \\Big\\{ \\cap_{k=n}^m A^c_k  \\Big\\} \\\\\n  & = \\lim_n \\uparrow \\lim_{m \\uparrow \\infty } \\downarrow \\prod_{k=n}^m \\Big( 1 - P (A_k)   \\Big\\} \\Big) \\\\\n  & = \\lim_n \\uparrow  \\prod_{k=n}^\\infty \\Big( 1 - P ( A_k ) \\Big) \\\\\n  & = \\lim_n \\uparrow \\exp\\Big( - \\sum_{k=n}^\\infty P ( A_k)\\Big) \\\\\n  & = \\lim_n \\uparrow 0 \\\\\n  & = 0 \\, .\n\\end{array}\\]\n\\(\\square\\)\n\n\nTheorem 10.3 (Strong law of large numbers, converse part) Let \\(X_1, \\ldots, X_n, \\ldots\\) be independently, identically distributed \\(\\mathbb{R}\\)-valued random variables over some \\((\\Omega, \\mathcal{F}, P)\\). If for some finite constant \\(\\mu\\), \\[\\lim_{n \\to \\infty}    \\sum_{i\\leq n} X_i/n =   \\mu \\qquad \\text{almost surely,}\\] then all \\(X_i\\) are integrable and \\(\\mathbb{E}X_i = \\mu.\\)\n\nWe may assume that \\(X_i\\)’s are non-negative random variables.\n\nProof. In order to check that the \\(X_i\\)’s are integrable, it suffices to show that \\[\\sum_{n=0}^\\infty P \\big\\{ X_1 &gt; n \\big\\} = \\sum_{n=0}^\\infty P \\big\\{ X_n &gt; n \\big\\} &lt; \\infty.\\]\nLet \\(S_n = \\sum_{i=1}^n X_i\\). Observe that \\[\\begin{array}{rl}\n\\Big\\{ \\omega : X_{n+1}(\\omega) &gt; n+1 \\Big\\}\n  & =  \\Big\\{ \\omega : S_{n+1}(\\omega) - S_{n}(\\omega) &gt; n+1 \\Big\\} \\\\\n  & =  \\Big\\{ \\omega : \\frac{S_{n+1}(\\omega)}{n+1} - \\frac{S_{n}(\\omega)}{n} &gt; 1 + \\frac{S_{n}(\\omega)}{n(n+1)} \\Big\\} \\, .\n\\end{array}\\] Assume by contradiction that the \\(X_i\\)’s are not integrable. Then by the second Borel-Cantelli Lemma, with probability \\(1\\), infinitely many events \\[\\Big\\{ \\omega : \\frac{S_{n+1}}{n+1} - \\frac{S_{n}}{n} &gt; 1 + \\frac{S_{n}}{n(n+1)} \\Big\\}\\]\noccur. But this cannot happen if \\(S_n/n\\) converges toward a finite limit.\n\\(\\square\\)\n\n\n\n\nThe law of large numbers is the cornerstone of consistency proofs.\nBefore shifting to non-exponential inequalities, we point a general result about events that depend on the limiting behavior of sequences of independent random variables.\n\nDefinition 10.3 (Tail sigma-algebra) Assume \\(X_1, \\ldots, X_n, \\ldots\\) are random variables. The tail \\(\\sigma\\)-algebra (or the \\(\\sigma\\)-algebra of tail events) is defined as: \\[\\mathcal{T} = \\cap_{n=}^\\infty \\sigma\\Big(X_n, X_{n+1}, \\ldots \\Big) \\, .\\]\n\nObserve that the event \\(\\sum_{i=1}^n X_i/n\\) converges towards a finite limit belongs to the tail \\(\\sigma\\)-algebra. The Strong Law of Large Numbers tells us that under integrability and independence assumptions, this tail event has probability \\(1\\). This is no accident. The \\(0-1\\)-law asserts that under independence, tail events have trivial probabilities.\n\nTheorem 10.4 (0-1-Law) Assume \\(X_1, \\ldots, X_n, \\ldots\\) are independent random variables. Any event in the tail \\(\\sigma\\)-algebra \\(\\mathcal{T}\\) has probability either \\(0\\) or \\(1\\).\n\n\nProof. It suffices to check that any event \\(A \\in \\mathcal{T}\\) satisfies \\(P(A)^2 = P(A)\\), or equivalently that \\(P(A) = P(A \\cap A) = P(A) \\times P(A)\\), that is \\(A\\) is independent of itself.\nFor any \\(n\\), as an event in \\(\\sigma\\big(X_n, X_{n+1}, \\ldots \\big)\\), \\(A\\) is independent from any event in \\(\\sigma\\big(X_1, \\ldots, X_n\\big)\\). But this entails that \\(A\\) is independent from any event in \\(\\cup_n \\sigma\\big(X_1, \\ldots, X_n\\big)\\).\nObserve that \\(\\cup_n \\sigma\\big(X_1, \\ldots, X_n\\big)\\) is a \\(\\pi\\)-system. Hence, \\(A\\) is independent from any event from the \\(\\sigma\\)-algebra generated by \\(\\cup_n \\sigma\\big(X_1, \\ldots, X_n\\big)\\), which happens to be \\(\\mathcal{F}\\). As \\(A \\in \\mathcal{T}  \\subset \\mathcal{F}\\), \\(A\\) is independent from itself.\n\\(\\square\\)\n\n\n\n\n\nExercise 10.2 Derive the second Borel-Cantelli Lemma as a special case of the \\(0-1\\)-law.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convergences I : almost sure, $L_2$, $L_1$, in Probability</span>"
    ]
  },
  {
    "objectID": "05-convergences-1.html#sec-expineq",
    "href": "05-convergences-1.html#sec-expineq",
    "title": "10  Convergences I : almost sure, \\(L_2\\), \\(L_1\\), in Probability",
    "section": "10.7 Exponential inequalities",
    "text": "10.7 Exponential inequalities\nLaws of large numbers are asymptotic statements. In applications, in Statistics, in Statistical Learning Theory, it is often desirable to have guarantees for fixed \\(n\\). Exponential inequalities are refinements of Chebychev inequality. Under strong integrability assumptions on the summands, it is possible and relatively easy to derive sharp tail bounds for sums of independent random variables.\n\n10.7.1 Hoeffding’s Lemma\nLet \\(Y\\) be a random variable taking values in a bounded interval \\([a,b]\\) and let \\(\\psi_Y(\\lambda)=\\log \\mathbb{E} e^{\\lambda (Y- \\mathbb{E}Y)}\\). Then \\[\\operatorname{var}(Y) \\leq \\frac{(b-a)^2}{4}\\qquad \\text{and} \\qquad \\psi_Y(\\lambda) \\leq \\frac{1}{2} \\frac{(b-a)^2}{4} \\, .\\]\n\n\nProof. The upper bound on the variance of \\(Y\\) has been established in Section 3.8).\nNow let \\(P\\) denote the distribution of \\(Y\\) and let \\(P_{\\lambda}\\) be the probability distribution with density \\[x \\rightarrow e^{-\\psi_{Y}\\left(  \\lambda\\right)  }e^{\\lambda (x - \\mathbb{E}Y)}\\]\nwith respect to \\(P\\).\nSince \\(P_{\\lambda}\\) is concentrated on \\([a,b]\\) (\\(P_\\lambda([a, b]) = P([a, b]) =1\\)), the variance of a random variable \\(Z\\) with distribution \\(P_{\\lambda}\\) is bounded by \\((b-a)^2/4\\). Note that \\(P_0 = P\\).\nDominated convergence arguments allow to compute the derivatives of \\(\\psi_Y(\\lambda)\\). Namely \\[\\psi'_Y(\\lambda) = \\frac{\\mathbb{E}\\Big[ (Y- \\mathbb{E}Y) e^{\\lambda (Y- \\mathbb{E}Y)} \\Big]}{\\mathbb{E} e^{\\lambda (Y- \\mathbb{E}Y)}}\n= \\mathbb{E}_{P_\\lambda} Z \\, .\\] and \\[\\psi^{\\prime\\prime}_Y(\\lambda) = \\frac{\\mathbb{E}\\Big[ (Y- \\mathbb{E}{Y})^2 e^{\\lambda (Y- \\mathbb{E}Y)} \\Big]}{\\mathbb{E} e^{\\lambda (Y- \\mathbb{E}Y)}} - \\Bigg(\\frac{\\mathbb{E}\\Big[ (Y- \\mathbb{E}{Y}) e^{\\lambda (Y- \\mathbb{E}Y)} \\Big]}{\\mathbb{E} e^{\\lambda (Y- \\mathbb{E}Y)}}\\Bigg)^2 = \\operatorname{var}_{P_\\lambda}(Z) \\, .\\]\nHence, thanks to the variance upper bound: \\[\\begin{array}{rl}\n\\psi_Y^{\\prime\\prime}(\\lambda) & \\leq \\frac{(b-a)^2}{4}~.\n\\end{array}\\] Note that \\(\\psi_{Y}(0)  = \\psi_{Y}'(0) =0\\), and by Taylor’s theorem, for some \\(\\theta \\in [0,\\lambda]\\), \\[  \\psi_Y(\\lambda) = \\psi_Y(0) + \\lambda\\psi_Y'(0)\n  + \\frac{\\lambda^2}{2}\\psi_Y''(\\theta)\n   \\leq  \\frac{\\lambda^2(b-a)^2}{8}~.\\]\n\\(\\square\\)\n\n\n\n\nThe upper bound on the variance is sharp in the special case of a Rademacher random variable \\(X\\) whose distribution is defined by \\(P\\{X =-1\\} = P\\{X =1\\} = 1/2\\). Then one may take \\(a=-b=1\\) and \\(\\operatorname{var}(X)  =1=\\left(  b-a\\right)^2/4\\).\nWe can now build on Hoeffding’s Lemma to derive very practical tail bounds for sums of bounded independent random variables.\n\nTheorem 10.5 (Hoeffding’s inequality) Let \\(X_1,\\ldots,X_n\\) be independent random variables such that \\(X_i\\) takes its values in \\([a_i,b_i]\\) almost surely for all \\(i\\leq n\\). Let \\[S=\\sum_{i=1}^n\\left(X_i- \\mathbb{E} X_i \\right)~.\\]\nThen \\[\\operatorname{var}(S) \\leq \\sum_{i=1}^n  \\frac{(b_i-a_i)^2}{4} \\, .\\]\nLet \\(v\\) denote the upper bound on the variance. For any \\(\\lambda \\in \\mathbb{R}\\), \\[\\log \\mathbb{E} \\mathrm{e}^{\\lambda S} \\leq \\frac{\\lambda^2 v}{2} \\, .\\]\nThen for every \\(t&gt;0\\), \\[P\\left\\{  S \\geq t \\right\\}  \\le\n\\exp\\left( -\\frac{t^2}{2 v}\\right)~.\\]\n\nThe proof is based on the so-called Cramer-Chernoff bounding technique and on Hoeffding’s Lemma.\n\nProof. The upper bound on variance follows from \\(\\operatorname{var}(S) = \\sum_{i=1}^n \\operatorname{var}(X_i)\\) and from the first part of Hoeffding’s Lemma.\nFor the upper-bound on \\(\\log \\mathbb{E} \\mathrm{e}^{\\lambda S}\\), \\[\\begin{array}{rl}\n\\log \\mathbb{E} \\mathrm{e}^{\\lambda S}\n& = \\log \\mathbb{E} \\mathrm{e}^{\\sum_{i=1}^n \\lambda (X_i - \\mathbb{E} X_i)} \\\\\n& = \\log \\mathbb{E} \\Big[\\prod_{i=1}^n  \\mathrm{e}^{\\lambda (X_i - \\mathbb{E} X_i)}\\Big]  \\\\\n& = \\log \\Big(\\prod_{i=1}^n  \\mathbb{E} \\Big[\\mathrm{e}^{\\lambda (X_i - \\mathbb{E} X_i)}\\Big]\\Big)  \\\\\n& = \\sum_{i=1}^n \\log \\mathbb{E} \\Big[\\mathrm{e}^{\\lambda (X_i - \\mathbb{E} X_i)}\\Big] \\\\\n& \\leq  \\sum_{i=1}^n \\frac{\\lambda^2 (b_i-a_i)^2}{8} \\\\\n& = \\frac{\\lambda^2 v}{2}\n\\end{array}\\] where the third equality comes from independence of the \\(X_i\\)’s and the inequality follows from invoking Hoeffding’s Lemma for each summand.\nThe Cramer-Chernoff technique consists of using Markov’s inequality with exponential moments. \\[\\begin{array}{rl}\n  P \\big\\{ S \\geq t \\big\\}\n  & \\leq \\inf_{\\lambda\n  \\geq 0}\\frac{\\mathbb{E} \\mathrm{e}^{\\lambda S}}{\\mathrm{e}^{\\lambda t}} \\\\\n  & \\leq \\exp\\Big(- \\sup_{\\lambda \\geq 0} \\big( \\lambda t - \\log \\mathbb{E} \\mathrm{e}^{\\lambda S}\\big) \\Big)\\\\\n  & \\leq \\exp\\Big(- \\sup_{\\lambda \\geq 0}\\big(  \\lambda t - \\frac{\\lambda^2 v}{2}\\big) \\Big) \\\\\n  & = \\mathrm{e}^{- \\frac{t^2}{2v}  } \\, .\n\\end{array}\\]\n\\(\\square\\)\n\n\n\n\nHoeffding’s inequality provides interesting tail bounds for binomial random variables which are sums of independent \\([0,1]\\)-valued random variables. However in some cases, the variance upper bound used in Hoeffding’s inequality is excessively conservative. Think for example of binomial random variable with parameters \\(n\\) and \\(\\mu/n\\), the variance upper-bound obtained from the boundedness assumption is \\(n\\) while the true variance is \\(\\mu\\). This motivates the next two exponential inequalities stated in Theorem 10.6) and Theorem 10.7).\n\nTheorem 10.6 (Bennett’s inequality) Let \\(X_1,\\ldots,X_n\\) be independent random variables with finite variance such that \\(X_i\\le b\\) for some \\(b&gt;0\\) almost surely for all \\(i\\leq n\\). Let \\[S=\\sum_{i=1}^n \\left(  X_i-\\mathbb{E} X_i\\right)\\]\nand \\(v=\\sum_{i=1}^n \\mathbb{E}\\left[X_i^2\\right]\\). Let \\(\\phi(u)=e^u-u-1\\) for \\(u\\in \\mathbb{R}\\).\nThen, for all \\(\\lambda &gt; 0\\), \\[\\log \\mathbb{E} e^{\\lambda S}  \\leq \\frac{v}{b^2} \\phi(b\\lambda) \\, ,\\] and for any \\(t&gt;0\\), \\[P\\{  S\\geq t\\}  \\leq\n\\exp\\left(  -\\frac{v}{b^2}h\\left(\\frac{bt}{v}\\right) \\right)\\]\nwhere \\(h(u)=\\phi^*(u) = (1+u)\\log(1+u) -u\\) for \\(u&gt;0\\).\n\n\nRemark 10.2. Bennett’s inequality provides us with improved tail bounds for the binomial random variable with parameters \\(n\\) and \\(\\mu/n\\). This binomial random variable is distributed like the sum \\(n\\) independent Bernoulli random variables with parameter \\(\\mu/n\\). This fits in the scope of Bennett’s inequality, we can choose \\(b=1\\) and \\(v=\\mu.\\) The obtained upper bound on the logarithmic moment generating function coincides with logarithmic moment generating function of a centered Poisson random variable with parameter \\(\\mu\\), see Theorem 8.5).\n\n\nProof. The proof combines the Cramer-Chernoff technique with an ad hoc upper bound on \\(\\log \\mathbb{E} \\mathrm{e}^{\\lambda (X_i - \\mathbb{E}X_i)}\\).\nBy homogeneity, we may assume \\(b=1\\).\nNote that \\(\\phi(\\lambda)/\\lambda^2\\) is non-decreasing over \\(\\mathbb{R}\\). For \\(x\\leq 1, \\lambda \\geq 0\\), \\(\\phi(\\lambda x)\\leq x^2 \\phi(\\lambda)\\).\n\\[\\begin{array}{rl}\n\\log \\mathbb{E} \\mathrm{e}^{\\lambda (X_i - \\mathbb{E}X_i)}\n& = \\log \\mathbb{E} \\mathrm{e}^{\\lambda X_i}  - \\lambda \\mathbb{E}X_i \\\\\n& \\leq \\mathbb{E} \\mathrm{e}^{\\lambda X_i} - 1 - \\lambda \\mathbb{E}X_i \\\\\n& =  \\mathbb{E} \\phi(\\lambda X_i) \\\\\n& = \\mathbb{E}X_i^2 \\phi(\\lambda) \\, .\n\\end{array}\\]\n\\(\\square\\)\n\n\n\n\nWhereas Bennett’s bound works well for Poisson-like random variables, our last bound is geared towards Gamma-like random variables. It is one of the pillars of statistical learning theory.\n\nTheorem 10.7 (Bernstein’s inequality) Let \\(X_1,\\ldots,X_n\\) be independent real-valued random variables. Assume that there exist positive numbers \\(v\\) and \\(c\\) such that \\(\\sum_{i=1}^n \\mathbb{E}\\left[X_i^2\\right]  \\leq v\\) and \\[\\sum_{i=1}^n\n\\mathbb{E}\\left[  \\left(X_i\\right)_+^q \\right]\n\\leq\\frac{q!}{2}vc^{q-2}\\quad \\text{for all integers $q\\geq3$}~.\\]\nLet \\(S=\\sum_{i=1}^n \\left(X_i-\\mathbb{E} X_i \\right).\\)\nThen for all \\(\\lambda\\in (0,1/c)\\), \\[\\log \\mathbb{E} \\mathrm{e}^{\\lambda (S- \\mathbb{E}S)} \\leq\n\\frac{v\\lambda^2}{2(1-c\\lambda)} \\, .\\]\nFor \\(t&gt;0\\), \\[P \\big\\{ S &gt; t \\big\\} \\leq\n\\exp\\Big( - \\frac{v}{c^2} h_1\\big(\\frac{ct}{v}\\big)\\Big)\\]\nwith \\(h_1(x)= 1+x - \\sqrt{1+2x}\\).\n\n\nProof. The proof combines again the Cramer-Chernoff technique with an ad hoc upper bound on \\(\\log \\mathbb{E} \\mathrm{e}^{\\lambda (S - \\mathbb{E}S)}\\).\nLet again \\(\\phi(u)=e^u-u-1\\) for \\(u\\in \\mathbb{R}\\).\nFor \\(\\lambda&gt;0\\), \\[\\begin{array}{rl}\n  \\phi(\\lambda X_i)\n  & = \\sum_{k=2}^\\infty \\frac{\\lambda^k X_i^k}{\\lambda^k} \\\\\n  & \\leq \\frac{\\lambda^2 X_i^2}{2!} + \\sum_{k=3}^\\infty \\frac{\\lambda^k (X_i)_+^k}{\\lambda^k} \\, .\n\\end{array}\\] For \\(c&gt; \\lambda&gt;0\\), \\[\\begin{array}{rl}\n\\log \\mathbb{E} \\mathrm{e}^{\\lambda S}\n  & = \\sum_{i=1}^n \\log \\mathbb{E} \\mathrm{e}^{\\lambda (X_i - \\mathbb{E}X_i)} \\\\\n  & \\leq \\sum_{i=1}^n \\mathbb{E} \\phi(\\lambda X_i) \\\\\n  & \\leq \\frac{\\lambda^2 \\sum_{i=1}^n  \\mathbb{E} X_i^2}{2!} + \\sum_{k=3}^\\infty \\frac{\\lambda^k \\sum_{i=1}^n \\mathbb{E}(X_i)_+^k}{k!} \\\\\n  & \\leq \\frac{\\lambda^2 v}{2} + \\sum_{k=3}^\\infty \\frac{\\lambda^k v c^{k-2}}{2} \\\\\n  & =  \\frac{\\lambda^2 v}{2 (1 - c \\lambda)} \\, .\n\\end{array}\\] The tail bound follows by maximizing \\[\\sup_{\\lambda \\in [0,1/c)}  \\lambda t - \\frac{\\lambda^2 v}{2 (1 - c \\lambda)} = \\frac{v}{c^2} \\sup_{\\eta \\in [0,1)} \\eta \\frac{ct}{v} - \\frac{\\eta^2}{2(1-\\eta)} \\, .\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convergences I : almost sure, $L_2$, $L_1$, in Probability</span>"
    ]
  },
  {
    "objectID": "05-convergences-1.html#sec-bibconvrv",
    "href": "05-convergences-1.html#sec-bibconvrv",
    "title": "10  Convergences I : almost sure, \\(L_2\\), \\(L_1\\), in Probability",
    "section": "10.8 Bibliographic remarks",
    "text": "10.8 Bibliographic remarks\n(Dudley, 2002) contains a thorough discussion of the various kinds of convergences that can be defined for random variables. In particular, (Dudley, 2002) offers a general perspective on topological issues in probability spaces. (Dudley, 2002) also tackles the problem raised by random variables that take values in (possibly infinite-dimensional) metric spaces.\nLaws of large numbers and \\(0-1\\)-laws fit in the more general framework of ergodic theorems, see (Dudley, 2002) or (Durrett, 2010). An important example of law of large numbers is the Asymptotic Equipartition Property (AEP) in Information Theory. Note that it holds for a much larger class of sources than the set of memoryless sources (infinite product probability spaces). See (Cover & Thomas, 1991) or [csiszar:korner:1981].\nIntroduction to exponential inequalities and their applications can be found in (Massart, 2007), (Boucheron, Lugosi, & Massart, 2013).\n\n\n\n\nBoucheron, S., Lugosi, G., & Massart, P. (2013). Concentration inequalities. Oxford University Press.\n\n\nCover, T., & Thomas, J. (1991). Elements of information theory. John Wiley & sons.\n\n\nDudley, R. M. (2002). Real analysis and probability (Vol. 74, p. x+555). Cambridge: Cambridge University Press.\n\n\nDurrett, R. (2010). Probability: Theory and examples. Cambridge University Press.\n\n\nMassart, P. (2007). Concentration inequalities and model selection. Ecole d’eté de probabilité de saint-flour xxxiv (Vol. 1896). Springer-Verlag.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Convergences I : almost sure, $L_2$, $L_1$, in Probability</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html",
    "href": "08-convergence-3.html",
    "title": "11  Convergence in distribution",
    "section": "",
    "text": "11.1 Motivation\nRecall Lesson 1. Consider Binomial distributions with parameters \\((n, \\lambda/n)\\) and Poisson distribution with parameter \\(\\lambda\\). Graphical inspection of probability mass functions suggests that as \\(n\\) grows, Binomial distributions with parameters \\((n, \\lambda/n)\\) look more and more alike Poisson distribution with parameter \\(\\lambda\\). Comparing probability generating functions is more compelling. The probability generating function of the Binomial distribution with parameters \\((n, \\lambda/n)\\) is \\(s \\mapsto (1 +\\lambda(s-1)/n )^n\\). As \\(n\\) tends towards infinity, the probability generating functions of the Binomials converge pointwise towards the probability generating function of the Poisson distribution with mean \\(\\lambda\\): \\(s \\mapsto \\exp(\\lambda(s-1))\\). In Lesson 1, we saw other examples of distributions which tend to look alike some limiting distributions as some parameter moves.\nIn Lesson 10, we equipped the set \\(L_0(\\Omega, \\mathcal{F}, P)\\) with topologies (\\(L_p\\), almost sure convergence, convergence in probability). In this lesson, we consider the set of probability distributions over some measurable space \\((\\Omega, \\mathcal{F})\\). This set can be equipped with a variety of topologies. We shall focus on the topology defined by convergence in distribution also called weak convergence.\nIn Section 11.2), we introduce weak and vague convergences for sequences of probability distributions. In Section 11.3) Weak convergence induces the definition of convergence in distribution for random variables that possibly live on different probability spaces (just as our occupancy scores in Lesson 1).\nSection 11.4) is dedicated to the Portemanteau Theorem. This theorem lists a number of alternative and equivalent characterizations of convergence in distribution. Alternative characterizations are useful in two respects: they may be easier to check than the characterization used in the definition; they may supply a larger range of applications.\nIn Section 11.5), we state and prove the Lévy continuity theorem. The Levy continuity theorem relates convergence in distribution with pointwise convergence of characteristic functions: characteristic functions not only allow us to identify probability distributions, they are also convergence determining. It could be one more line in the statement of Theorem 11.1). But the Lévy continuity Theorem stands out because it provides us with a concise proof of the Central Limit Theorem for normalized sums of centered i.i.d. random variables. This is the content of Section 11.8).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-weakandvague",
    "href": "08-convergence-3.html#sec-weakandvague",
    "title": "11  Convergence in distribution",
    "section": "11.2 Weak convergence, vague convergence",
    "text": "11.2 Weak convergence, vague convergence\nWeak convergence of probability measures assesses the proximity of probability measures by comparing their action on a collection of test functions.\n\nDefinition 11.1 (Weak convergence) A sequence of probability distributions \\((P_n)_{n \\in\\mathbb{N}}\\) sur \\(\\mathbb{R}^k\\) converges weakly towards probability distribution \\(P\\) (on \\(\\mathbb{R}^k\\))\niff\nfor any bounded and continuous function \\(f\\) from \\(\\mathbb{R}^k\\) to \\(\\mathbb{R}\\), the sequence \\((\\mathbb{E}_{P_n} [f])_{n \\in \\mathbb{N}}\\) converges towards \\(\\mathbb{E}_P [f]\\).\n\n\nRemark 11.1. We shall see that the there is some flexibility in the choice of the class of test functions.\nBut this choice is not unlimited.\nIf we restrict the collection of test functions to continuous functions with compact support (which are always bounded), we obtain a different notion of convergence.\n\n\nDefinition 11.2 (Vague convergence) A sequence of probability distributions \\((P_n)_{n \\in\n\\mathbb{N}}\\) sur \\(\\mathbb{R}^k\\) converges vaguely towards measure \\(\\mu\\) (on \\(\\mathbb{R}^k\\)) iff for any continuous function \\(f\\) with compact support from \\(\\mathbb{R}^k\\) to \\(\\mathbb{R}\\), the sequence \\((\\mathbb{E}_{P_n} [f])_{n \\in \\mathbb{N}}\\) converges towards \\(\\mathbb{E}_P [f]\\).\n\n\nExample 11.1 Consider the sequence of probability masses over the integers \\((\\delta_n)_{n \\in \\mathbb{N}}\\). This sequence converges vaguely towards the null measure. It does not converge weakly.\n\nThe next question deserves further thinking.\n\nExercise 11.1 If a sequence of probability distributions over \\(\\mathbb{R}^k\\) converges vaguely towards a probability measure, does it also converge weakly towards this probability measure?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-ConvDistribution",
    "href": "08-convergence-3.html#sec-ConvDistribution",
    "title": "11  Convergence in distribution",
    "section": "11.3 Convergence in distribution",
    "text": "11.3 Convergence in distribution\n\nDefinition 11.3 (Convergence in distribution) A sequence \\((X_n)_{n \\in \\mathbb{N}}\\) of \\(\\mathbb{R}^k\\)-valued random variables defined on a sequence of probability spaces \\((\\Omega_n, \\mathcal{F}_n, P_n)\\) converges in distribution if the sequence \\((P_n \\circ X_n^{-1})_{n \\in \\mathbb{N}}\\) converges weakly. This is denoted by \\[\nX_n \\rightsquigarrow X \\qquad \\text{or} \\qquad  X_n \\rightsquigarrow \\mathcal{L}\n\\]\n(\\(\\mathcal{L}\\) denotes a probability distribution), the probability spaces are defined implicitly\n\nIn order to check or use convergence in distribution, many equivalent characterizations are available. Some of them are listed in the Portemanteau Theorem.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-PortManteau",
    "href": "08-convergence-3.html#sec-PortManteau",
    "title": "11  Convergence in distribution",
    "section": "11.4 Portemanteau Theorem",
    "text": "11.4 Portemanteau Theorem\nThe next list of equivalent characterizations of weak convergence is not exhaustive.\n\nTheorem 11.1 (Portemanteau Theorem) A sequence of probability distributions \\((P_n)_{n \\in \\mathbb{N}}\\) on \\(\\mathbb{R}^k\\) converges weakly towards a probability distribution \\(P\\) (on \\(\\mathbb{R}^k\\)) iff one of the equivalent properties hold:\n\nFor every bounded continuous function \\(f\\) from \\(\\mathbb{R}^k\\) to \\(\\mathbb{R}\\), the sequence \\(\\mathbb{E}_{P_n} [f]\\) converges towards \\(\\mathbb{E}_P [f]\\).\nFor every bounded uniformly continuous function \\(f\\) from \\(\\mathbb{R}^k\\) to \\(\\mathbb{R}\\), the sequence \\(\\mathbb{E}_{P_n} [f]\\) converges towards \\(\\mathbb{E}_P [f]\\).\nFor every bounded Lipschitz function \\(f\\) from \\(\\mathbb{R}^k\\) to \\(\\mathbb{R}\\), the sequence \\(\\mathbb{E}_{P_n} [f]\\) converges towards \\(\\mathbb{E}_P [f]\\).\nFor every \\(P\\)-almost surely bounded and continuous function \\(f\\) from \\(\\mathbb{R}^k\\) to \\(\\mathbb{R}\\), the sequence \\((\\mathbb{E}_{P_n} [f])\\) converges towards \\(\\mathbb{E}_P [f]\\).\nFor every closed subset \\(F\\) of \\(\\mathbb{R}^k\\), \\(\\limsup P_n (F) \\leq  P(F).\\)\nFor every open subset \\(O\\) of \\(\\mathbb{R}^k\\), \\(\\liminf P_n (O) \\geq  P(O).\\)\nFor every Borelian \\(A\\) such that \\(P(A^\\circ) = P(\\overline{A})\\) (the boundary of \\(A\\) is \\(P\\)-negligible), \\(\\lim_n P_n(A)=P(A)\\).\n\n\nIn English, as in French, a portemanteau is a suitcase.\n\nProof. Implications \\(1) \\Rightarrow 2) \\Rightarrow 3)\\) are obvious. Lévy’s continuity theorem, the major result from Section 11.5) entails that \\(3) \\Rightarrow 1)\\).\n\\(4)\\).\nThat \\(5) \\Leftrightarrow 6)\\) follows from the fact that the complement of a closed set is an open set.\n\\(5)\\) and \\(6)\\) imply \\(7)\\): \\[\n\\limsup_n P_n(\\overline{A}) \\leq P(\\overline{A}) = P(A^\\circ) \\leq \\liminf_n P_n(A^\\circ) \\, .\n\\] By monotony: \\[\n\\liminf_n P_n(A^\\circ) \\leq \\liminf_n P_n (A) \\leq \\limsup_n P_n(A) \\leq \\limsup_n P_n (\\overline(A)) \\, .\n\\] Combining leads to \\[\n\\lim_n P_n(A) = \\liminf_n P_n (A) = \\limsup_n P_n(A) = P(A^\\circ) = P(\\overline{A}) \\, .\n\\]\nLet us check that \\(3) \\Rightarrow 5)\\). Let \\(F\\) be a closed subset of \\(\\mathbb{R}^k\\). For \\(x\\in \\mathbb{R}^k\\), let \\(\\mathrm{d}(x,F)\\) denote the distance from \\(x\\) to \\(F\\). For \\(m \\in \\mathbb{N}\\), let \\(f_m(x) = \\big(1 - m \\mathrm{d}(x, F)\\big)_+\\). The function \\(f_m\\) is \\(m\\)-Lipschitz, lower bounded by \\(\\mathbb{I}_F\\), and for every \\(x \\in \\mathbb{R}^k\\) \\(\\lim_m \\downarrow f_m(x)= \\mathbb{I}_F(x)\\).\nWeak convergence of \\(P_n\\) to \\(P\\) implies \\[\n\\lim_n \\mathbb{E}_{P_n} f_m = \\mathbb{E}_P f_m\n\\] hence for every \\(m \\in \\mathbb{N}\\) \\[\n\\limsup_n \\mathbb{E}_{P_n} \\mathbb{I}_F \\leq \\mathbb{E}_P f_m \\, .\n\\] Taking the limit on the right side leads to \\[\n\\limsup_n P_n(F) = \\limsup_n \\mathbb{E}_{P_n} \\mathbb{I}_F \\leq \\lim_m \\downarrow \\mathbb{E}_P f_m = \\mathbb{E}_P \\mathbb{I_F} = P(F) \\, .\n\\]\nAssume now that \\(7)\\) holds. Let us show that this entails \\(1)\\)\nLet \\(f\\) be a bounded continuous function. Assume w.l.o.g. that \\(f\\) is non-negative and upper-bounded by \\(1\\). Recall that for each \\(\\sigma\\)-finite measure \\(\\mu\\) \\[\n\\int f \\mathrm{d}\\mu = \\int_{[0,\\infty)} \\mu\\{f &gt; t\\} \\mathrm{d}t \\, .\n\\] This holds for all \\(P_n\\) and \\(P\\). Hence \\[\n\\mathbb{E}_{P_n} f = \\int_{[0,\\infty)} P_n \\{ f &gt; t \\} \\mathrm{d}t\n\\] As \\(\\overline{\\{ f &gt; t\\}}= \\{ f \\geq t\\}\\), \\(\\overline{\\{ f &gt; t\\}} \\setminus \\{ f &gt; t\\}^\\circ = \\{ f = t\\}\\). The set of values \\(t\\) such that \\(P \\{ f = t\\}&gt;0\\) is at most countable and thus Lebesgue-negligible. Let \\(E\\) be its complement. For \\(t\\in E\\), \\(\\lim_n P_n\\{ f&gt; t\\} = P\\{f &gt;t\\}\\). \\[\\begin{align*}\n  \\lim_n \\mathbb{E}_{P_n} f\n    & = \\lim_n \\int_{[0, 1]} P_n \\{ f &gt;t \\} \\mathrm{d}t \\\\\n    & = \\lim_n \\int_{[0, 1]} P_n \\{ f &gt;t \\} \\mathbb{I}_E(t) \\mathrm{d}t \\\\\n    & = \\int_{[0, 1]} \\lim_n  P_n \\{ f &gt;t \\} \\mathbb{I}_E(t) \\mathrm{d}t \\\\\n    & = \\int_{[0,1]} P\\{f &gt;t\\}  \\mathbb{I}_E(t) \\mathrm{d}t \\\\\n    & = \\int_{[0,1]} P\\{f &gt;t\\}   \\mathrm{d}t \\\\\n    & = \\mathbb{E}_P f \\, .\n\\end{align*}\\]\n\nFor probability measures over \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\), weak convergence is determined by cumulative distribution functions. This is sometimes taken as a definition of weak convergence in elementary books.\n\nCorollary 11.1 A sequence of probability measures defined by their cumulative distribution functions \\((F_n)_n\\) converges weakly towards a probability measure defined by cumulative distribution function \\(F\\) iff \\(\\lim_n F_n(x) = F(x)\\) at every \\(x\\) which is a continuity point of \\(F\\).\n\nFor probability measures over \\((\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))\\), weak convergence is also determined by quantile functions.\n\nProposition 11.1 A sequence of probability measures defined by their quantile functions \\((F^\\leftarrow_n)_n\\) converges weakly towards a probability measure defined by quantile function \\(F^\\leftarrow\\) iff \\(\\lim_n F_n^\\leftarrow(x) = F^\\leftarrow(x)\\) at every \\(x\\) which is a continuity point of \\(F^\\leftarrow\\).\n\n\nProve Proposition Proposition 11.1.\n\n\nProposition 11.2 (Almost sure representation) If \\((X_n)_n\\) converges in distribution towards \\(X\\), then there exists a probability space \\((\\Omega, \\mathcal{F}, P)\\) with random variables \\((Y_n)_n\\) and \\(Y\\) such that \\(X_n \\sim Y_n\\) for all \\(n\\), \\(X\\sim Y\\), and \\[\nY_n \\to Y \\qquad P\\text{-a.s.}\n\\]\n\n\nRemark 11.2. The random variables \\((X_n)_n\\) and \\(X\\) may live on different probability spaces.\n\nWhen random variables \\(X_n\\) are real-valued, Proposition 11.2 follows easily from Proposition Proposition 11.1.\n\nProof. Let \\(\\Omega= [0,1], \\mathcal{F}=\\mathcal{B}(\\mathbb{R})\\) and \\(\\omega\\) be uniformly distributed over \\(\\Omega = [0,1]\\). Let \\(Y_n = F_n^\\leftarrow(\\omega)\\) and \\(Y = F^\\leftarrow(\\omega)\\).\nThen for each \\(n\\), \\[\nP \\Big\\{ Y_n \\leq t \\Big\\} = P\\Big\\{\\omega : F_n^\\leftarrow(\\omega) \\leq t \\Big\\}\n= P\\Big\\{\\omega : \\omega \\leq F_n(t) \\Big\\} = F_n(t)\n\\] so that \\(Y_n \\sim F_n\\). And by the same argument, \\(Y \\sim F\\).\nAs a non-decreasing function has at most countably many discontinuities, \\[\nP\\Big\\{\\omega : F^\\leftarrow\\text{ is continuous at }\\omega \\Big\\}=1\\,.\n\\] Now, assume \\(\\omega\\) is a continuity point of \\(F^{\\leftarrow}\\). Then by Proposition Proposition 11.1, \\(\\lim_n F_n^{\\leftarrow}(\\omega) = F^\\leftarrow(\\omega)\\). This translates to \\[\nP \\Big\\{\\omega :  \\lim_n Y_n(\\omega) =Y(\\omega) \\Big\\} = 1 \\, .\n\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-LevyCont",
    "href": "08-convergence-3.html#sec-LevyCont",
    "title": "11  Convergence in distribution",
    "section": "11.5 Lévy continuity theorem",
    "text": "11.5 Lévy continuity theorem\n\nTheorem 11.2 (Lévy’s continuity theorem) A sequence \\((P_n)_n\\) of probability distributions over \\(\\mathbb{R}^d\\) converges weakly towards a probability distribution \\(P\\) over \\(\\mathbb{R}^d\\) iff the sequence of characteristic functions converges pointwise towards the characteristic function of \\(P\\).\n\n\nRemark 11.3. Theorem 11.2 asserts that weak convergence of probability measures is characterized by a very small subset of bounded continuous functions. To warrant weak convergence of \\((P_n)_n\\) towards \\(P\\) it is enough to check that \\(\\mathbb{E}_{P_n}f \\to \\mathbb{E}_Pf\\) for functions \\(f\\) in family \\(\\{ \\cos(t \\cdot), \\sin(t \\cdot) : t \\in \\mathbb{R} \\}\\). These functions are bounded and infinitely many times differentiable.\n\n\nLet \\((X_n)_n, X\\) and \\(Z\\) live on the same probability space. If \\((X_n)_n\\), \\(X\\) and \\(Z\\) are random variables such that for every \\(\\sigma&gt;0\\), \\(X_n + \\sigma Z \\rightsquigarrow X+\\sigma Z\\), then \\(X_n \\rightsquigarrow X\\).\n\n\nProof. Let \\(h\\) be bounded by \\(1\\) and \\(1\\)-Lipschitz \\[\\begin{align*}\n\\Big| \\mathbb{E} h(X_n) -h(X) \\Big|\n& \\leq \\Big| \\mathbb{E} h(X_n) - h(X_n + \\sigma Z) \\Big| \\\\\n& \\qquad +  \\Big| \\mathbb{E} h(X_n + \\sigma Z) - h(X + \\sigma Z) \\Big| \\\\\n& \\qquad +  \\Big| \\mathbb{E} h(X + \\sigma Z) - h(X) \\Big|\n\\end{align*}\\] The first and third summand can be handled in the same way.\nLet \\(\\epsilon &gt;0\\), \\[\\begin{align*}\n\\Big| \\mathbb{E} h(X_n) - h(X_n + \\sigma Z) \\Big|\n& \\leq \\Big| \\mathbb{E} (h(X_n) - h(X_n + \\sigma Z)) \\mathbb{I}_{\\sigma |Z|&gt;\\epsilon} \\Big| \\\\\n& \\qquad + \\Big| \\mathbb{E} (h(X_n) - h(X_n + \\sigma Z)) \\mathbb{I}_{\\sigma |Z|\\leq\\epsilon} \\Big| \\\\\n& \\leq 2 P\\{\\sigma |Z|&gt;\\epsilon\\} + \\epsilon \\, .\n\\end{align*}\\] Combining the different bounds leads to \\[\\begin{align*}\n  \\Big| \\mathbb{E} h(X_n) -h(X) \\Big|\n  & \\leq  2  P\\{\\sigma |Z|&gt;\\epsilon\\} + \\epsilon + \\Big| \\mathbb{E} h(X_n + \\sigma Z) - h(X + \\sigma Z) \\Big|\n\\end{align*}\\] The last summand on the right-hand-side tends to \\(0\\) as \\(n\\) tends to infinity. The first summand tends to \\(0\\) as \\(\\sigma\\) tends to \\(0\\).\nHence\n\\[\n\\limsup_n \\Big| \\mathbb{E} h(X_n) - h(X_n + \\sigma Z) \\Big| \\leq \\epsilon \\, .\n\\]\n\\(\\square\\)\n\n\nLemma 11.1 (Scheffé’s Lemma) Let \\((P_n)_n\\) be a sequence of absolutely continuous probability distributions with densities \\((f_n)_n\\). Assume that densities \\((f_n)_n\\) converge pointwise towards the density \\(f\\) of some probability distribution \\(P\\), then \\(P_n \\rightsquigarrow P\\).\n\n\nProof. \\[\\begin{align*}\n\\int_{\\mathbb{R}} |f_n(x) - f(x)| \\mathrm{d}x\n& = \\int_{\\mathbb{R}} (f(x) - f_n(x))_+  \\mathrm{d}x + \\int_{\\mathbb{R}} (f(x) - f_n(x))_-  \\mathrm{d}x \\\\\n& = 2 \\int_{\\mathbb{R}} (f(x) - f_n(x))_+  \\mathrm{d}x  \\, .\n\\end{align*}\\] Observe \\((f - f_n)_+ \\leq f\\) which belongs to \\(\\mathcal{L}_1(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}), \\text{Lebesgue})\\). And \\((f-f_n)_+\\) converges pointwise to \\(0\\). Hence, by the dominated convergence theorem, \\(\\lim_n \\int_{\\mathbb{R}} |f_n - f| \\mathrm{d}x =0\\).\nFor any \\(A \\in \\mathcal{B}(\\mathbb{R})\\), \\[\nP_n(A) - P(A) = \\int_{\\mathbb{R}} \\mathbb{I}_A (f_n -f) \\leq \\int_{\\mathbb{R}} |f_n -f| \\, .\n\\] We have proved more than weak convergence, namely \\[\n\\lim_n \\sup_{A \\in \\mathcal{B}(\\mathbb{R})} |P_n(A) - P(A)| = 0 \\, .\n\\]\n\\(\\square\\)\n\n\nProof (Proof of continuity theorem). Assume the characteristic functions of \\((X_n)_n\\) converges pointwise towards the characteristic function of \\(X\\).\nLet \\(Z\\) be a standard Gaussian random variable, independent of all \\((X_n)_n\\) and of \\(X\\). For \\(\\sigma&gt;0\\), the distributions of \\(X_n + \\sigma Z\\) and \\(X + \\sigma Z\\) have densities that are uniquely determined by the characteristic functions of \\(X_n\\) and \\(X\\). Moreover, a dominated convergence argument shows that the densities of \\(X_n + \\sigma Z\\) converge pointwise towards the density of \\(X + \\sigma Z\\). By Scheffé’s Lemma, this entails that \\(X_n + \\sigma Z \\rightsquigarrow X + \\sigma Z\\).\nAs this holds for all \\(\\sigma&gt;0\\), this entails that \\(X_n \\rightsquigarrow X.\\)\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-refineLevy",
    "href": "08-convergence-3.html#sec-refineLevy",
    "title": "11  Convergence in distribution",
    "section": "11.6 Refining the continuity theorem",
    "text": "11.6 Refining the continuity theorem\nIn some situations, we can prove that a sequence of characteristic functions converges pointwise towards some function, but we have no candidate for the limiting distribution. The question arises whether the pointwise limit of characteristic functions is the characteristic function of some probability distribution or something else.\nThe answer may be negative: if \\(P_n = \\mathcal{N}(0, n)\\), the sequence of characteristic functions is \\(\\big(t \\mapsto \\exp(-nt^2/2)\\big)_n\\) which converges pointwise to \\(0\\) except at \\(0\\) where it is equal to \\(1\\) all along. The limit is not the characteristic function of any probability measure: it is not continuous at \\(0\\).\nThe next Theorem settles the question.\n\nTheorem 11.3 (Lévy’s continuity theorem, second form) A sequence \\((P_n)_n\\) of probability distributions over \\(\\mathbb{R}\\) converges weakly towards a probability distribution over \\(\\mathbb{R}\\) iff the sequence of characteristic functions converges pointwise towards a function that is continuous at \\(0\\). The limit function is the characteristic function of some probability distribution.\n\n\nDefinition 11.4 (Uniform tightness) A sequence of Probability measures \\((P_n)_n\\) over \\(\\mathbb{R}\\) is uniformly tight if for every \\(\\epsilon &gt;0\\), there exists some compact \\(K \\subseteq \\mathbb{R}\\) such that\n\\[\\forall n, \\qquad P_n(K) \\geq 1 - \\epsilon\\, .\\]\n\n\nExercise 11.2 To establish uniform tightness of \\((P_n)_n\\), it is enough to show that for every \\(\\epsilon&gt;0\\), there exists some \\(n_0(\\epsilon)\\), and some compact \\(K \\subseteq \\mathbb{R}\\) such that\n\\[\\forall n \\geq n_(\\epsilon), \\qquad P_n(K) \\geq 1 - \\epsilon\\, .\\]\n\nWe admit the (important) next Theorem.\n\nTheorem 11.4 (Prokhorov-Le Cam) If \\((P_n)_n\\) is a uniformly tight sequence of probability measures on \\(\\mathbb{R}\\), then there exists some probability measure \\(P\\) and some subsequence \\((P_{n(k)})_{k \\in \\mathbb{N}}\\) such that\n\\[P_{n(k)} \\rightsquigarrow P \\, .\\]\n\nThen\n\nLemma 11.2 (Uniform tightness Lemma) Let \\((P_n)_n\\) be a sequence of probability distributions over \\(\\mathbb{R}\\), with characteristic functions \\(\\widehat{F}_n\\). If the sequence \\((\\widehat{F}_n)_n\\) converge pointwise towards a function that is continuous at \\(0\\) then the sequence \\((P_n)_n\\) is uniformly tight.\n\nWe shall use the following technical upper bound which is illustrated in Figure 11.1:\n\\[\n\\forall t \\in \\mathbb{R} \\setminus[-1,1], \\qquad   \\frac{\\sin(t)}{t} \\leq \\sin(1) \\leq \\frac{6}{7}\\, .\n\\]\n\n\n\n\n\n\n\n\n\nFigure 11.1: The proof of the truncation inequality takes advantage on easy bounds satisfied by the \\(\\operatorname{sinc}\\) function.\n\n\n\n\n\n\n\nProposition 11.3 (Truncation Lemma) Let \\(\\widehat{F}\\) be the characteristic function of some probability measure \\(P\\) on the real line, then for all \\(u &gt; 0\\): \\[\n\\frac{1}{u}\\int_0^u \\big(1 - \\operatorname{Re}\\widehat{F}(v)\\big) \\mathrm{d}v  \\geq \\frac{1}{7} P \\Big[\\frac{-1}{u}, \\frac{1}{u}\\Big]^c \\, .\n\\]\n\n\nProof (Proof of Truncation Lemma). \\[\\begin{align*}\n\\frac{1}{u}\\int_0^u \\big(1 - \\operatorname{Re}\\widehat{F}_n(v)\\big) \\mathrm{d}v\n  & = \\frac{1}{u}\\int_0^u \\Big(\\int_{\\mathbb{R}} \\big(1 - \\cos(v w)\\big)\\mathrm{d}F(w) \\Big)\\mathrm{d}v \\\\\n  & =  \\int_{\\mathbb{R}}\\int_0^u \\frac{1}{u} \\Big( \\big(1 - \\cos(v w)\\big) \\mathrm{d}v  \\Big)\\mathrm{d}F(w) \\\\\n  & =  \\int_{\\mathbb{R}}  \\Big( 1 - \\frac{\\sin(uw)}{uw}  \\Big)\\mathrm{d}F(w) \\\\\n  & \\geq  \\int_{|uw| \\geq 1}  \\Big( 1 - \\frac{\\sin(uw)}{uw}  \\Big)\\mathrm{d}F_n(w) \\\\\n  & \\geq (1- \\sin(1)) P \\Big[\\frac{-1}{u}, \\frac{1}{u}\\Big]^c \\,\n\\end{align*}\\] where the two inequalities follow from the bounds on the \\(\\operatorname{sinc}\\) function.\n\\(\\square\\)\n\n\nProof (Proof of Uniform tightness Lemma). Assume that the sequence \\((\\widehat{F}_n)_n\\) converge pointwise towards a function \\(\\widehat{F}\\) that is continuous at \\(0\\).\nNote that \\(\\widehat{F}_n(0)=1\\) for all \\(n\\), hence, trivially, \\(1 =\\lim_n \\widehat{F}_n(0) = \\widehat{F}(0)\\).\nAs \\(|\\operatorname{Re}\\widehat{F}_n(t)|\\leq 1\\), \\(|\\operatorname{Re}\\widehat{F}(t)|\\leq 1\\) also holds.\nFix \\(\\epsilon&gt;0\\), as \\(\\widehat{F}\\) is continuous at \\(0\\), for some \\(u&gt;0\\), for all \\(v \\in [-u,u]\\), \\(0 \\geq 1- \\widehat{F}(u) \\leq \\epsilon/2.\\) Hence, \\[\n0 \\leq \\frac{1}{u}\\int_0^u \\big(1 - \\operatorname{Re}\\widehat{F}(v)\\big) \\mathrm{d}v \\leq \\epsilon/2 \\, .\n\\]\nBy dominated convergence, \\[\n\\lim_n \\frac{1}{u}\\int_0^u \\big(1 - \\operatorname{Re}\\widehat{F}_n(v)\\big) \\mathrm{d}v\n= \\frac{1}{u}\\int_0^u \\big(1 - \\operatorname{Re}\\widehat{F}(v)\\big) \\mathrm{d}v  \\leq \\epsilon/2 \\, .\n\\]\nFor sufficiently large \\(n\\), \\(0 \\leq \\frac{1}{u}\\int_0^u \\big(1 - \\operatorname{Re}\\widehat{F}_n(v)\\big) \\mathrm{d}v \\leq \\epsilon\\).\nApplying the truncation Lemma, for sufficiently large \\(n\\), we have \\[\nP_n \\Big[\\frac{-1}{u}, \\frac{1}{u}\\Big]^c \\leq 7 \\epsilon \\, .\n\\] The interval \\(\\Big[\\frac{-1}{u}, \\frac{1}{u}\\Big]\\) is compact.\n\\(\\square\\)\n\n\nProof (Proof of the second form of the continuity theorem). We combine the Uniform Tightness Lemma and the Prokhorov-Le Cam Theorem. Under the assumptions of the second form of the continuity Theorem, there is a probability measure \\(P\\) (with characteristic function \\(\\widehat{F}\\)), and a subsequence \\((P_{n(k)})_{k \\in \\mathbb{N}}\\) such that \\(P_{n(k)} \\rightsquigarrow P \\text{ as } {k \\to \\infty}\\). This entails \\(\\widehat{F}_{n(k)} \\rightarrow \\widehat{F} \\text{ as } {k \\to \\infty}\\) pointwise. This also entails that \\(\\widehat{F}_n \\rightarrow \\widehat{F}\\) pointwise for the whole sequence. Finally, we are able to invoke Theorem 11.2) to conclude \\(P_{n}\\rightsquigarrow  P \\text{ as }{n \\to \\infty}\\).\n\\(\\square\\)\n\n\nRemark 11.4. All definitions and results in this section can be extended to the \\(k\\)-dimensional setting for all \\(k \\in \\mathbb{N}\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-relatconv",
    "href": "08-convergence-3.html#sec-relatconv",
    "title": "11  Convergence in distribution",
    "section": "11.7 Relations between convergences",
    "text": "11.7 Relations between convergences\nThe alternative characterizations of weak convergence provided by the Portemanteau Theorem (Theorem 11.1)) facilitate the proof of the next Proposition.\n\nConvergence in probability implies convergence in distribution.\n\n\nProof. Assume \\((X_n)_n\\) converges in probability towards \\(X\\).\nLet \\(h\\) be a bounded and Lipschitz function. Without loss of generality, assume that \\(|f(x)|\\leq 1\\) for all \\(x\\) and \\(|f(x)-f(y)|\\leq \\mathrm{d}(x,y)\\).\nLet \\(\\epsilon&gt;0\\). \\[\\begin{align*}\n\\Big| \\mathbb{E}h(X_n) - \\mathbb{E}h(X)  \\Big|\n& = \\Big| \\mathbb{E}\\Big[(h(X_n) - h(X) \\mathbb{I}_{\\mathrm{d}(X,X_n)&gt; \\epsilon}\\Big] \\\\\n& \\qquad  + \\mathbb{E}\\Big[(h(X_n) - h(X) \\mathbb{I}_{\\mathrm{d}(X,X_n)\\leq \\epsilon}\\Big] \\Big| \\\\\n& \\leq  \\mathbb{E}\\Big[2 \\mathbb{I}_{\\mathrm{d}(X,X_n)&gt; \\epsilon} \\Big] \\\\\n& \\qquad + \\mathbb{E}\\Big[ |h(X_n) - h(X)| \\mathbb{I}_{\\mathrm{d}(X,X_n)\\leq \\epsilon}\\Big] \\\\\n& \\leq 2 P\\big\\{ \\mathrm{d}(X,X_n)&gt; \\epsilon \\big\\} + \\epsilon \\, .\n\\end{align*}\\] Convergence in probability entails that \\[\n\\limsup_n \\Big| \\mathbb{E}h(X_n) - \\mathbb{E}h(X)  \\Big| \\leq \\epsilon.\n\\] As this holds for every \\(\\epsilon &gt;0\\), for every bounded Lipschitz function \\(h\\), \\(\\lim_n \\Big| \\mathbb{E}h(X_n) - \\mathbb{E}h(X)  \\Big|=0\\). This is sufficient to establish convergence in distribution of \\((X_n)_n\\).\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-clt",
    "href": "08-convergence-3.html#sec-clt",
    "title": "11  Convergence in distribution",
    "section": "11.8 Central limit theorem",
    "text": "11.8 Central limit theorem\nThe Lévy Continuity Theorem (Theorem 11.2)) is the conerstone of a very concise proof the simplest version of the Central Limit Theorem (CLT). Under square-integrability assumption, the CLT refines the Laws of Large Numbers. It states that as \\(n\\) tends to infinity, the fluctuations of the empirical mean \\(\\sum_{i=1}^n X_i/n\\) around its expectation tends to be of order \\(1/\\sqrt{n}\\) and, once rescaled, to be normally distributed.\n\nTheorem 11.5 Let \\(X_1, \\ldots, X_n, \\ldots\\) be i.i.d. with finite variance \\(\\sigma^2\\) and expectation \\(\\mu\\). Let \\(S_n = \\sum_{i=1}^n X_i\\).\n\\[\\sqrt{n} \\left(\\frac{S_n}{n} - \\mu \\right) \\rightsquigarrow \\mathcal{N}\\big(0, \\sigma^2 \\big) \\, .\\]\n\n\nProof. Let \\(\\widehat{F}\\) denote the characteristic function of the (common) distribution of the random variables \\(((X_i-\\mu)/\\sigma)_i\\). Recall fron Lesson 8, that the centering and square integrability assumptions imply that\n\\[\n\\widehat{F}(t) = \\widehat{F}(0) + \\widehat{F}'(0) t + \\frac{\\widehat{F}'(0)}{2} t^2\n+ t^2 R(t)= 1 - \\frac{t^2}{2} + t^2 R(t)\n\\] where \\(\\lim_{t \\to 0} R(t)=0\\). Let \\(\\widehat{F}_n\\) denote the characteristic function of \\(\\sqrt{n} \\left(\\frac{S_n}{n} - \\mu \\right)/\\sigma\\). Fix \\(t \\in \\mathbb{R}\\), \\[\n\\widehat{F}_n(t) = \\Big(\\widehat{F}(t/\\sqrt{n})\\Big)^n\n= \\Big(1 - \\frac{t^2}{2n} + \\frac{t^2}{n} R(t/\\sqrt{n})\\Big)^n \\, .\n\\] As \\(n\\to \\infty\\), \\[\n\\lim_n \\Big(1 - \\frac{t^2}{2n} + \\frac{t^2}{n} R(t/\\sqrt{n})\\Big)^n  = \\mathrm{e}^{- \\frac{t^2}{2}} \\, .\n\\]\nOn the right-hand-side, we recognize the characteristic function of \\(\\mathcal{N}(0,1)\\).\n\\(\\square\\)\n\n\nRemark 11.5. The conditions in the Theorem statement allows for a short proof. They are by no mean necessary. The summands need not be identically distributed. The summands need not be independent. A version of the Lindeberg-Feller Theorem states that under mild assumptions, centered and normalized sums of independent square-integrable random variables converge in distribution towards a Gaussian distribution.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-cramerwolddevice",
    "href": "08-convergence-3.html#sec-cramerwolddevice",
    "title": "11  Convergence in distribution",
    "section": "11.9 Cramer-Wold device",
    "text": "11.9 Cramer-Wold device\nSo far, we have discussed characteristic functions for real valued random variables. But characteristic functions can be defined for vector-valued random variables. If \\(X\\) is a \\(\\mathbb{R}^k\\)-valued random variable, its characteristic function maps \\(\\mathbb{R}^k\\) to \\(\\mathbb{C}\\): \\[\\begin{align*}\n\\mathbb{R}^k & \\to \\mathbb{C} \\\\\nt & \\mapsto \\mathbb{E}\\mathrm{e}^{i \\langle t, X\\rangle} \\, .\n\\end{align*}\\]\nThe importance of multivariate characteristic functions is reflected in the next device which proof is left to the reader. It consists in the adapting the proof of Theorem 8.6).\n\nTheorem 11.6 (Cramer-Wold) The distribution of a \\(\\mathbb{R}^k\\)-valued random vector \\(X = (X_1, \\ldots, X_k)^T\\) is completely determined by the collection of distributions of univariate random variables \\(\\langle t, X\\rangle =\\sum_{i=1}^n t_i X_i\\) where \\((t_1, \\ldots, t_n)^T\\) belongs to \\(\\mathbb{R}^n.\\)\n\nTheorem 12.1) provides a short path to the Multivariate Central Limit Theorem.\n\nTheorem 11.7 Let \\(X_1, \\ldots, X_n, \\ldots\\) be i.i.d. vector valued random variables with finite covariance \\(\\Gamma\\) and expectation \\(\\mu\\). Let \\(S_n = \\sum_{i=1}^n X_i\\). \\[\n\\sqrt{n} \\left(\\frac{S_n}{n} - \\mu \\right) \\rightsquigarrow \\mathcal{N}\\big(0, \\Gamma\\big) \\, . s\n\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-weakconvtransforms",
    "href": "08-convergence-3.html#sec-weakconvtransforms",
    "title": "11  Convergence in distribution",
    "section": "11.10 Weak convergence and transforms",
    "text": "11.10 Weak convergence and transforms\nIn Lesson Chapter 8, we introduced different characterizations of probability distributions: probability generating functions, Laplace transforms, Fourier transforms (characteristic functions), cumulative distribution functions, quantiles functions. Within their scope, all those transforms are convergence determining: if a sequence of probability distributions converges weakly, so does (pointwise) the corresponding sequence of transforms, at least at the continuity points of the limiting transform.\nIn the next two theorems, each random variable is assumed to live on some (implicit) probability space.\n\nA sequence of non-negative random variables \\((X_n)_n\\) converges in distribution towards the non negative random variable \\(X\\) iff the sequence of Laplace transforms converges pointwise towards the Laplace transform of the probability distribution of \\(X\\).\n\nThe proof parallels the derivation of Theorem 11.2).\nAs probability generating functions allows us to recover Laplace transforms, the next theorem is a special case of the statement concerning Laplace transforms.\n\nA sequence of integer-valued random variables \\((X_n)_n\\) converges in distribution towards the integer-valued random variable \\(X\\) iff the sequence of Laplace transforms converges pointwise towards the Laplace transform of the probability distribution of \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3.html#sec-rem-biblio-conv-3",
    "href": "08-convergence-3.html#sec-rem-biblio-conv-3",
    "title": "11  Convergence in distribution",
    "section": "11.11 Bibliographic remarks",
    "text": "11.11 Bibliographic remarks\nDudley (2002) discusses convergence in distributions in two chapters: the first one is dedicated to distributions on \\(\\mathbb{R}^d\\) and the central limit theorem; the second chapter addresses more general universes. In the first chapter, the central limit theorem is extended to triangular arrays that is to sequences of not necessarily identically distributed random variables (Lindeberg’s Theorem).\nDudley (2002) investigates convergence in distributions as convergence of laws on separable metric spaces, that is in a much broader context than we do in these notes. The reader will find there a complete proof of the Prokhorov-Le Cam Theorem and an in-depth discussion of its corollaries. In (Dudley, 2002), a great deal of effort is dedicated to the metrization of the weak convergence topology. The reader will also find in this book a full picture of almost sure representation arguments.\nThe proof of the Lévy Continuity Theorem given here is taken from (Pollard, 2002).\nUsing metrizations for weak convergence allows us to investigate rate of convergence in limit theorems. This goes back at least to the Berry-Esseen’s Theorem (1942). Quantitative approaches to weak convergence have acquired a new momentum with the popularization of Stein’s method. This methods is geared towards, but exclusively focused on, general yet quantitative versions of the Central Limit Theorem (Chen, Goldstein, & Shao, 2011) . A thorough yet readable introduction to Stein’s method is (Ross, 2011).\n\n\n\n\nChen, L. H. Y., Goldstein, L., & Shao, Q.-M. (2011). Normal approximation by Stein’s method (p. xii+405). Springer, Heidelberg. https://doi.org/10.1007/978-3-642-15007-4\n\n\nDudley, R. M. (2002). Real analysis and probability (Vol. 74, p. x+555). Cambridge: Cambridge University Press.\n\n\nPollard, D. (2002). A user’s guide to measure theoretic probability (Vol. 8, p. xiv+351). Cambridge University Press, Cambridge.\n\n\nRoss, N. (2011). Fundamentals of Stein’s method. Probab. Surv., 8, 210–293. https://doi.org/10.1214/11-PS182",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Convergence in distribution</span>"
    ]
  },
  {
    "objectID": "08-convergence-3b.html",
    "href": "08-convergence-3b.html",
    "title": "12  Refinments and extensions of thr Central Limit Theorem",
    "section": "",
    "text": "12.1 Motivation\nIn Section 12.4), we associate convergence in distribution with metrics. By metrizing convergence in distribution, we can quantify rates of convergence.\nIn Section 12.5) we turn a relatively recent approach to weak convergence: Stein’s method. This is best illustrated with variations on the Central Limit Theorem. Recall that a byproduct of the proof that characteristic functions identify probability distributions was Stein’s identity.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Refinments and extensions of thr Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "08-convergence-3b.html#sec-cltbis",
    "href": "08-convergence-3b.html#sec-cltbis",
    "title": "12  Refinments and extensions of thr Central Limit Theorem",
    "section": "12.2 Central limit theorem",
    "text": "12.2 Central limit theorem\n\nLet \\(X_1, \\ldots, X_n, \\ldots\\) be i.i.d. with finite variance \\(\\sigma^2\\) and expectation \\(\\mu\\). Let \\(S_n = \\sum_{i=1}^n X_i\\).\n\\[\n\\sqrt{n} \\left(\\frac{S_n}{n} - \\mu \\right) \\rightsquigarrow \\mathcal{N}\\big(0, \\sigma^2 \\big) \\, .\n\\]\n\n\nProof. TODO\n\\(\\square\\)\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: Pointwise convergence of cumulative distribution functions towards the Gaussian cumulative distribution functions of \\(\\mathcal{N}(0,1)\\) (plain line) illustrates the Central Limit Theorem. The dotted and the dashed lines represent the cumulative distribution function of \\((X_n -np)/\\sqrt{n(p*(1-p))}\\) where \\(X_n \\sim \\text{Binomial}(n,p)\\) for \\(p=.3\\) and \\(n=30 \\text(dotted), 100 \\text(dashed)\\).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Refinments and extensions of thr Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "08-convergence-3b.html#cramer-wold-device",
    "href": "08-convergence-3b.html#cramer-wold-device",
    "title": "12  Refinments and extensions of thr Central Limit Theorem",
    "section": "12.3 Cramer-Wold device",
    "text": "12.3 Cramer-Wold device\nThe next theorem is a corollary of ?thm-thmLevyInj).\n\nTheorem 12.1 (Cramer-Wold) The distribution of a \\(\\mathbb{R}^n\\)-valued random vector \\(X = (X_1, \\ldots, X_n)^T\\) is completely determined by the collection of distributions of univariate random variables \\(\\langle t, X\\rangle =\\sum_{i=1}^n t_i X_i\\) where \\((t_1, \\ldots, t_n)^T\\) belongs to \\(\\mathbb{R}^n.\\)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Refinments and extensions of thr Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "08-convergence-3b.html#sec-MetrisationWeakConv",
    "href": "08-convergence-3b.html#sec-MetrisationWeakConv",
    "title": "12  Refinments and extensions of thr Central Limit Theorem",
    "section": "12.4 Metrizations of weak convergence",
    "text": "12.4 Metrizations of weak convergence\nWhen we talk about laws on complete separable metric spaces, convergence in distribution can be defined in relation to a distance. In fact, several distances are possible and you can choose any of them according to your needs. The distances we are interested in are of the form \\[\n\\mathrm{d}_{\\mathcal{H}}(P, Q) = \\sup_{h \\in \\mathcal{H}}\n\\int h \\mathrm{d}P - \\int h \\mathrm{d}Q\n\\] where \\(\\mathcal{H}\\) is a well-chosen collection of measurable functions.\nThe Kolmogorov-Lévy metric is used implicitly in the formulation of Glivenko-Cantelli’s theorem, it is controlled in the inequality of Dvoretzky-Kiefer-Wolfowitz.\n\nDefinition 12.1 (Kolmogorov-Lévy distance) If \\(W\\) and \\(Z\\) are two real random variables, the Kolmogorov-Lévy distance between their distributions is defined by \\[\n\\mathrm{d}_K(\\mathcal{L}(W), \\mathcal{L}(Z)) =  \\sup_{x \\in \\mathbb{R}} \\left| \\mathbb{P}\\{ W \\leq x\\} - \\mathbb{P}\\{ Z \\leq x\\} \\right| \\, .\n\\]\n\nThe fact that the Kolmogorov-Lévy distance metrizes the weak convergence towards a limiting distribution with a continuous cumulative distribution function is checked with the argument used in the classical proofs of Glivenko-Cantelli’s theorem.\nIf \\(\\lim_n x_n x_n = x\\), the mass sequence \\(\\delta_{x_n}\\) converges weakly towards the mass \\(\\delta_x\\), however, if \\(x_n \\neq x\\), \\(\\mathrm{d}_K(\\delta_{x_n}, \\delta_x)=1\\).\nTo metrize weak convergence in general settings, we can use the Levy-Prokhorov distance:\n\\[\n\\inf \\Big\\{\\epsilon : \\epsilon &gt;0,   \\qquad  P(-\\infty, x-\\epsilon] -\\epsilon \\leq Q(-\\infty, x] \\leq P(-\\infty, x-\\epsilon] + \\epsilon\\Big\\} \\, .\n\\]\nThe Kolmogorov-Lévy distance represents a relevant relaxation of the total variation distance (which does not metrize convergence in distribution). The fact that the test functions which define the Kolmogorov-Lévy distance are not absolutely continuous does not make life any easier. That is why we often work with a distance defined by more user-friendly functions. This distance belongs to the family of distances known as Monge-Wasserstein distances, or transportation distance.\n\n\n12.4.1 Wasserstein distance\nLet \\(W\\) and \\(Z\\) be \\(\\mathbb{R}^k\\)-valued random variables with distributions \\(\\mathcal{L}(W)\\), \\[\n\\mathrm{d}_M(\\mathcal{L}(W), \\mathcal{L}(Z)) =  \\sup_{h: 1-\\text{Lipschitz}} \\left| \\mathbb{E} h(W) - \\mathbb{E} h(Z) \\right| \\, .\n\\]\n\n\nWhen we want to evaluate the distance with respect to an absolutely continuous distribution like the Gaussian distribution, the use of one metric over another is not very important.\n\nSi \\(P\\) et \\(Q\\) sont deux lois sur \\(\\mathbb{R}\\), et si \\(Q\\) possède une densité majorée par \\(C\\) vis-à-vis de la mesure de Lebesgue, alors\n\\[\n\\mathrm{d}_K(P,Q) \\leq \\sqrt{ 2 C \\mathrm{d}_{M}(P,Q)} \\, .\n\\]\n\n\nProof. Pour \\(x \\in \\mathbb{R}\\), on note \\(h_x = \\mathbb{I}_{(-\\infty,x]}\\) et pour \\(\\epsilon&gt;0\\), on définit \\(h_{x, \\epsilon}\\) par\n\\[ h_{x,\\epsilon}(w) =\n  \\begin{cases}\n  1 & \\text{si } x\\geq w\\\\\n  0 & \\text{si } w&gt; x + \\epsilon \\\\\n  1 - \\frac{w-x}{\\epsilon}  & \\text{si } x\\leq w \\leq x+ \\epsilon\n  \\end{cases}\n\\]\nOn note \\(q\\) la densité de \\(Q\\) par rapport à la mesure de Lebesgue. La fonction \\(h_{x,\\epsilon}\\) est \\(1/\\epsilon\\)-Lipschitz et elle approche \\(h_x\\). Si \\(W \\sim P\\) et \\(Z \\sim Q\\), \\[\\begin{align*}\n  \\mathbb{E} h_x(W) - \\mathbb{E} h_x(Z)\n  & = \\underbrace{\\mathbb{E} h_x(W) - \\mathbb{E} h_{x,\\epsilon} (Z)}_{\\textsf{(i)}}  +\n  \\underbrace{\\mathbb{E} h_{x, \\epsilon}(Z) - \\mathbb{E} h_x(Z)}_{\\textsf{(II)}} \\,.\n\\end{align*}\\] On majore ensuite (simplement) les deux expressions et . \\[\\begin{align*}\n\\textsf{(i)} & \\leq \\mathbb{E} h_{x,\\epsilon} (W) - \\mathbb{E} h_{x,\\epsilon} (Z)\\\\\n& \\leq \\frac{1}{\\epsilon} \\mathrm{d}_{M}(P,Q) \\,\n\\end{align*}\\] et \\[\\begin{align*}\n\\textsf{(ii)} & \\leq \\int_{x}^{x+\\epsilon}\\left(1 - \\frac{w-x}{\\epsilon} \\right) q(z)\\mathrm{d}z\\\\\n& \\leq C \\frac{\\epsilon}{2} \\, .\n\\end{align*}\\] En choisissant \\(\\epsilon = \\sqrt{2\\mathrm{d}_{M}(P,Q)/C}\\), et en optimisant le choix de \\(x\\), on obtient le résultat désiré.\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Refinments and extensions of thr Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "08-convergence-3b.html#sec-steinsmethod",
    "href": "08-convergence-3b.html#sec-steinsmethod",
    "title": "12  Refinments and extensions of thr Central Limit Theorem",
    "section": "12.5 Stein’s method",
    "text": "12.5 Stein’s method\nIn Stein’s approach, we are interested in the approximation of a target law (normal law, Fish, \\(\\chi^2_k\\), ) by the law of a random variable whose mode of manufacturing (sum of independent random variables, order statistics, logarithm of likelihood ratio,…). The target law is characterized by an identity. In the case of of normal law, it’s Stein’s identity.\n\n\n12.5.1 Stein’s Identity\nLet the real random variable \\(Z\\) be distributed according to \\(Q\\). if for every absolutely continuous function \\(f\\) such that \\(f'(Z)\\) is \\(Q\\)-integrable, we have \\[\\mathbb{E}[Z f(Z)] = \\mathbb{E}[f'(Z)]\\] then \\[Q = \\mathcal{N}(0,1) \\, .\\] The converse is true.\n\n\n\nProof. See Lemma 13.1 and ?lem-steinslemma-bis.\n\nThis identity leads us to define an operator \\(\\mathcal{A}\\) that acts on absolutely continuous functions: \\(\\mathcal{A} f(x) = f'(x) - x f(x)\\). This operator allows us to characterize \\(\\mathcal{N}(0,1)\\): \\[\nQ = \\mathcal{N}(0,1) \\Longleftrightarrow \\forall f \\text{a.c}\\quad 0 = \\int \\mathcal{A}f(z) Q(\\mathrm{d}z) \\, .\n\\]\nNow, for function \\(h \\in \\mathcal{H}\\), si on peut définir \\(f_h\\) tel que \\[\\mathcal{A} f_h(w) = h(w) - \\int h(z) \\phi(z) \\mathrm{d}z\\, ,\\] on a \\[\n\\mathbb{E}_Q h(W) - \\mathbb{E}_{\\mathcal{N}(0,1)} h(Z) = \\mathbb{E}_Q \\mathcal{A} f_h(W) \\, .\n\\] In order to upper bound \\(d_{\\mathcal{H}}(Q, \\mathcal{N}(0,1))\\), it suffices to upper bound the following collection of expectations: \\[\n\\mathbb{E}_Q \\mathcal{A} f_h(W) \\qquad\\text{for} h \\in \\mathcal{H}, \\text{ and } \\mathcal{A} f_h(w) = h(w) - \\int h(z) \\phi(z) \\mathrm{d}z \\, .\n\\] This last goal led to the development of creative methods for approximating integrals. In order to understand what is at stake, we have to investigate the smoothness and boundedness properties of functions \\(\\mathcal{A} f_h, h \\in \\mathcal{H}\\), when \\(\\mathcal{H}\\) itself is defined by smoothness constraints.\nIn the sequel, we abbreviate \\(\\int_{\\mathbb{R}}  h(z) \\phi(z)\\mathrm{d}z\\) to \\(\\Phi(h)\\).\n\nLemma 12.1 Let \\(f_h\\) be the solution of the differential equation \\[\nf'_h(w) - w f_h(w) =  h(w) - \\Phi(h)\\, .\n\\] given by \\[\nf_h(w) = \\begin{cases}\n\\mathrm{e}^{w^2/2} \\int_w^\\infty \\mathrm{e}^{-t^2/2}(\\Phi(h)-h(t)) \\mathrm{d}t & \\text{if } w \\geq 0 \\\\\n\\mathrm{e}^{w^2/2} \\int^w_{-\\infty} \\mathrm{e}^{- t^2/2} \\left(h(t) -\\Phi(h)\\right) \\mathrm{d}t & \\text{if } w \\leq 0  \\,.\n\\end{cases}\n\\] If \\(h\\) is absolutely continuous then \\(f_h\\) is the only bounded solution of the differential equation and \\[\n\\|f_h\\|_\\infty \\leq 2 \\|h'\\|_\\infty,\\qquad \\|f'_h\\|_\\infty \\leq \\sqrt{\\frac{2}{\\pi}}\\|h'\\|_\\infty,\\qquad\\|f^{\\prime\\prime}_h\\|_\\infty \\leq 2\\|h'\\|_\\infty \\, .\n\\]\n\n\nProof. We first check that \\(f_h\\) is the unique bounded solution of the differential equation,\nNote that adding a constant to \\(h\\) leaves \\(h - \\Phi(h)\\) invariant. So we assume \\(h(0)=0\\). We have \\(|h(w)| \\leq \\|h'\\|_{\\infty} |w|\\).\nFor \\(w&gt;0\\), observe \\[\n\\mathrm{e}^{w^2/2} \\int_w^{\\infty} t \\, \\mathrm{e}^{-t^2/2}  \\mathrm{d}t\n= 1 \\qquad\n\\Phi(|h|) \\leq \\| h' \\|_\\infty \\sqrt{\\frac{2}{\\pi}}\n\\, .\n\\] Function \\(w \\mapsto \\mathrm{e}^{w^2/2}\\) satisfies the differential equation. Now look for a bounded solution like \\[\nf(w) = g(w) \\mathrm{e}^{w^2/2} \\, .\n\\] The differential equation reads \\[\ng'(w) = \\mathrm{e}^{- w^2/2} \\left(h(w) - \\Phi(h)\\right) \\, .\n\\] A solution is provied by function \\[\ng(w) =  \\begin{cases}\n\\int_w^\\infty \\mathrm{e}^{- t^2/2} \\left(\\Phi(h) -h(t)\\right) \\mathrm{d}t & \\text{if } w\\geq 0 \\\\\n\\int^w_{-\\infty} \\mathrm{e}^{- t^2/2} \\left(h(t) -\\Phi(h)\\right) \\mathrm{d}t & \\text{if } w\\leq 0 \\, .\n\\end{cases}\n\\] (integrability assumptions about \\(h\\) warrant that \\(g\\) is well-defined), hence \\(f_h\\) is a solution of the differential equation stated in Lemma Lemma 12.1\nBesides, for \\(w&gt;0\\) \\[\nf_h(w) = g(w) \\mathrm{e}^{w^2/2} \\leq \\| h'\\|_\\infty \\left(\\sqrt{\\frac{2}{\\pi}}\\frac{\\overline{\\Phi}(w)}{\\phi(w)} +1 \\right) \\, ,\n\\] the ratio \\(\\frac{\\overline{\\Phi}(w)}{\\phi(w)}\\) is non-increasing over \\([0, \\infty)\\), it is always smaller than \\(\\sqrt{\\frac{\\pi}{2}}\\). The boundedness of \\(f_h\\) follows: \\[\n\\| f_h \\|_\\infty \\leq 2   \\|h'\\|_\\infty  \\, .\n\\] The other solutions of the differential equation are \\(f_h + c \\mathrm{e}^{w^2/2}\\). They are not bounded.\nLes us now bound the first and second derivatives of \\(f_h\\).\nWe first check \\[\\begin{align*}\nh(w) - \\Phi(h)  & = \\int_{-\\infty}^w h'(t) \\Phi(t) \\mathrm{d}t - \\int_w^\\infty h'(t) \\overline{\\Phi}(t) \\mathrm{d}t \\,.\n\\end{align*}\\] Plugging into the definition de \\(f_h\\), we get : \\[\\begin{align*}\nf_h(w) & =  - \\sqrt{2\\pi} \\mathrm{e}^{w^2/2} (1-\\Phi(w)) \\int_{-\\infty}^w h'(t) \\Phi(t) \\mathrm{d}t \\\\\n& \\phantom{=} - \\sqrt{2\\pi} \\mathrm{e}^{w^2/2} \\Phi(w) \\int_w^{\\infty}  h'(t) (1-\\Phi(t)) \\mathrm{d}t\n\\end{align*}\\]\n\\[\\begin{align*}\nf'_h(w)\n& = w f_h(w)  + h(w) -\\Phi(h) \\\\\n& = (1 - \\sqrt{2\\pi} \\mathrm{e}^{w^2/2} (1-\\Phi(w)))\\int_{-\\infty}^w h'(t) \\Phi(t) \\mathrm{d}t \\\\\n& \\phantom{=} - (1 + \\sqrt{2\\pi} \\mathrm{e}^{w^2/2} \\Phi(w)) \\int_w^{\\infty}  h'(t) (1-\\Phi(t)) \\mathrm{d}t\\\\\n\\end{align*}\\] hence \\[\\begin{align*}\n|f'_h(w)|\n& \\leq \\|h'\\|_\\infty \\sup_{w \\in \\mathbb{R}} \\Big(|1 - \\sqrt{2\\pi} \\mathrm{e}^{w^2/2} (1-\\Phi(w))|\\int_{\\infty}^w \\Phi(t) \\mathrm{d}t \\Big. \\\\\n& \\phantom{\\leq \\|h'\\|_\\infty \\sup_{w \\in \\mathbb{R}} \\Big( \\Big.)} \\Big. |1 + \\sqrt{2\\pi} \\mathrm{e}^{w^2/2} \\Phi(w)| \\int_w^{\\infty}   (1-\\Phi(t))\\Big)\n\\end{align*}\\] To complete the upper bound on \\(\\|f'_h\\|_\\infty\\), it suffices to bound the supremum as \\(w\\) varies.\nTo establish existence and boundedness of the second derivative, we differentiate \\(f'_h(w)  = w f_h(w)  + h(w) -\\Phi(h)\\) to get \\[\\begin{align*}\nf^{\\prime\\prime}_h(w)\n& = f_h(w) + w f'_h(w) + h'(w) \\\\\n& = (1+ w^2)f_h(w) + ww (h(w) - \\Phi(h)) + h'(w) \\, .\n\\end{align*}\\] On réutilise les calculs esquissés précédemment pour majorer \\(\\|f^{\\prime\\prime}_h\\|_\\infty\\).\n\\(\\square\\)\n\nLe fait que si \\(h\\) est \\(1\\)-Lipschitz, \\(f_h\\) soit deux fois dérivable et de dérivée seconde bounded is convenient lorsqu’il faut majorer \\(\\mathbb{E} \\mathcal{A}f_h(W)\\). La façon dont on peut tirer avantage de cette régularité dépend de ce qu’on sait sur la structure de \\(W\\). Le theorem suivant qui peut être vu comme une combinaison des theorems de Berry-Esseen et de Lindeberg-Feller, montre ce qui peut etre obtenu lorsque \\(W\\) est une somme de variables aléatoires indépendantes pas nécessairement identique distribuées, centrées et suffisamment intégrables.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Refinments and extensions of thr Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "08-convergence-3b.html#sec-secBerryLindeberg",
    "href": "08-convergence-3b.html#sec-secBerryLindeberg",
    "title": "12  Refinments and extensions of thr Central Limit Theorem",
    "section": "12.6 A Berry-Esseen bound for a Lindeberg-Feller-like Central Limit Theorem",
    "text": "12.6 A Berry-Esseen bound for a Lindeberg-Feller-like Central Limit Theorem\nLe theorem suivant peut etre lu comme un résultat intermédiaire entre le theorem de Berry-Esseen ?thm-tcl-berry (il donne une distance à la loi normale) et le theorem central limite de Lindeberg-Feller ?thm-tcl-lindeberg (il traite de sommes de variables aléatoires indépendantes mais pas nécessairement identiquement distribuées). Les conditions utilisées ici sont plus fortes que celles décrites dans les deux theorems précédents. Dans le theorem de Lindeberg-Feller, la condition d’intégrabilité uniforme est minimale. Dans l’énoncé du theorem de Berry-Esseen, on se contente de postuler que les summands have a finite third moment. Ici on suppose un peu plus: les summands ont un kurtosis bounded.\nRappelons que le kurtosis d’une loi est défini par \\[\n\\frac{\\mathbb{E}[(X -\\mathbb{E}X)^4]}{(\\mathbb{E}[(X -\\mathbb{E}X)^2])^2} \\, .\n\\]\nThe kurtosis of Gausian est toujours égal à \\(3\\). Pour les lois Gamma, le kurtosis ne dépend que du paramètre de forme \\(p\\) (il vaut \\(3+ 6/p\\)).\n\nSoient \\(X_1, \\ldots, X_n\\) des variables aléatoires indépendantes centrées, de kurtosis majoré par \\(\\kappa\\). On définit \\(\\sigma^2 = \\sum_{i=1}^n  \\operatorname{var}\\left(X_i\\right)\\) et \\(W = \\sum_{i=1}^n X_i / \\sigma\\). \\[\n\\mathrm{d}_M(\\mathcal{L}(W), \\mathcal{N}(0, 1))\n\\leq \\sqrt{\\frac{2}{\\pi}}\n\\left(\\frac{1}{\\sigma^3} \\sum_{i=1}^n \\mathbb{E}|X_i|^3\n+ \\sqrt{\\kappa \\sum_{i=1}^n \\frac{\\operatorname{var}(X_i)^2}{\\left(\\sum_{i=1}^n \\operatorname{var}(X_i)\\right)^2}} \\right) \\, .\n\\]\n\n\nProof (Proof of ?thm-lindeberg-feller-stein). Dans la preuve \\(h\\) est une fonction \\(1\\)-Lipschitzienne, \\(f\\) la solution bounded \\(\\mathcal{A} f = h - \\Phi(h)\\). Pour majorer \\(\\mathbb{E} h(W) - \\Phi(h)\\), on va majorer \\(|\\mathbb{E} [f'_h(W) -W f(W)]|\\).\nDans la suite, pour \\(i \\in 1, \\ldots, n\\), \\(W_i = \\sum_{j\\neq i} X_j/\\sigma\\).\nObserve first that as \\(W_i\\) and \\(X_i\\) are independent and centered \\[\n\\mathbb{E}[X_i f(W_i)]  = 0 \\, .\n\\]\n\\[\\begin{align*}\n  \\mathbb{E}[W f(W)] & =  \\mathbb{E}\\left[ \\frac{1}{\\sigma}\\sum_{i=1}^n X_i f(W)\\right] \\\\\n  & =   \\mathbb{E}\\left[ \\frac{1}{\\sigma}\\sum_{i=1}^n (X_i f(W) - X_i f(W_i))\\right] \\\\\n  & =  \\mathbb{E}\\left[ \\frac{1}{\\sigma}\\sum_{i=1}^n X_i (f(W)- f(W_i))\\right] \\, .\n\\end{align*}\\] In the sequel, we will rely on \\[\\begin{align*}\n  \\mathbb{E}[W f(W)] & = \\mathbb{E}\\left[ \\frac{1}{\\sigma}\\sum_{i=1}^n X_i (f(W)- f(W_i) - (W- W_i) f'(W)) \\right] \\\\\n  & \\phantom{====} + \\mathbb{E}\\left[ \\frac{1}{\\sigma}\\sum_{i=1}^n X_i ((W- W_i) f'(W)) \\right] \\, .\n\\end{align*}\\]\n\\[\\begin{align*}\n\\left| \\mathbb{E}[f'(W) - W f(W)]\\right| & \\leq\n\\underbrace{\\left| \\mathbb{E}\\left[ \\frac{1}{\\sigma}\\sum_{i=1}^n X_i (f(W)- f(W_i) - (W- W_i) f'(W)) \\right] \\right|}_{\\textsf{(i)}}\\\\\n& \\phantom{====} +\n\\underbrace{\\left|\\mathbb{E}\\left[ \\left( 1-\\frac{1}{\\sigma}\\sum_{i=1}^n X_i (W- W_i)\\right) f'(W) \\right] \\right|}_{\\textsf{(i)}} \\, .\n\\end{align*}\\]\nTo upper bound , l’inégalité des accroissements finis suffit \\[\\begin{align*}\n\\textsf{(i)} &  \\leq   \\mathbb{E}\\left[ \\frac{1}{\\sigma}\\sum_{i=1}^n |X_i|  \\left|f(W)- f(W_i) - (W- W_i) f'(W)\\right| \\right]  \\\\\n& \\leq \\frac{1}{\\sigma}\\sum_{i=1}^n\\mathbb{E}\\left[ |X_i|\n  \\frac{|X_i|^2}{\\sigma^2} \\| f^{\\prime\\prime}\\|_\\infty \\right]  \\\\\n& \\leq {\\| f^{\\prime\\prime}\\|_\\infty}\\frac{\\sum_{i=1}^n\\mathbb{E} |X_i|^3  }{\\sigma^3} \\, .\n\\end{align*}\\] Pour majorer , on utilise Cauchy-Schwarz et l’hypothèse de kurtosis. \\[\\begin{align*}\n\\textsf{(ii)} & \\leq  \\mathbb{E}\\left[ \\left|\\left( 1-\\frac{1}{\\sigma^2}\\sum_{i=1}^n X^2_i \\right)\\right| \\left|f'(W)\\right| \\right] \\\\\n& \\leq \\|f'\\|_\\infty \\mathbb{E}\\left[ \\left|\\left( 1-\\frac{1}{\\sigma^2}\\sum_{i=1}^n X^2_i \\right)\\right|\\right] \\\\\n& \\leq \\|f'\\|_\\infty \\frac{1}{\\sigma^2} \\left(\\sum_{i=1}^n \\operatorname{var}(X_i^2)\\right)^{1/2} \\\\\n& \\leq \\|f'\\|_\\infty \\frac{1}{\\sigma^2} \\left(\\kappa \\sum_{i=1}^n \\operatorname{var}(X_i)^2\\right)^{1/2}  \\\\\n& \\leq \\|f'\\|_\\infty  \\left(\\kappa \\sum_{i=1}^n \\left( \\frac{\\operatorname{var}(X_i)}{\\sum_{i=1}^n \\operatorname{var}(X_i)} \\right)^2\\right)^{1/2}\n\\end{align*}\\]\n\\(\\square\\)\n\nEn utilisant le fait que pour \\(X\\) centrée, de kurtosis \\(\\kappa\\) \\[\n\\mathbb{E} |X_i|^3 \\leq (\\mathbb{E} |X_i|^4)^{3/4} \\leq \\kappa^{3/4}\n(\\mathbb{E} |X_i|^2)^{3/2} \\, ,\n\\]\nle membre droit du majorant peut se majorer lui même en \\[\n\\kappa^{3/4} \\|f^{\\prime\\prime}\\|_\\infty\\left(\\sum_{i=1}^n \\left(\\frac{\\operatorname{var}(X_i)}{\\sum_{j=1}^n \\operatorname{var}(X_j)}\\right)^{3/2}\\right) +\n\\kappa^{1/2} \\|f'\\|_\\infty\n\\left( \\sum_{i=1}^n \\left( \\frac{\\operatorname{var}(X_i)}{\\sum_{i=1}^n \\operatorname{var}(X_i)} \\right)^2\\right)^{1/2}\n\\]\nSi les \\(X_i\\) sont identiquement distribuées, alors le majorant du theorem s’écrit \\[\n\\sqrt{\\frac{2}{\\pi}} \\frac{1}{\\sqrt{n}} \\left( \\kappa^{3/4} + \\kappa^{1/2} \\right)  \\, .\n\\]\n\n\n\n\n\n\n\n\n\nFigure 12.2: Kolmogorov distance between standard Gaussian and distribution of \\((X_n - np)/\\sqrt{np(1-p}\\) with \\(X_n\\) binomially distributed with parameters \\(n\\) and \\(p=.3\\). The rate of convergence in the central limit theorem is asymptotically of order \\(1/\\sqrt{n}\\).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Refinments and extensions of thr Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "08-convergence-3b.html#bibliographic-remarks",
    "href": "08-convergence-3b.html#bibliographic-remarks",
    "title": "12  Refinments and extensions of thr Central Limit Theorem",
    "section": "12.7 Bibliographic remarks",
    "text": "12.7 Bibliographic remarks\nLa possibilité de métriser la convergence en distribution (et la convergence en probabilité) est traitée avec beaucoup de rigueur et de clarté dans (Dudley, 2002).\nStein’s method pour établir des versions précises et générales du theorem central limite est décrite dans (Ross, 2011).\nA thorough yet readable treatment of Stein’s method is (Chen, Goldstein, & Shao, 2011).\n[Section ]) follows the first pages of (Ross, 2011)\nLa démonstration complète du ?lem-regu-stein} se trouve dans (Chen et al., 2011).\n\n\n\n\nChen, L. H. Y., Goldstein, L., & Shao, Q.-M. (2011). Normal approximation by Stein’s method (p. xii+405). Springer, Heidelberg. https://doi.org/10.1007/978-3-642-15007-4\n\n\nDudley, R. M. (2002). Real analysis and probability (Vol. 74, p. x+555). Cambridge: Cambridge University Press.\n\n\nRoss, N. (2011). Fundamentals of Stein’s method. ArXiv e-Prints. Retrieved from https://arxiv.org/abs/1109.1880",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Refinments and extensions of thr Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html",
    "href": "09-gaussian-vectors.html",
    "title": "13  Gaussian vectors",
    "section": "",
    "text": "13.1 Univariate Gaussian distribution\nThe standard Gaussian density is denoted by \\(\\phi\\): \\[\n\\phi(x) = \\frac{\\mathrm{e}^{- \\frac{x^2}{2} }}{\\sqrt{2 \\pi}} \\, .\n\\] The corresponding cumulative distribution function is denoted by \\(\\Phi\\). The survival function \\(1- \\Phi\\) is denoted by \\(\\overline{\\Phi}\\). \\[\n\\Phi(x) =  \\int_{- \\infty}^x \\phi(u) \\mathrm{d}u \\, .\n\\] Let is denote by \\(\\mathcal{N} (0, 1)\\) (expectation \\(0\\), variance \\(1\\)) the standard Gaussian probability distribution, that is the probability distribution defined by \\(\\phi\\).\nAny affine transform of a standard Gaussian random variable is distributed according to a univariate Gaussian distribution. If \\(X \\sim \\mathcal{N} (0, 1)\\) then \\(\\sigma X + \\mu \\sim\n\\mathcal{N} \\left( \\mu, \\sigma^2 \\right)\\) with density \\(\\frac{1}{\\sigma}\\phi\\left(\\frac{\\cdot- \\mu}{\\sigma}\\right)\\), cumulative distribution function \\(\\Phi\\left(\\frac{\\cdot - \\mu}{\\sigma}\\right)\\).\nFigure 13.1\nThe standard Gaussian distribution is characterized by the next identity.\nThe characteristic function is a very efficient tool when handling Gaussian distributions.\nThe fact that the characteristic function completely defines the probability distribution provides us with a converse of Lemma 13.1.\nIt is now easy to check that the distribution of the sum of two independent Gaussian random variables is a Gaussian random variable.\nThe moment generating function of a Gaussian random variable is given by \\[\ns \\mapsto \\mathbb{E} \\left[ \\mathrm{e}^{s X} \\right] =\n\\text{e}^{\\frac{s^2}{2}} \\,  .\n\\]\nFrom Markov’s inequality, we obtain interesting upper bounds on the Gaussian tail function. Some calculus allows us to refine the tail bounds\nNote that \\((2k)!/(2^k k!)\\) is also the number of partitions of \\(\\{1, \\ldots, 2k\\}\\) into subsets of cardinality \\(2\\).\nThe skewness is null, the kurtosis (ratio of fourth centred moment over squared variance equals \\(3\\):\n\\[\n\\mathbb{E}[X^4] =  3 \\times  \\mathbb{E}[X^2]^2 \\, .\n\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#sec-secGaussUnivar",
    "href": "09-gaussian-vectors.html#sec-secGaussUnivar",
    "title": "13  Gaussian vectors",
    "section": "",
    "text": "Lemma 13.1 (Stein’s Lemma) Let \\(X \\sim \\mathcal{N}(0,1)\\), let \\(g\\) be an absolutely continuous function with derivative \\(g'\\) such that \\(\\mathbb{E}[ |X g(X)|]&lt;\\infty\\), then \\(g'(X)\\) is integrable and\n\\[\n\\mathbb{E}[g'(X)] = \\mathbb{E}[Xg(X)] \\, .\n\\]\n\n\nProof. The proof relies on integration by parts. First note that replacing \\(g\\) by \\(g - g(0)\\) changes neither \\(g'\\), nor \\(\\mathbb{E}[Xg(X)]\\). We may assume that \\(g(0)=0\\).\n\\[\\begin{align*}\n\\mathbb{E}[Xg(X)] & = \\int_{\\mathbb{R}} xg(x) \\phi(x) \\mathrm{d}x \\\\\n& = \\int_0^\\infty xg(x) \\phi(x) \\mathrm{d}x + \\int_{-\\infty}^0 xg(x) \\phi(x) \\mathrm{d}x \\\\\n& = \\int_0^\\infty x \\int_0^\\infty g'(y) \\mathbb{I}_{y\\leq x}\\mathrm{d}y \\phi(x) \\mathrm{d}x -\\int^0_{-\\infty} x \\int^0_{-\\infty} g'(y) \\mathbb{I}_{y\\geq x}\\mathrm{d}y \\phi(x) \\mathrm{d}x\\\\\n& = \\int_0^\\infty g'(y) \\int_0^\\infty  \\mathbb{I}_{y\\leq x} x\\phi(x)\\mathrm{d}x  \\mathrm{d}y -\n\\int_{-\\infty}^0 g'(y) \\int^0_{-\\infty}  x \\phi(x)\\mathbb{I}_{y\\geq x}\\mathrm{d}x \\mathrm{d}y  \\\\\n& = \\int_0^\\infty g'(y) \\int_y^\\infty x\\phi(x)\\mathrm{d}x  \\mathrm{d}y -\n\\int_{-\\infty}^0 g'(y) \\int^y_{-\\infty}  x \\phi(x)\\mathrm{d}x \\mathrm{d}y  \\\\\n& = \\int_0^\\infty g'(y) \\phi(y) \\mathrm{d}y -\n\\int_{-\\infty}^0 - g'(y) \\phi(y)\\mathrm{d}y \\\\\n& = \\int_{-\\infty}^\\infty g'(y) \\phi(y) \\mathrm{d}y \\, .\n\\end{align*}\\]\nThe last inequality is justified by Tonelli-Fubini’s Theorem. Then, we rely on \\(\\phi'(x)=-x \\phi(x)\\).\n\n\n\nProposition 13.1 The characteristic function of \\(\\mathcal{N}(\\mu,\\sigma^2)\\) is \\[\n\\widehat{\\Phi}(t):= \\mathbb{E}\\left[\\mathrm{e}^{\\imath t X}\\right] = \\mathrm{e}^{\\imath t \\mu - \\frac{t^2 \\sigma^2}{2}}  \\, .\n\\]\n\n\nProof. It is enough to check the proposition for \\(\\mathcal{N}(0,1)\\). As \\(\\phi\\) is even,\n\\[\\begin{eqnarray*}\n\\widehat{\\Phi}(t) &= & \\int_{-\\infty}^{\\infty} \\mathrm{e}^{\\imath t x} \\frac{\\mathrm{e}^{- \\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\mathrm{d} x \\\\\n& = & \\int_{-\\infty}^{\\infty} \\cos(tx) \\frac{\\mathrm{e}^{- \\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\mathrm{d} x \\, .\n\\end{eqnarray*}\\]\nDerivation with respect to \\(t\\), interchanging derivation and expectation (why can we do that?)\n\\[\\begin{eqnarray*}\n\\widehat{\\Phi}'(t)  & = &    \\int_{-\\infty}^{\\infty} -x \\sin(tx) \\frac{\\mathrm{e}^{- \\frac{x^2}{2}}}{\\sqrt{2 \\pi}} \\mathrm{d} x    \\, .\n\\end{eqnarray*}\\] Now relying on Stein’s Identity with \\(g(x)=-\\sin(tx)\\) and \\(g'(x)=-t\\cos(tx)\\) \\[\\begin{eqnarray*}\n\\widehat{\\Phi}'(t)  & = &\n- t \\int_{-\\infty}^{\\infty} \\cos(tx) \\phi(x) \\mathrm{d} x \\\\\n& = & -t \\widehat{\\Phi}(t) \\, .\n\\end{eqnarray*}\\]\nWe immediately get \\(\\widehat{\\Phi}(0)=1\\), and solving the differential equation leads to \\[\n\\log \\widehat{\\Phi}(t) =  - \\frac{t^2}{2} \\, .\n\\]\n\n\n\nLemma 13.2 (Stein’s Lemma (bis)) Let \\(X\\) be a real-valued random variable on some probability space. If, for any differentialle function \\(g\\) such that \\(g'\\) and \\(x \\mapsto xg(x)\\) are integrable, the following holds \\[\n\\mathbb{E}[g'(X)] = \\mathbb{E}[X g(X)]\n\\] then the distribution of \\(X\\) is standard Gaussian\n\n\nProof. Consider the real \\(\\widehat{F}\\) and the imaginary part \\(\\widehat{G}\\) of the characteristic function of the distribution pf \\(X\\), the identity entails that \\(\\widehat{F}'(t) = -t \\widehat{F}(t)\\) and \\(\\widehat{G}'(t) = -t \\widehat{G}(t)\\) with \\(\\widehat{F}(0)=1\\) and \\(\\widehat{G}(0)=0\\). Solving the two differential equations leads to \\(\\widehat{F}(t) = \\mathrm{e}^{-t^2/2}\\) and \\(\\widehat{G}(t)=0\\). We just checked that the characteristic function of the distribution of \\(X\\) is the characteristic function of \\(\\mathcal{N}(0,1).\\)\n\n\n\nIf \\(X\\) and \\(Y\\) are two independent random variables distributed according to \\(\\mathcal{N} (\\mu, \\sigma^2)\\) and \\(\\mathcal{N}\n(\\mu', \\sigma^{\\prime 2})\\) then \\(X + Y\\) is distributed according to \\(\\text{$\\mathcal{N} (\\mu + \\mu', \\sigma^2 +\n\\sigma^{\\prime 2})$}\\).\n\n\nCheck and justify.\n\n\n\n\nProposition 13.2 (Tail probabilities for Gaussian distribution) For \\(x \\geq 0,\\) \\[\n\\frac{\\phi(x)}{x} \\left( 1\n- \\frac{1}{x^2} \\right)  \\leq \\overline{\\Phi} (x) \\leq \\min \\left( \\mathrm{e}^{-\\frac{x^2}{2}},\n\\frac{\\phi(x)}{x} \\right)\\, .\n\\]\n\n\nProof. The proof boils down to repeated integration by parts.\n\\[\\begin{eqnarray*}\n\\overline{\\Phi}(x) & = & \\int_x^{\\infty}  \\frac{1}{ \\sqrt{2\n\\pi}} \\mathrm{e}^{- \\frac{u^2}{2}} \\mathrm{d} u\\\\\n& = & \\left[ -  \\frac{1}{ \\sqrt{2 \\pi} u} \\mathrm{e}^{-\n\\frac{u^2}{2}} \\right]^{\\infty}_x - \\int_x^{\\infty}  \\frac{1}{\n\\sqrt{2 \\pi}}  \\frac{1}{u^2} \\mathrm{e}^{- \\frac{u^2}{2}} \\mathrm{d} u .\n\\end{eqnarray*}\\]\nAs the second term is non-positive,\n\\[\n\\overline{\\Phi}(x)\\leq  \\left[ -  \\frac{1}{ \\sqrt{2\n\\pi} u} \\mathrm{e}^{- \\frac{u^2}{2}} \\right]^{\\infty}_x =  \\frac{\\phi(x)}{\nx}  .\n\\]\nThis is the first part of the right-hand inequality, the other part comes from Markov’s inequality. For the left-hand inequality, we have to upper bound \\(\\int_x^{\\infty}  \\frac{1}{ \\sqrt{2 \\pi}}  \\frac{1}{u^2} \\mathrm{e}^{- \\frac{u^2}{2}} \\mathrm{d} u\\).\n\\[\\begin{eqnarray*}\n\\int_x^{\\infty}  \\frac{1}{ \\sqrt{2 \\pi}}  \\frac{1}{u^2} \\mathrm{e}^{-\n\\frac{u^2}{2}} \\mathrm{d} u & = & \\left[ \\frac{- 1}{ \\sqrt{2 \\pi}}\n\\frac{1}{u^3} \\mathrm{e}^{- \\frac{u^2}{2}} \\right]_x^{\\infty} -\n\\int_x^{\\infty}  \\frac{1}{ \\sqrt{2 \\pi}}  \\frac{3}{u^4} \\mathrm{e}^{-\n\\frac{u^2}{2}} \\mathrm{d} u\\\\\n& \\leq  & \\frac{1}{ \\sqrt{2 \\pi}}  \\frac{1}{x^3} \\mathrm{e}^{-\n\\frac{x^2}{2}} .\n\\end{eqnarray*}\\]\n\n\nProposition 13.3 (Moments) For a standard Gaussian random variable, \\[\n\\mathbb{E} \\left[ X^k \\right] =\n=\n\\begin{cases}\n0 &  \\text{ if } k \\text{ is odd}\\\\\n\\frac{k!}{2^{k / 2} (k / 2) !} = \\frac{\\Gamma (k + 1)}{2^{k / 2}\n\\Gamma (k / 2 + 1)} & \\text{ if }  k \\text{ is even.}\n\\end{cases}\n\\]\n\n\nProof. Thanks to distributional symmetry, \\(\\mathbb{E} \\left[ X^k \\right]=0\\) for all odd\\(k\\). We handle even powers using integration by parts:\n\\[\\begin{eqnarray*}\n\\mathbb{E} \\left[ X^{k+2} \\right] & = & (k+1) \\mathbb{E} \\left[ X^{k} \\right]  \\, .\n\\end{eqnarray*}\\]\nInduction on \\(k\\) leads to,\n\\[\\begin{eqnarray*}\n\\mathbb{E} \\left[ X^{2k} \\right] &= & \\prod_{j=1}^k (2j-1) =  \\frac{(2k) !}{2^k k! } \\, .\n\\end{eqnarray*}\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#sec-GaussVec",
    "href": "09-gaussian-vectors.html#sec-GaussVec",
    "title": "13  Gaussian vectors",
    "section": "13.2 Gaussian vectors",
    "text": "13.2 Gaussian vectors\nA Gaussian vector is a collection of univariate Gaussian random variables that satisfies a very stringent property:\n\nDefinition 13.1 (Gaussian Vector) A random vector \\((X_1, \\ldots, X_n)^T\\) is a Gaussian vector iff for any real vector \\((\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)^T\\), the distribution of the univariate random variable \\(\\sum_{i = 1}^n \\ \\lambda_i X_i\\) is Gaussian.\n\n\nNot every collection of Gaussian random variables forms a Gaussian vector.\nThe random vector \\((X, \\epsilon X)\\) with \\(X \\sim \\mathcal{N}(0.1)\\), independent of \\(\\epsilon\\) which is worth \\(\\pm 1\\) with probability \\(1/2\\), is not a Gaussian vector although both \\(X\\) and \\(\\epsilon X\\) are univariate Gaussian random variables.\n\n\nCheck that \\(\\epsilon X\\) is a Gaussian random variable.\n\nYet there are Gaussian vectors! A simple way to obtain a Gaussian vector is provided by the next proposition (checked by a characteristic function argument).\n\nIf \\(X_1, \\ldots, X_n\\) is a sequence of independent Gaussian random variables, then \\((X_1, \\ldots, X_n)^t\\) is a Gaussian vector.\n\nIn the sequel, a standard Gaussian vector is a random vector with independent coordinates with each coordinate distributed according to \\(\\mathcal{N}(0,1)\\).\nWe will see how to construct general Gaussian vectors. Before this, let us check that the joint distribution of a Gaussian random vector is completely characterized by its covariance matrix and its expectation vector.\nRecall that the covariance of random vector \\(X= (X_1, \\ldots, X_n)^T\\) is the matrix \\(K\\) with dimension \\(n \\times n\\) with coefficients\n\\[\nK [i, j] = \\operatorname{Cov} (X_i, X_j) = \\mathbb{E} [X_i X_j] - \\mathbb{E} [X_i] \\mathbb{E} [X_j] .\n\\]\nWithout loss of generality, we may assume that random vector \\(X\\) is centered For every \\(\\lambda = (\\lambda_1, \\ldots, \\lambda_n)^T \\in \\mathbb{R}^n\\), we have:\n\\[\n\\operatorname{var}(\\langle \\lambda, X \\rangle) =  \\lambda^t K \\lambda = \\text{trace} (K \\lambda \\lambda^t)\\,\n\\] (this is does not depend on any Gaussianity assumption).\nIndeed,\n\\[\\begin{eqnarray*}\n\\operatorname{var}(\\langle \\lambda, X \\rangle) & = &\n\\mathbb{E} \\left[ \\left( \\sum_{i=1}^n \\lambda_i X_i\\right)^2\\right]  \\\\\n& = & \\sum_{i,j=1}^n \\mathbb{E} \\left[\\lambda_i \\lambda_j X_i X_j \\right] \\\\\n& = & \\sum_{i,j=1}^n \\lambda_i \\lambda_j K[i,j] \\\\\n& = & \\lambda^t K \\lambda\\, .\n\\end{eqnarray*}\\]\nThe characteristic function of a Gaussian vector \\(X\\) with expectation vector \\(\\mu\\) and covariance \\(K\\) satisfies\n\\[\n\\mathbb{E} \\mathrm{e}^{\\imath \\langle \\lambda, X \\rangle } =  \\mathrm{e}^{\\imath \\langle \\lambda, \\mu \\rangle - \\frac{\\lambda^t K \\lambda}{2}} \\,  \\, .\n\\]\nA linear transform of a Gaussian vector is a Gaussian vector.\n\nIf \\({Y} = (Y_1, \\ldots, Y_n)^T\\) is a Gaussian vector with covariance \\(K\\) and \\(A\\) a real matrix with dimensions \\(p \\times n\\), then \\(A \\times Y\\) is Gaussian vector with expectation \\(A \\times \\mathbb{E}Y\\) and covariance matrix\n\\[A K A^T .\\]\n\n\nProof. Without loss of generality, we assume \\(Y\\) is centred.\nFor any \\(\\lambda \\in \\mathbb{R}^p\\), \\(\\langle \\lambda , A Y \\rangle = \\langle A^T \\lambda, Y \\rangle\\) , thus \\(A \\times Y\\) is Gaussian with variance\n\\[\n\\lambda^T A K A^T \\lambda \\, .\n\\]\nThe covariance of \\(A \\times Y\\) is determined by this observation.\n\nTo manufacture Gaussian vectors with general covariance matrices, we rely on an important notion from matrix analysis.\n\nDefinition 13.2 (Semi-Definite Positive matrices) A symmetric matrix \\(M\\) with dimensions \\(k \\times k\\) is Definite Positive (respectively Semi-Definite Positive) iff, for any non-null vector \\(v \\in \\mathbb{R}^k\\),\n\\[\nv^T M v &gt; 0 \\qquad (\\text{resp.} \\qquad v^T M v \\geq 0) \\, .\n\\]\nWe denote by \\(\\textsf{dp}(k)\\) (resp. \\(\\textsf{sdp}(k)\\)), the cones of Definite Positive (resp. Semi-Definite Positive) matrices.\n\n\nIf \\(K\\) is the covariance matrix of a random vector, \\(K\\) is symmetric, Semi-Definite Positive.\n\n\nProof. If \\(X\\) is a \\(\\mathbb{R}^k\\)-valued random vector, with covariance \\(K\\), for any vector \\(\\lambda \\in \\mathbb{R}^n\\),\n\\[\n\\lambda^T K \\lambda = \\sum_{i,j\\leq k} K_{i,j} \\lambda_i \\lambda_j = \\operatorname{cov}(\\langle \\lambda, X \\rangle, \\langle \\lambda, X \\rangle)\n\\] soit \\(\\lambda^T K \\lambda =  \\operatorname{var}(\\langle \\lambda, X \\rangle)\\). The variance of a univariate random variable is always non-negative.\n\nThe next observation is the key to the construction to general Gaussian vectors.\n\nProposition 13.4 (Cholesky’s factorization) If \\(A\\) is a Semi-definite Positive symmetric matrix then there exists (at least) a real matrix \\(B\\) such that \\(A = B^T B\\).\n\nWe do not check this proposition. This is a basic Theorem from matrix analysis. It can be established from the spectral decomposition theorem for symmetric matrices. It can also be established by a simple constructive approach: a positive definite matrix \\(K\\) admits a Cholesky decomposition, in other words, there exists a triangular matrix lower than \\(L\\) such that \\(K = L \\times L^T\\).\nThe next proposition is a corollary of the general formula for image densities.\n\nIf \\(A\\) is a symmetric positive definite matrix (\\(A \\in \\textsf{dp}(n)\\)), then the distribution of the centred Gaussian vector with covariance matrix \\(A\\) is absolutely continuous with respect to Lebesgue’s measure on \\(\\mathbb{R}^n\\): \\[\n\\frac{1}{({2 \\pi})^{n/2}  \\operatorname{det}(A)^{1/2}} \\exp\\left( - \\frac{x^t A^{-1} x}{2} \\right) \\, .\n\\]\n\n\nProof. The density formula is trivially correct for standard Gaussian vectors. For the general case, it is enough to invoke the image density formula to the image of the standard Gaussian vector by the bijective linear transformation defined by the Cholesky factorization of \\(A\\). The determinant of the Cholesky factor is the square root of the determinant of \\(A\\).\n\n\nIs the distribution of a Gaussian vector \\(X\\) with singular covariance matrix absolutely continuous with respect to Lebesgue measure?\n\n\nDefinition 13.3 (Gaussian space) If \\(X= (X_1, \\ldots, X_n)^T\\) is a centered Gaussian vector with covariance matrix \\(K\\) , the set \\(\\left\\{ \\sum_{i = 1}^n \\lambda_i X_i = \\langle \\lambda, X\\rangle ; \\lambda \\in \\mathbb{R}^n\\right\\}\\) is the Gaussian space generated by \\(X = (X_1, \\ldots, X_n)^T\\)).\n\nThe Gaussian space is a real vector space. If \\((\\Omega, \\mathcal{F},P)\\) denotes the probability space, \\(X\\) lives on, the Gaussian space is a subspace of \\(L^2_{\\mathbb{R}}(\\Omega, \\mathcal{F},P)\\). It inherits the inner product structure from \\(L^2_{\\mathbb{R}}(\\Omega, \\mathcal{F},P)\\).\nThis inner-product is completely defined by the covariance matrix \\(K\\).\n\\[\\begin{eqnarray*}\n\\left\\langle \\sum_{i = 1}^n \\lambda_i X_i, \\sum_{i = 1}^n \\lambda'_i X_i\n\\right\\rangle & \\equiv & \\mathbb{E}_P \\left[ \\left( \\sum_{i = 1}^n\n\\lambda_i X_i \\right) \\left( \\sum_{i = 1}^n \\lambda'_i X_i \\right) \\right]\\\\\n& = & \\sum^n_{i, i' = 1} \\lambda_i \\lambda_{i'}'  K [i, i']\\\\\n& = & (\\lambda_1, \\ldots, \\lambda_n) K \\left(\\begin{array}{c}\n\\lambda'_1\\\\\n\\vdots\\\\\n\\lambda'_n\n\\end{array}\\right) \\\\\n& = & \\text{trace} \\left( K \\left(\\begin{array}{c}\n\\lambda_1\\\\\n\\vdots\\\\\n\\lambda_n\n\\end{array}\\right) \\left(\\begin{array}{ccc}\n\\lambda'_1 &\n\\dots &\n\\lambda'_n\n\\end{array}\\right) \\right).\n\\end{eqnarray*}\\]\n\nDifferent Gaussian vectors may generate the same Gaussian space. Explain how and why.\n\nGaussian spaces enjoy remarkable properties. Independence of random variables belonging to the same Gaussian space may checked very easily.\n\nProposition 13.5 Two random variables \\(Z\\) and \\(Y\\), belonging to the same Gaussian space, are independent iff they are orthogonal (or decorrelated), that is iff \\[\n\\operatorname{Cov}_P [Y ,Z] = \\mathbb{E}_P [Y Z] = 0 .\n\\]\n\nWithout loss of generality, we assume covariance matrix \\(K\\) is positive definite.\n\nProof. Independence always implies orthogonality.\nWithout loss of generality, we assume that the Gaussian space is generated by a standard Gaussian vector, let \\(Z = \\sum_{i = 1}^n \\lambda_i X_i\\) and \\(Y = \\sum_{i = 1}^n \\lambda'_i X_i\\).\nIf \\(Z\\) and \\(Y\\) are orthogonal (or non-correlated)\n\\[\\mathbb{E} [ZY] =  \\sum_{i = 1}^n \\lambda_i \\lambda_{i}' = 0 .\\]\nTo show that \\(Z\\) and \\(Y\\) are independent, it is enough to check that for all \\(\\mu\\) and \\(\\mu'\\) in \\(\\mathbb{R}\\) \\[\\mathbb{E} \\left[ \\mathrm{e}^{\\imath \\mu Z} \\mathrm{e}^{\\imath \\mu' Y} \\right] =\n\\mathbb{E} \\left[ \\mathrm{e}^{\\imath \\mu Z}  \\right] \\times \\mathbb{E}\n\\left[ \\mathrm{e}^{\\imath \\mu' Y} \\right] .\\]\n\\[\\begin{align*}\n\\mathbb{E} \\left[ \\mathrm{e}^{\\imath \\mu Z} \\mathrm{e}^{\\imath \\mu' Y} \\right] &\n=  \\mathbb{E} \\left[ \\mathrm{e}^{\\imath \\mu \\sum_i \\lambda_i    X_i} \\mathrm{e}^{\\imath \\mu' \\sum_i \\lambda'_i    X_i} \\right]\\\\\n& =  \\mathbb{E} \\left[ \\prod_{i = 1}^n    \\mathrm{e}^{\\imath (\\mu\n\\lambda_i + \\mu' \\lambda'_i)    X_i} \\right]\\\\\n&   X_i \\text{ are  independent} \\ldots\\\\\n& =  \\prod_{i = 1}^n  \\mathbb{E} \\left[ \\mathrm{e}^{\\imath (\\mu \\lambda_i +\n\\mu' \\lambda'_i)    X_i} \\right]\\\\\n& =  \\prod_{i = 1}^n \\mathrm{e}^{- (\\mu \\lambda_i + \\mu' \\lambda'_i)\n^2 / 2}\\\\\n& =  \\exp \\left( - \\frac{1}{2} \\sum_{i = 1}^n \\mu^2 \\lambda_i^2 + 2 \\mu\n\\mu' \\lambda_i \\lambda'_i + \\mu'^2 \\lambda'^2_i    \\right)\\\\\n&   \\text{orthogonality}\\\\\n& =  \\exp \\left( - \\frac{1}{2} \\sum_{i = 1}^n \\mu^2 \\lambda_i^2 + \\mu'^2\n\\lambda'^2_i    \\right)\\\\\n&   \\ldots\\\\\n& =  \\mathbb{E} \\left[ \\mathrm{e}^{\\imath \\mu Z}  \\right] \\times\n\\mathbb{E} \\left[ \\mathrm{e}^{\\imath \\mu' Y} \\right] .\n\\end{align*}\\]\n\nThe next proposition is a direct consequence.\n\nIf \\(E\\) and \\(E'\\) are two linear sub-spaces of the Gaussian space generated by the Gaussian vector with independent coordinates \\(X_1, \\ldots, X_n\\), the (Gaussian) random variables belonging to subspace \\(E\\) and the random (Gaussian) variables belonging to the \\(E'\\) space are independent if and only these two subspaces are orthogonal.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#sec-convergenceGaussVectors",
    "href": "09-gaussian-vectors.html#sec-convergenceGaussVectors",
    "title": "13  Gaussian vectors",
    "section": "13.3 Convergence of Gaussian vectors",
    "text": "13.3 Convergence of Gaussian vectors\nRecall the Lévy continuity theorem (Theorem 11.2)), it relates weak convergence for probability measures and simple convergence for characteristic functions.\n\nA sequence of probability distributions \\((P_n)_{n \\in \\mathbb{N}}\\) over \\(\\mathbb{R}^k\\) converges weakly towards \\(P\\) iff for every \\(\\vec{s} \\in \\mathbb{R}^k\\):\n\\[\\mathbb{E}_{P_n} \\left[ \\mathrm{e}^{\\imath \\langle \\vec{s},\n\\vec{X} \\rangle} \\right] \\rightarrow \\mathbb{E}_{P_{}} \\left[\n\\mathrm{e}^{\\imath \\langle \\vec{s}, \\vec{X} \\rangle} \\right] .\\]\n\nFor every \\(\\vec{s} \\in \\mathbb{R}^k\\), functions \\(\\vec{x}\\mapsto \\cos ( \\left\\langle \\vec{s}, \\vec{x} \\right\\rangle)\\) and \\(\\vec{x} \\mapsto \\sin ( \\left\\langle \\vec{s}, \\vec{x}\\right\\rangle)\\) are continuous and bounded, They are also infinitely many times differentiable.\nIt is remarkable and useful that weak convergence can be checked on this small set of functions.\n\n\n13.3.1 Lévy-Continuity Theorem (bis)\nA sequence of probability distributions \\((P_n)_{n \\in \\mathbb{N}}\\) sur \\(\\mathbb{R}^k\\) converges weakly towards a probability distribution iff there exists a function \\(f\\) over \\(\\mathbb{R}^k\\), continuous at \\(\\vec{0}\\), such that for all \\(\\vec{s} \\in \\mathbb{R}^k\\):\n\\[\\mathbb{E}_{P_n} \\left[ \\mathrm{e}^{\\imath \\langle \\vec{s}, \\vec{X} \\rangle} \\right] \\rightarrow f(\\vec{s})\\,.\\]\nThen, function \\(f\\) is the characteristic function of some probability distribution \\(P\\).\n\n\nThe continuity condition at \\(0\\) is necessary. The characteristic function of a probability distribution is always continuous at \\(0\\). Continuity at \\(0\\) warrants the tightness of the sequence of probability distributions.\n\nIf a sequence of \\(k\\)-dimensional Gaussian vectors \\((X_n)\\) is defined by a \\(\\mathbb{R}^k\\)-valued sequence \\((\\vec{\\mu}_n)_n\\) and a \\(\\textsf{SDP}(k)\\)-valued sequence \\((K_n)_n\\) and\n\\[\\begin{eqnarray*}\n\\lim_n \\vec{\\mu}_n & = & \\mu \\in \\mathbb{R}^k\\\\\n\\lim_n K_n & = & K \\in \\textsf{SDP}(k)\n\\end{eqnarray*}\\]\nthen the sequence \\((X_n)_n\\) converges in distribution towards \\(\\mathcal{N}\\left(\\vec{\\mu}, K\\right)\\) (if \\(K = 0\\), the limit distribution is \\(\\delta_\\mu\\)).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#gaussian-conditioning",
    "href": "09-gaussian-vectors.html#gaussian-conditioning",
    "title": "13  Gaussian vectors",
    "section": "13.4 Gaussian conditioning",
    "text": "13.4 Gaussian conditioning\nLet \\((X_1,\\ldots,X_n)^T\\) be a Gaussian vector with distribution \\(\\mathcal{N}(\\mu, K)\\) where \\(K \\in \\textsf{DP}(n)\\). The covariance matrix \\(K\\) is partitioned into blocks \\[\nK = \\left[\n\\begin{array}{cc}\nA & B^t \\\\\nB & W\n\\end{array}\n\\right]\n\\] where \\(A \\in \\textsf{DP}(k)\\), \\(1 \\leq k &lt; n\\), and \\(W \\in \\textsf{DP}(n-k)\\).\nWe are interested in the conditional expectation of \\((X_1, \\ldots, X_k)^T\\) with repsect to \\(\\sigma(X_{k+1},\\ldots,X_n)\\) and in the conditional distribution of \\((X_1, \\ldots, X_k)^T\\) with respect to \\(\\sigma(X_{k+1},\\ldots,X_n)\\).\nThe Schur complement of \\(A\\) in \\(K\\) is defined as \\[\nW - B A^{-1} B^T\\, .\n\\] This definition makes sense for symmetric matrices when \\(A\\) is non-singular.\nIf \\(K \\in \\textsf{DP}(n)\\) then the Schur complement of \\(A\\) in \\(K\\) also belongs to \\(\\textsf{DP}(n-k)\\)\nIn the statement of the next theorems, \\(A^{-1/2}\\) denotes the Cholesky factor of \\(A^{-1}\\): \\(A^{-1} = A^{-1/2} \\times (A^{-1/2})^T\\).\n\nL’espérance conditionnelle de \\((X_{k+1}, \\ldots, X_n)^t\\) sachant \\((X_{1},\\ldots,X_k)^t\\) est une transformation affine de \\((X_{1},\\ldots,X_{k})^t\\):\n\\[\n\\mathbb{E}\\left[ \\begin{pmatrix}\nX_{k+1} \\\\ \\vdots \\\\ X_{n}\n\\end{pmatrix} \\mid \\begin{matrix}\nX_{1} \\\\ \\vdots \\\\ X_k\n\\end{matrix}\\right] = \\begin{pmatrix}\n\\mu_{k+1} \\\\ \\vdots \\\\ \\mu_n \\end{pmatrix}   + \\left(B A^{-1}  \\right) \\times  \\left( \\begin{pmatrix}\nX_{1} \\\\ \\vdots \\\\ X_{k}\n\\end{pmatrix}  - \\begin{pmatrix}\n\\mu_{1} \\\\ \\vdots \\\\ \\mu_k\n\\end{pmatrix}\\right) \\, .\n\\]\n\n\nThe conditional distribution of \\((X_{k+1}, \\ldots, X_n)^T\\) with respect to \\(\\sigma(X_{1},\\ldots,X_k)\\) is a Gaussian distribution whose expectation is the conditional expectation \\((X_{k+1}, \\ldots, X_n)^T\\) with respect to \\(\\sigma(X_{1},\\ldots,X_k)\\) and whose variance is the Schur complement of the covariance of \\((X_{1},\\ldots,X_k)^T\\) in the covariance matrix of \\((X_1, \\ldots, X_n)^T\\).\n\nWe will first study the conditional density, and, with a minimum amount of calculation, establish that it is Gaussian. Conditional expectation will be calculated as expectation under conditional distribution.\nTo characterize conditional density, we rely on a distributional representation argument (any Gaussian vector is distributed as the image of a standard Gaussian vector by an affine transformation) and a matrix analysis result that is at the core of the Cholesky factorization of positive semi-definite matrices.\n\\((X_1, \\ldots, X_n)^T\\) is distributed as the image of standard Gaussian vector by a block triangular matrix\n, et utiliser des propriétés des lois conditionnelles pour établir à la fois les deux résultats.\n\nLet \\(K\\) be a symmetric definite positive matrix with dimensions \\(n \\times n\\)\n\\[\nK = \\left[\n\\begin{array}{cc}\nA & B^t \\\\\nB & W\n\\end{array}\n\\right]\n\\]\nwhere \\(A\\) has dimensions \\(k \\times k\\), \\(1 \\leq k &lt; n\\).\nThen, the Schur-complement of \\(A\\) with respect to \\(K\\)\n\\[\nW - B A^{-1} B^t\n\\]\nis positive definite. Sub-matrices \\(A\\) and \\(W - B A^{-1} B^t\\) both have a Cholesky decomposition $A = L_1 L_1^t \\(,\\)W - B A^{-1} B^t = L_2 L_2^t$ where \\(L_1, L_2\\) are lower triangular, and \\(K\\)’s factorization reads like:\n\\[\nK = \\left[\n\\begin{array}{cc}\nL_1 & 0 \\\\\nB (L_1^t)^{-1}  & L_2\n\\end{array}\n\\right] \\times \\left[\n\\begin{array}{cc}\nL_1^t & L_1^{-1} B^t \\\\\n0 & L_2^t\n\\end{array}\n\\right] \\, .\n\\]\n\n\nProof. Without loss of generality, we check the statement on centered vectors. The Cholesky factorization of \\(K\\) allows us to write\n\\[\n\\begin{pmatrix}\nX_1 \\\\\n\\vdots \\\\\nX_n\n\\end{pmatrix} \\sim \\left[\n\\begin{array}{cc}\nL_1 & 0 \\\\\nB (L_1^t)^{-1}  & L_2\n\\end{array}\n\\right] \\times \\begin{pmatrix}\nY_1 \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix}\n\\]\nwhere \\(( Y_1,\n\\ldots,\nY_n)^t\\) is a centered standard Gaussian vector.\nIn the sequel, we assume \\((X_1, \\ldots,X_n)^T\\) and \\((Y_1,\\ldots,Y_n)^T\\) live on the same probability space. As \\(L_1\\) is invertible, the \\(\\sigma\\)-algebras generated by \\((X_1, \\ldots,X_k)^T\\) and \\((Y_1, \\ldots,Y_k)^T\\) are equal. We agree on \\(\\mathcal{G}=\\sigma(X_1, \\ldots,X_k)\\). The conditional expectations and conditional distributions also coincide .\n\\[\\begin{eqnarray*}\n\\mathbb{E} \\left[ \\begin{pmatrix}\nX_{k+1} \\\\\n\\vdots \\\\\nX_n\n\\end{pmatrix} \\mid \\mathcal{G} \\right] &= &\\mathbb{E} \\left[ B (L_1^t)^{-1} \\begin{pmatrix}\nY_{1} \\\\\n\\vdots \\\\\nY_k\n\\end{pmatrix} \\mid \\mathcal{G} \\right] + \\mathbb{E} \\left[ L_2 \\begin{pmatrix}\nY_{k+1} \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix} \\mid \\mathcal{G} \\right]  \\\\\n& = & B (L_1^t)^{-1} L_1^{-1}\\begin{pmatrix}\nX_{1} \\\\\n\\vdots \\\\\nX_k\n\\end{pmatrix} =  B A^{-1} \\begin{pmatrix}\nX_{1} \\\\\n\\vdots \\\\\nX_k\n\\end{pmatrix} \\, ,\n\\end{eqnarray*}\\]\ncar \\((Y_{k+1}, \\ldots,Y_n)^t\\) is centered and independent from \\(\\mathcal{G}\\).\nNote that residuals\n\\[\n\\begin{pmatrix}\nX_{k+1} \\\\\n\\vdots \\\\\nX_n\n\\end{pmatrix} -\\mathbb{E} \\left[ \\begin{pmatrix}\nX_{k+1} \\\\\n\\vdots \\\\\nX_n\n\\end{pmatrix} \\mid \\mathcal{G} \\right] = L_2 \\begin{pmatrix}\nY_{k+1} \\\\\n\\vdots \\\\\nY_n\n\\end{pmatrix}\n\\]\nare independent from \\(\\mathcal{G}\\). This is a Gaussian property. For general square integrable random variables, we may only assert that residuals are orthogonal to \\(\\mathcal{G}\\)-measurable random variables.\nThe conditional distribution of \\((X_{k+1},\\ldots, X_n)^T\\) with respect to \\((X_1,\\ldots, X_k)^T\\) coincides with the conditional distribution of\n\\[\nB (L_1^t)^{-1}   \\times\n\\begin{pmatrix}\nY_1\\\\ \\vdots \\\\ Y_k\n\\end{pmatrix} + L_2 \\times\n\\begin{pmatrix}\nY_{k+1}\\\\ \\vdots \\\\ Y_n\n\\end{pmatrix}\n\\]\nconditionally on \\((Y_1,\\ldots, Y_k)^T\\). As \\((Y_1,\\ldots, Y_k)^t = L_1^{-1}(X_1,\\ldots,X_k)^T\\), the conditional distribution we are looking for is Gaussian with expectation \\[B A^{-1}   \\times\n\\begin{pmatrix}\nX_1\\\\ \\vdots \\\\ X_k\n\\end{pmatrix}\\] (the conditional expectation) and variance \\(L_2 \\times L_2^t = W - B A^{-1} B^t\\).\n\n\nIf \\((X,Y)^T\\) is a centered Gaussian vector with \\(\\operatorname{var}(X)=\\sigma_x^2\\), \\(\\operatorname{var}(Y)=\\sigma^2_y\\) and \\(\\operatorname{cov}(X,Y)= \\rho \\sigma_x \\sigma_y\\), the conditional distribution of \\(Y\\) with respect to \\(X\\) is\n\\[\n\\mathcal{N}\\left( \\rho \\sigma_y/\\sigma_x X, \\sigma^2_y (1- \\rho^2)  \\right) \\, .\n\\]\nThe quantity \\(\\rho\\) is called the linear correlation coefficient between \\(X\\) and \\(Y\\). By Cauchy-Schwarz’s inequality, \\(\\rho \\in [-1,1]\\).\n\nThese two theorems are usually addressed in the order in which they are stated. Conditional expectation is characterized by adopting the \\(L^2\\) (predictive) viewpoint: the conditional expectation of the random vector \\(Y\\) knowing \\(X\\) is defined as the best \\(X\\)-measurable predictor of the vector \\(Y\\) with respect to quadratic error (the random vector \\(Z\\), \\(X\\)-measurable that minimizes \\(\\mathbb{E} \\left[ \\| Y- Z\\|^2 \\right]\\)).\nIn order to characterize conditional expectation, we first compute the optimal affine predictor of \\((X_{k+1},\\ldots,X_n)^T\\) based on \\((X_{1},\\ldots,X_k)^T\\). This optimal affine predictor is\n\\[\n\\begin{pmatrix}\n\\mu_{k+1} \\\\ \\vdots \\\\ \\mu_n \\end{pmatrix}   + \\left(B A^{-1}  \\right) \\times  \\left( \\begin{pmatrix}\nX_{1} \\\\ \\vdots \\\\ X_{k}\n\\end{pmatrix}  - \\begin{pmatrix}\n\\mu_{1} \\\\ \\vdots \\\\ \\mu_k\n\\end{pmatrix}\\right) \\, ,\n\\]\n(if Gaussian vectors are centred, this amounts to determine the matrix \\(P\\) with dimensions \\((n-k)\\times k\\) which minimizes \\(\\text{trace}(PA P^t -2 B P^t\\))). The optimal affine predictor is a Gaussian vector, one can check that the residual vector\n\\[\n\\begin{pmatrix}\nX_{k+1}\\\\ \\vdots \\\\ X_n\n\\end{pmatrix} - \\left\\{ \\begin{pmatrix}\n\\mu_{k+1} \\\\ \\vdots \\\\ \\mu_n \\end{pmatrix}   + \\left(B A^{-1}\\right) \\times  \\left( \\begin{pmatrix}\nX_{1} \\\\ \\vdots \\\\ X_{k}\n\\end{pmatrix}  - \\begin{pmatrix}\n\\mu_{1} \\\\ \\vdots \\\\ \\mu_k\n\\end{pmatrix}\\right) \\right\\}\n\\] is also Gaussian and orthogonal to the affine predictor. The residual vector is independent from the affine predictor.\nThis is enough to establish that the affine predictor is the orthogonal projection of \\((X_{k+1}, \\ldots, X_n)^T\\) on the closed linear subspace of square-integrable \\((X_{1},\\ldots,X_k)^T\\)-measurable random vectors.\nThis proves that the affine predictor is the conditional expectation.\nIn the notes, we deal with a special case of linear conditioning.\nTo fugure out general linear conditioning, consider \\(X \\sim \\mathcal{N}(0, {K})\\) (we assume centering to alleviate notation and computations, translating does not change the relevant \\(\\sigma\\)-algebras and thus conditioning), where \\({K} \\in \\textsf{DP}(n)\\), and a linear transformation defined by matrix \\(H\\) with dimensions \\(m \\times n\\). \\(H\\) is assumed to have rank \\(m\\). Agree on \\(Y= {H} X\\). Considering the Gaussian vector \\([ X^T : Y^T]\\) with covariance matrix \\[\n\\left[\n\\begin{array}{cc}\n{K} & {K} {H}^t \\\\\n{H}{K} & {H} {K} {H}^t\n\\end{array}\n\\right]\n\\] and adapting the previous computations (the covariance matrix is not positive definite any more), we may check that the conditional distribution of \\(X\\) with respect to \\(Y\\) is Gaussian with expectation \\[\n  K H^T (HKH^T)^{-1}\n\\]\nand variance\n\\[\n  K - K H^t (HKH^T)^{-1} H K \\, .\n\\]\nThe linearity of conditional expectation is a property of Gaussian vectors and linear conditioning. If you condition with respect to the norm \\(\\| X\\|_2\\), the conditional distribution is not Gaussian anymore.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#about-gamma-distributions",
    "href": "09-gaussian-vectors.html#about-gamma-distributions",
    "title": "13  Gaussian vectors",
    "section": "13.5 About Gamma distributions",
    "text": "13.5 About Gamma distributions\nInvestigating the norm of Gaussian vectors will prompt us to introduce \\(\\chi^2\\) distributions, a sub-family of Gamma distributions.\n\n\n13.5.1 Gamma distributions\nA Gamma distribution with parameters \\((p, \\lambda)\\)} (\\(\\lambda \\in\n\\mathbb{R}_+\\) and \\(p \\in \\mathbb{R}_+\\)), is a distribution on \\((\\mathbb{R}_+, \\mathcal{B}(\\mathbb{R}_+))\\) with density\n\\[g_{p, \\lambda} (x) \\equiv \\frac{\\lambda^p}{\\Gamma (p)} \\mathbf{1}_{x\n\\geq 0} x^{p - 1} e^{- \\lambda x}\n\\] where \\(\\Gamma (p) \\equiv \\int_0^{\\infty} t^{p - 1} e^{- t} \\mathrm{d} t\\).\nParameter \\(p\\) is called the shape parameter, \\(\\lambda\\) is called the rate or intensity parameter, \\(1/\\lambda\\) is called the scale parameter.\n\n\nIf \\(X \\sim  \\text{Gamma}(p,1)\\) then \\(\\sigma X \\sim \\text{Gamma}(p,1/\\sigma)\\) for \\(\\sigma&gt;0\\).\nEuler’s \\(\\Gamma ()\\) function interpolates the factorial. For every positive real \\(p\\), \\(\\Gamma (p + 1) = p \\Gamma\n(p) .\\) If \\(p\\) is integer, \\(\\Gamma (p + 1) = p!\\)\n\nCheck that \\(\\Gamma(1/2)=\\sqrt{\\pi}\\).\n\n\nIf \\(X \\sim \\mathrm{Gamma}(p, \\lambda)\\) \\(\\mathbb{E}X = \\frac{p}{\\lambda}\\) and \\(\\operatorname{var}(X) = \\frac{p}{\\lambda^2}\\).\n\nThe next proposition is a cornerstone of Gamma-calculus. The sum of two independent Gamma-distributed random variables is Gamma distributed if they have the same intensity (or scale) parameter.\n\nIf \\(X\\) and \\(Y\\) are independent Gamma-distributed random variables with the same intensity parameter \\(\\lambda\\) \\(X \\sim \\mathrm{Gamma}(p,\n\\lambda), Y\\sim \\mathrm{Gamma}(q, \\lambda)\\) then \\(X + Y \\sim \\mathrm{Gamma}(p+q, \\lambda)\\).\n\n\nProof. The density of the distribution of \\(X+Y\\) is the convolution of the densities \\(g_{p, \\lambda}\\) et \\(g_{q, \\lambda}\\). \\[\\begin{eqnarray*}\ng_{p, \\lambda} \\ast g_{q, \\lambda} (x) & = & \\int_{\\mathbb{R_{}}}\ng_{p, \\lambda} (z) g_{_{q, \\lambda}} (x - z) \\mathrm{d} z\\\\\n& = & \\int_0^x    g_{p, \\lambda} (z) g_{_{q,\n\\lambda}} (x - z) \\mathrm{d} z\\\\\n& = & \\int_0^x  \\frac{\\lambda^p}{\\Gamma (p)} z^{p - 1} \\mathrm{e}^{- \\lambda\nz}     \\frac{\\lambda^q}{\\Gamma (q)} (x - z)^{q - 1} \\mathrm{e}^{-\n\\lambda (x - z)} \\mathrm{d} z\\\\\n& = & \\frac{\\lambda^{p + q}}{\\Gamma (p) \\Gamma (q)} \\mathrm{e}^{- \\lambda x}\n\\int_0^x z^{p - 1} (x - z)^{q - 1}    \\mathrm{d} z\\\\\n&  & \\operatorname{changement} \\operatorname{de} \\operatorname{variable} z = x u\\\\\n& = & \\frac{\\lambda^{p + q}}{\\Gamma (p) \\Gamma (q)} \\mathrm{e}^{- \\lambda x}\nx^{p + q - 1} \\int_0^{1} u^{p-1} (1 - u)^{q - 1}\n\\mathrm{d} u\\\\\n& = & g_{p + q, \\lambda} (x) \\frac{\\Gamma(p+q)}{\\Gamma(p)\\Gamma(q)} \\int_0^{1} u^{p-1} (1 - u)^{q - 1}\n\\mathrm{d} u \\, .\n\\end{eqnarray*}\\] We may pocket the next observation: \\[\nB(p,q):=   \\int_0^{1} u^{p-1} (1 - u)^{q - 1}\n\\mathrm{d} u\n\\] satisfies \\(B(p,q) = \\frac{\\Gamma(p)\\Gamma(q)}{\\Gamma(p+q)}.\\)\n\nGamma distributions with parameters \\((k / 2, 1 / 2)\\) for \\(k \\in\n\\mathbb{N}\\) deserve to be named: they are \\(\\chi^2\\) distributions with \\(k\\) degrees of freedom.\n\nProposition 13.6 (Chi-square distributions) The \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom (denoted by \\(\\chi^2_k\\)) has density over \\([0,\\infty)\\), \\[\n\\frac{x^{ \\frac{1}{2} (k - 2)} e^{- \\frac{x}{2}}}{2^{k / 2} \\Gamma (k /\n2)} .\n\\]\n\n\nThe sum of \\(k\\) independent squared standard Gaussian random variables is distributed according to the chi-square distributions with \\(k\\) degrees of freedom \\(\\chi^2_k\\).\n\n\nProof. According to proposition ?prp-densite-gamma), it suffices to establish the proposition \\(k = 1.\\)\nLet \\(X \\sim \\mathcal{N}(0,1)\\), for \\(t\\geq 0\\), \\[\\begin{align*}\n\\mathbb{P} \\left\\{ X^2 \\leq t\\right\\}\n& = & \\Phi(\\sqrt{t}) - \\Phi(-\\sqrt{t}) \\\\\n& = & 2 \\Phi(\\sqrt{t})  - 1 \\,.\n\\end{align*}\\] Now, differentiating with respect to \\(t\\), applying the chain rule provides us with a formula for the density: \\[\n2 \\frac{1}{2\\sqrt{t}} \\phi(\\sqrt{t}) = \\frac{1}{\\sqrt{2\\pi t}} \\mathrm{e}^{-\\frac{t}{2}} = \\left(\\frac{1}{2}\\right)^{1/2} \\frac{t^{-1/2}}{\\Gamma(1/2)} \\mathrm{e}^{-\\frac{t}{2}} \\,.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#norms-of-centred-gaussian-vectors",
    "href": "09-gaussian-vectors.html#norms-of-centred-gaussian-vectors",
    "title": "13  Gaussian vectors",
    "section": "13.6 Norms of centred Gaussian vectors",
    "text": "13.6 Norms of centred Gaussian vectors\nThe distribution of the squared Euclidean norm of a centered Gaussian vector only depends on the spectrum of its covariance matrix.\n\nIf \\({X}:= (X_1, X_2,\n\\ldots, X_n)^{^T} \\sim \\mathcal{N}\\left(0, A\\right)\\) with \\(A = L L^T\\) (\\(L\\) lower triangular), if \\(M \\in \\mathrm{SDP}(n)\\), then \\({X}^T M {X}\\) is distributed like \\(\\sum_{i = 1}^n \\lambda_i Z_i\\) where \\((\\lambda_i)_{i \\in \\{1, \\ldots, n\\}}\\) denote the eigenvalues of \\(L^T \\times M\\times L\\) and where \\(Z_i\\) are independent \\(\\chi^2_1\\)-distributed random variables.\n\nThis is a corollary of an important property of standard Gaussian vectors: rotational invariance. The standard Gaussian distribution is invariant under orthogonal transform (a matrix \\(O\\) is orthogonal iff \\(OO^T=\\text{Id})\\).\n\nProof. Matrix \\(A\\) may be factorized as \\(A = LL^t\\) (Cholesky), and \\({X}\\) is distributed like \\(L {Y}\\) where \\({Y}\\) is standard Gaussian. The quadratic form \\({X}^T M\n{X}\\) is thus distributed like \\({Y}^T {L}^T M\n{L}  {Y}\\). There exist an orthogonal transform \\(O\\) such that \\(L^T M L = O^t \\operatorname{diag}\n(\\lambda_i) O.\\) Random vector \\(O {Y}\\) is distributed like \\(\\mathcal{N} (0, I_n)\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#norm-of-non-centred-gaussian-vectors",
    "href": "09-gaussian-vectors.html#norm-of-non-centred-gaussian-vectors",
    "title": "13  Gaussian vectors",
    "section": "13.7 Norm of non-centred Gaussian vectors",
    "text": "13.7 Norm of non-centred Gaussian vectors\nThe distribution of the squared norm of a Gaussian vector with covariance matrix \\(\\sigma^2 \\operatorname{Id}\\) depends on the norm of the expectation but does not depend on its direction. In addition, this distribution stochastically can be compared with the distribution of the squared norm of a centred Gaussian vector with the same covariance.\n\n\n13.7.1 rdering\nIn a probability space endowed with distribution \\(\\mathbb{P}\\), a real random variable \\(X\\) is stochastically smaller than random variable \\(Y\\), if \\[\n\\mathbb{P} \\{ X \\leq Y \\} =  1 \\, .\n\\] The distribution of \\(Y\\) is said to stochastically dominate the distribution of \\(X\\).\n\n\nIf \\(X\\) is stochastichally less than \\(Y\\) and if \\(F\\) and \\(G\\) denote the cumulative distribution functions of \\(X\\) and \\(Y\\), then for all \\(x \\in  \\mathbb{R}\\), \\(F(x)\\geq G(x)\\). Quantile functions \\(F^\\leftarrow, G^\\leftarrow\\) satisfy \\(F^\\leftarrow(p) \\leq G^\\leftarrow(p)\\) for \\(p \\in (0,1)\\).\nConversely.\n\nIf \\(F\\) and \\(G\\) are two cumulative distribution functions that satisfy \\(\\forall x \\in \\mathbb{R}\\) \\(F(x)\\geq G(x)\\) then there exists a probability space equipped with a probability distribution \\(\\mathbb{P}\\) and two random variables \\(X\\) and \\(Y\\) with cumulative distribution functions \\(F, G\\) that satisfy:\\[\n\\mathbb{P}\\{ X \\leq Y\\} = 1 \\, .\n\\]\n\nThe proof proceeds by a quantile coupling argument.\n\nProof. It is enough to endow \\(([0,1], \\mathcal{B}([0,1])\\) with the uniform distribution. Let \\(X (\\omega)=  F^{\\leftarrow}(\\omega)\\), \\(Y(\\omega) = G^\\leftarrow(\\omega)\\). Then the distribution of \\(X\\) (resp. \\(Y\\)) has cumulative distribution function \\(F\\) (resp. \\(G\\)) and the following holds: \\[\n\\mathbb{P} \\{ X \\leq Y\\}  = \\mathbb{P} \\{ F^{\\leftarrow}(U) \\leq  G^{\\leftarrow}(U)\\} = 1 \\, .\n\\]\n\n\nIf \\(X \\sim \\mathcal{N}\\left( 0, \\sigma^2 \\operatorname{Id}\\right)\\) and \\(Y \\sim \\mathcal{N}\\left( \\theta, \\sigma^2 \\operatorname{Id}\\right)\\) with \\(\\theta \\in \\mathbb{R}^d\\) then \\[\n\\left\\Vert Y \\right\\Vert^2 \\sim  \\left( (Z_1 + \\|\\theta\\|_2)^2 + \\sum_{i=1}^d Z_i^2 \\right)\n\\] where \\(Z_i\\) are i.i.d. according to \\(\\mathcal{N}(0,\\sigma^2)\\).\nFor every \\(x \\geq 0\\), \\[\n\\mathbb{P} \\left\\{ \\| Y \\|\\leq x\\right\\} \\leq \\mathbb{P} \\left\\{ \\| X \\| \\leq x \\right\\} \\, .\n\\] The distribution of \\(\\| Y\\|^2/\\sigma^2\\) (non-centred \\(\\chi^2\\) with parameter \\(\\| \\theta\\|_2/\\sigma\\)) stochastichally dominates the distribution of \\(\\| X\\|^2/\\sigma^2\\) (centred \\(\\chi^2\\) with the same number of degrees of freedom).\n\n\nProof. The Gaussian vector \\(Y\\) is distributed like \\(\\theta + X\\). There exists an orthogonal transform \\(O\\) such that \\[\nO \\theta = \\begin{pmatrix}\n\\| \\theta\\|_2 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{pmatrix} \\, .\n\\] Vectors \\(OY\\) and \\(OX\\) respectively have the same norms as \\(X\\) and \\(Y\\).\nThe squared norm of \\(Y\\) is distributed as the squared norm of \\(OY\\), that is like \\((Z_1+ \\|\\theta\\|_2)^2 +\\sum_{i=2}^d Z_i^2\\). This proves the first part of the theorem.\nTo establish the second part of the theorem, it suffices to check that for every \\(x\\geq 0\\), \\[\n\\mathbb{P} \\left\\{ (Z_1+ \\|\\theta\\|_2)^2  \\leq x \\right\\} \\leq\n\\mathbb{P} \\left\\{ X_1^2 \\leq x \\right\\} \\, ,\n\\] that is \\[\n\\mathbb{P} \\left\\{ |Z_1+ \\|\\theta\\|_2|  \\leq \\sqrt{x} \\right\\} \\leq\n\\mathbb{P} \\left\\{ |X_1| \\leq \\sqrt{x} \\right\\} \\, ,\n\\] or \\[\n\\Phi(\\sqrt{x}- \\|\\theta\\|_2) - \\Phi(-\\sqrt{x}-\\|\\theta\\|_2)  \\leq \\Phi(\\sqrt{x}) -  \\Phi(-\\sqrt{x}) \\, .\n\\] For \\(y&gt;0\\), the function mapping \\([0,\\infty)\\) to \\(\\mathbb{R}\\), defined by \\(a \\mapsto \\Phi(y-a) - \\Phi(-y-a)\\) is non-increasing with respect to \\(a\\): it derivative with respect to \\(a\\) equals \\(-\\phi(y-a)+\\phi(-y-a)=\\phi(y+a)-\\phi(y-a)\\leq 0\\). The conclusion follows\n\nThe last step of the proof reads as\\[\n\\mathbb{P} \\left\\{ X \\in \\theta + C \\right\\} \\leq \\mathbb{P} \\left\\{ X \\in C\\right\\}\n\\] where \\(X \\sim \\mathcal{N}(0,\\operatorname{Id}_1)\\), \\(\\theta \\in \\mathbb{R}\\) and \\(C = [-\\sqrt{x},\\sqrt{x}]\\). This inequality holds in dimension \\(d\\geq 1\\) if \\(C\\) is compact, convex, symmetric. This (subtle) result is called Anderson’s Lemma.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#sec-cochran",
    "href": "09-gaussian-vectors.html#sec-cochran",
    "title": "13  Gaussian vectors",
    "section": "13.8 Cochran Theorem and consequences",
    "text": "13.8 Cochran Theorem and consequences\n\n\n13.8.1 Cochran\nLet \\(X \\sim \\mathcal{N}(0, \\text{I}_n)\\) and \\(\\mathbb{R}^n = \\oplus_{j=1}^k E_j\\) where \\(E_j\\) are pairwise orthogonal linear subspaces of \\(\\mathbb{R}^n\\). Denote by \\(\\pi_{E_j}\\) the orthogonal projection on \\(E_j\\).\nThe collection of Gaussian vectors \\(\\left( \\pi_{E_j} X\\right)_{j \\leq k}\\) is independent and for each~\\(j\\) \\[\n\\| \\pi_{E_j} X\\|_2^2 \\sim \\chi^2_{\\text{dim}(E_j)} \\, .\n\\]\n\n\n\nProof. The covariance matrix of \\(\\pi_{E_j} X\\) is \\(\\pi_{E_j} \\pi_{E_j}^t = \\pi_{E_j}\\). The eigenvalues of \\(\\pi_{E_j}\\) are \\(1\\) with multiplicity \\(\\text{dim}(E_j)\\) and \\(0\\). The statement about the distribution of \\(\\| \\pi_{E_j} X\\|_2^2\\) is a corollary of ?prp-normgaussstand and ?prp-normespectre.\nTo prove stochastic independence, let us consider \\(\\mathcal{I}, \\mathcal{J} \\subset \\{1,\\ldots,k\\}\\) with \\(\\mathcal{I} \\cap \\mathcal{J} = \\emptyset.\\) It is enough to check that for all \\((\\alpha)_{j \\in \\mathcal{I}}, (\\beta_j)_{j \\in \\mathcal{J}}\\), the characteristic functions of \\[\\left(\\sum_{j\\in \\mathcal{I}} \\langle \\alpha_j, \\pi_{E_j} X \\rangle, \\sum_{j\\in \\mathcal{J}} \\langle \\beta_j, \\pi_{E_j} X \\rangle\\right)\\] can be factorized. It suffices to check that the two Gaussians are orthogonal.\n\\[\\begin{eqnarray*}\n{ \\mathbb{E} \\left[ \\left(\\sum_{j\\in \\mathcal{I}} \\langle \\alpha_j, \\pi_{E_j} X \\rangle \\right) \\times \\left(\\sum_{j'\\in \\mathcal{J}} \\langle \\beta_{j'}, \\pi_{E_{j'}} X \\rangle\\right)\\right]}\n& = & \\sum_{j \\in \\mathcal{I}, j' \\in \\mathcal{J}}\n\\alpha_j^t \\pi_{E_j} \\pi_{E_{j'}} \\beta_{j'}\n=  0 \\, .\n\\end{eqnarray*}\\]\n\nThe next result is a cornerstone of statistical inference in Gaussian models. It is a corollary of Cochran’s Theorem.\n\n\n13.8.2 Student\nOf \\((X_1, \\ldots, X_n)\\) are i.i.d. according to \\(\\mathcal{N} (\\mu, \\sigma^2)\\), if \\(\\overline{X}_n:=\n\\sum^n_{i = 1}  X_i / n\\) et \\(V:= \\sum^{n}_{i = 1} (X_i - \\overline{X}_n)^2\\), then\n\n\n\nProof. Without loss of generality, we may assume that \\(\\mu=0\\) et \\(\\sigma=1\\).\nAs \\[\n\\begin{pmatrix}\n\\overline{X}_n \\\\\n\\vdots\\\\\n\\overline{X}_n \\\\\n\\end{pmatrix}\n=  \\frac{1}{n} \\begin{pmatrix}\n1 \\\\\n\\vdots\\\\\n1 \\\\\n\\end{pmatrix} \\times \\begin{pmatrix}\n1 & \\ldots & 1\n\\end{pmatrix} X\n\\] the vector \\((\\overline{X}_n, \\ldots , \\overline{X}_n)^t\\) is the orthogonal projection of the standard Gaussian vector \\(X\\) on the line generated by \\((1, \\ldots, 1)^t\\).\nVector \\((X_1- \\overline{X}_n, \\ldots , X_n -\\overline{X}_n)^t\\) is the orthogonal projection fo Gaussian vector \\(X\\) on the hyperplane which is orthogonal to \\((1, \\ldots, 1)^t\\).\nAccording to Cochran’s Theorem (Section 13.8), random vectors \\((\\overline{X}_n, \\ldots , \\overline{X}_n)^t\\), and \\((X_1- \\overline{X}_n, \\ldots , X_n -\\overline{X}_n)^t\\) are independent.\nThe distribution of \\(\\overline{X}_n\\) is trivially Gaussian.\nThe distribution of \\(V\\) is characterized using Cochran’s Theorem.\n\n\n\n13.8.3 Distribution\nIf \\(X \\sim \\mathcal{N}(0,1)\\), \\(Y \\sim \\chi_p^2\\) and if \\(X\\) and \\(Y\\) are independent, then \\(Z =  X/ \\sqrt{Y/p}\\) is distributed according to a (centred) Student distribution with \\(p\\) degrees of freedom.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#gaussian-concentration",
    "href": "09-gaussian-vectors.html#gaussian-concentration",
    "title": "13  Gaussian vectors",
    "section": "13.9 Gaussian concentration",
    "text": "13.9 Gaussian concentration\nThe very definition of Gaussian vectors characterizes he distribution of any affine function of a standard Gaussian vector. If the linear part of the affine function is defined by a vector \\(\\lambda\\), we know that the variance will be \\(\\|\\lambda\\|^2_2\\). What happens if we are interested in fairly regular functions of a standard Gaussian vector? for example if we consider \\(L\\)-lipschitzian functions? These are generalizations of affine functions. We cannot therefore expect a general increase in the variance of the \\(L\\)-Lipschitzian functions of a standard Gaussian vector better than \\(L^2\\) (in the linear case the Lipschitz constant is the Euclidean norm of \\(\\lambda\\)). It is remarkable that the bound provided for linear functions extends to Lipschitzian functions. It is even more remarkable that this bound does not involve the dimension of the ambient space.\n\nLet \\(X \\sim \\mathcal{N}(0 , \\text{Id}_d)\\).\n\nThe proof relies on the next identity.\n\n\n13.9.1 Covariance identity\nLet \\(X,Y\\) be two independent \\(\\mathbb{R}^d\\)-valued standard Gaussian vectors, let \\(f,g\\) be two differentiable functions from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}\\). \\[\n\\operatorname{cov}(f(X),g(X)) = \\int_0^1 \\mathbb{E}\\left\\langle \\nabla f(X) , \\nabla g\\left(\\alpha X +\\sqrt{1- \\alpha^2} Y \\right) \\right\\rangle \\mathrm{d} \\alpha\n\\]\n\n\nWe start by checking this proposition on functions \\(x \\mapsto  \\mathrm{e}^{\\imath \\langle \\lambda, x\\rangle}, x \\in \\mathbb{R}^d\\).\n\nProof. Let us first check Poincaré’s inequality.\nWe choose \\(f=g\\). Starting from the covariance identity, thanks to Cauchy-Schwarz’s inequality: \\[\\begin{eqnarray*}\n\\operatorname{var}(f(X) )     &= & \\operatorname{cov}(f(X),f(X)) \\\\\n& = & \\int_0^1  \\mathbb{E}\\left\\langle \\nabla f(X) , \\nabla f\\left(\\alpha X +\\sqrt{1- \\alpha^2} Y \\right) \\right\\rangle \\mathrm{d} \\alpha \\\\\n& \\leq & \\int_0^1 \\left( \\mathbb{E}\\|  \\nabla f(X) \\|^2\\right)^{1/2} \\times\n\\left(\\mathbb{E} \\|\\nabla f\\left(\\alpha X +\\sqrt{1- \\alpha^2} Y\\right)\\|^2 \\right)^{1/2} \\mathrm{d} \\alpha \\, .\n\\end{eqnarray*}\\] The desired results follows by noticing that \\(X\\) and \\(\\alpha X + \\sqrt{1- \\alpha^2}Y\\) are both \\(\\mathcal{N}(0,\\text{Id})\\)-distributed.\nTo obtain the exponential inequality, choose \\(f\\) differentiable and 1-Lipschitz, and \\(g = \\exp(\\lambda f)\\) pour \\(\\lambda\\geq 0\\). Without loss of generality, assume \\(\\mathbb{E}f(X)=0\\). The covariance identity and the chain rule imply \\[\\begin{eqnarray*}\n\\operatorname{cov}\\left(f(X),\\mathrm{e}^{\\lambda f(X)}\\right)  & = & \\lambda\n\\int_0^1  \\mathbb{E}\\left[\\left\\langle \\nabla f(X) , \\nabla f\\left(\\alpha X +\\sqrt{1- \\alpha^2} Y \\right) \\right\\rangle \\mathrm{e}^{\\lambda f\\left(\\alpha X +\\sqrt{1- \\alpha^2} Y \\right)}\\right] \\mathrm{d} \\alpha \\\\\n& \\leq & \\lambda L^2\n\\int_0^1  \\mathbb{E}\\left[ \\mathrm{e}^{\\lambda f\\left(\\alpha X +\\sqrt{1- \\alpha^2} Y \\right)}\\right] \\mathrm{d} \\alpha \\\\\n& = & \\lambda L^2 \\mathbb{E}\\left[ \\mathrm{e}^{\\lambda f\\left(X\\right)}\\right]\n\\end{eqnarray*}\\] Define \\(F(\\lambda):= \\mathbb{E}\\left[ \\mathrm{e}^{\\lambda f\\left(X\\right)}\\right]\\). Note that we have just established a differential inequality for \\(F\\), checking \\(\\operatorname{cov}( f , \\mathrm{e}^{\\lambda f})= F'(\\lambda)\\) since \\(f\\) is centred: \\[\nF'( \\lambda) \\leq \\lambda L^2 F(\\lambda) \\,.\n\\] Solving this differential inequality under \\(F(0)=1\\), for \\(\\lambda\\geq 0\\) \\[\nF( \\lambda) \\leq \\mathrm{e}^{\\frac{\\lambda^2L^2}{2}} \\, .\n\\] The same approach works for \\(\\lambda&lt;0\\). It is enough to invoke Markov’s exponential inequality and to optimize with respect to \\(\\lambda=t/L^2\\).\n\nConcentration inequalities describe the behavior of the norm of high-dimensional Gaussian vectors.\n\nIf \\(X\\) is a standard \\(d\\)-dimensional Gaussian vector, then \\[\n\\operatorname{var}(\\|X\\|_2) \\leq 1 \\,\n\\] and \\[\n\\sqrt{d-1} \\leq \\mathbb{E} \\|X\\|_2 \\leq \\sqrt{d} \\, .\n\\]\n\n\nProof. The Euclidean norm is \\(1\\)-Lipschitz (triangle inequality). The first inequality follows fron Poincaré’s inequality.\nThe upper bound on expectation follows from Jensen’s inequality.\nThe lower bound on expectation follows from \\((\\mathbb{E} \\|X\\|_2)^2 = \\mathbb{E} \\|X\\|_2^2 - \\operatorname{var}(\\|X\\|_2)= d -\\operatorname{var}(\\|X\\|_2)\\) and from the variance upper bound.\n\n\nLet \\(X \\sim \\mathcal{N} (0,K)\\) where \\(K\\) is in \\(\\textsf{DP}(d)\\) and \\(Z=  \\max_{i\\leq d} X_i\\).\nShow \\[\n\\operatorname{Var}(Z) \\leq \\max_{i \\leq d } K_{i,i}:= \\max_{i \\leq d} \\operatorname{Var} (X_i) .\n\\]\n\n\nLet \\(X, Y\\sim \\mathcal{N} (0,\\text{Id}_n)\\) with \\(X,Y\\) indépendent.\nShow\n\\[\n\\sqrt{2n-1} \\leq   \\mathbb{E}[\\|X-Y\\|] \\leq \\sqrt{2 n}\n\\] and\n\\[\n\\mathbb{P}   \\left\\{ \\|X-Y\\| - \\mathbb{E}[\\|X-Y\\|] \\geq t \\right\\} \\leq \\mathrm{e}^{-t^2} \\, .\n\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  },
  {
    "objectID": "09-gaussian-vectors.html#bibliographic-remarks",
    "href": "09-gaussian-vectors.html#bibliographic-remarks",
    "title": "13  Gaussian vectors",
    "section": "13.10 Bibliographic remarks",
    "text": "13.10 Bibliographic remarks\nGaussian literature is very abundant, see for example (janson1997gaussian?). Much of this literature is relevant to statistics.\nThe lemmas ?lem-stein and ?lem-steinbis that characterize the Gaussian standard are the starting point of Stein’s (Charles) method to demonstrate the central limit theorem (and many other results). This relatively recent development is described in (Ross, 2011).\nMatrix analysis and algorithmics play an important role in Gaussian analysis and statistics. The books (Horn & Johnson, 1990), and if we wish to go further (Bhatia, 1997), provide an introduction to the concepts and techniques of matrix factorization and elements of perturbation theory.\nThere is a multi-dimensional version of the laws of \\(\\chi^2\\) that appear when determining the law of variance empirical. These are the laws of Wishart. They were the subject of intensive studies in random matrix theory, see for example (Anderson, Guionnet, & Zeitouni, 2010)\nGaussian concentration plays an important role in non-parametric statistics and is a source of inspiration in statistical learning. M. Ledoux’s book (Ledoux, 2001) provides an elegant perspective on this issue.\n\n\n\n\nAnderson, G. W., Guionnet, A., & Zeitouni, O. (2010). An introduction to random matrices (Vol. 118). Cambridge: Cambridge University Press.\n\n\nBhatia, R. (1997). Matrix analysis. Springer-Verlag.\n\n\nHorn, R. A., & Johnson, C. R. (1990). Matrix analysis. Cambridge University Press.\n\n\nLedoux, M. (2001). The concentration of measure phenomenon. AMS.\n\n\nRoss, N. (2011). Fundamentals of Stein’s method. ArXiv e-Prints. Retrieved from https://arxiv.org/abs/1109.1880",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Gaussian vectors</span>"
    ]
  }
]