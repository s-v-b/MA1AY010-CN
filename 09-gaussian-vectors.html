<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>16&nbsp; Gaussian vectors – MA1AY010 Class Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./08-convergence-3.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3a70adc2469c323d0515427c5f9cb542.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">MA1AY010 Class Notes</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
    <a href="https://github.com/s-v-b/MA1AY010-CN/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MA1AY010-Class-Notes.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./09-gaussian-vectors.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Gaussian vectors</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Warm up</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-language.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">A modicum of measure theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./031-moments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A modicum of integration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./033-integration2moments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">From integrals to expectation and moments</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Families of discrete distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./0225-pgf.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Characterizations of discrete probability distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./022-product-measures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Product distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./021-independence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Independence and product spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./032-acfamilies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Absolutely continuous probability measures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./023-discrete-condition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Discrete Conditioning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-conditioning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Conditioning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-characterizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Characterizations of probability distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./041-characterization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Quantile functions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-convergences-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Convergences I : almost sure, <span class="math inline">\(L_2\)</span>, <span class="math inline">\(L_1\)</span>, in Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-convergence-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Convergence in distribution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-gaussian-vectors.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Gaussian vectors</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-secGaussUnivar" id="toc-sec-secGaussUnivar" class="nav-link active" data-scroll-target="#sec-secGaussUnivar"><span class="header-section-number">16.1</span> Univariate Gaussian distribution</a></li>
  <li><a href="#sec-GaussVec" id="toc-sec-GaussVec" class="nav-link" data-scroll-target="#sec-GaussVec"><span class="header-section-number">16.2</span> Gaussian vectors</a></li>
  <li><a href="#sec-convergenceGaussVectors" id="toc-sec-convergenceGaussVectors" class="nav-link" data-scroll-target="#sec-convergenceGaussVectors"><span class="header-section-number">16.3</span> Convergence of Gaussian vectors</a></li>
  <li><a href="#sec-gauss-conditioning" id="toc-sec-gauss-conditioning" class="nav-link" data-scroll-target="#sec-gauss-conditioning"><span class="header-section-number">16.4</span> Gaussian conditioning</a></li>
  <li><a href="#sec-about-gamma" id="toc-sec-about-gamma" class="nav-link" data-scroll-target="#sec-about-gamma"><span class="header-section-number">16.5</span> About Gamma distributions</a></li>
  <li><a href="#norms-of-centred-gaussian-vectors" id="toc-norms-of-centred-gaussian-vectors" class="nav-link" data-scroll-target="#norms-of-centred-gaussian-vectors"><span class="header-section-number">16.6</span> Norms of centred Gaussian vectors</a></li>
  <li><a href="#sec-norm-non-centered" id="toc-sec-norm-non-centered" class="nav-link" data-scroll-target="#sec-norm-non-centered"><span class="header-section-number">16.7</span> Norm of non-centred Gaussian vectors</a></li>
  <li><a href="#sec-cochran" id="toc-sec-cochran" class="nav-link" data-scroll-target="#sec-cochran"><span class="header-section-number">16.8</span> Cochran Theorem and consequences</a></li>
  <li><a href="#gaussian-concentration" id="toc-gaussian-concentration" class="nav-link" data-scroll-target="#gaussian-concentration"><span class="header-section-number">16.9</span> Gaussian concentration</a></li>
  <li><a href="#bibliographic-remarks" id="toc-bibliographic-remarks" class="nav-link" data-scroll-target="#bibliographic-remarks"><span class="header-section-number">16.10</span> Bibliographic remarks</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/s-v-b/MA1AY010-CN/edit/main/09-gaussian-vectors.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/s-v-b/MA1AY010-CN/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-chapGaussianVectors" class="quarto-section-identifier"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Gaussian vectors</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="sec-secGaussUnivar" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="sec-secGaussUnivar"><span class="header-section-number">16.1</span> Univariate Gaussian distribution</h2>
<p>The standard Gaussian density is denoted by <span class="math inline">\(\phi\)</span>: <span class="math display">\[
\phi(x) = \frac{\mathrm{e}^{- \frac{x^2}{2} }}{\sqrt{2 \pi}} \, .
\]</span> The corresponding cumulative distribution function is denoted by <span class="math inline">\(\Phi\)</span>. The survival function <span class="math inline">\(1- \Phi\)</span> is denoted by <span class="math inline">\(\overline{\Phi}\)</span>. <span class="math display">\[
\Phi(x) =  \int_{- \infty}^x \phi(u) \mathrm{d}u \, .
\]</span> Let is denote by <span class="math inline">\(\mathcal{N} (0, 1)\)</span> (expectation <span class="math inline">\(0\)</span>, variance <span class="math inline">\(1\)</span>) the standard Gaussian probability distribution, that is the probability distribution defined by <span class="math inline">\(\phi\)</span>.</p>
<p>Any affine transform of a standard Gaussian random variable is distributed according to a univariate Gaussian distribution. If <span class="math inline">\(X \sim \mathcal{N} (0, 1)\)</span> then <span class="math inline">\(\sigma X + \mu \sim
\mathcal{N} \left( \mu, \sigma^2 \right)\)</span> with density <span class="math inline">\(\frac{1}{\sigma}\phi\left(\frac{\cdot- \mu}{\sigma}\right)\)</span>, cumulative distribution function <span class="math inline">\(\Phi\left(\frac{\cdot - \mu}{\sigma}\right)\)</span>.</p>
<!-- ::: {#fig-plotdensity} -->
<!-- ::: -->
<!-- ::: {#fig-witgetdensity} -->
<!-- ::: -->
<p>The standard Gaussian distribution is characterized by the next identity.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="lem-steinslemma" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 16.1 (Stein’s Lemma)</strong></span> Let <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span>, let <span class="math inline">\(g\)</span> be an absolutely continuous function with derivative <span class="math inline">\(g'\)</span> such that <span class="math inline">\(\mathbb{E}[ |X g(X)|]&lt;\infty\)</span>, then <span class="math inline">\(g'(X)\)</span> is integrable and</p>
<p><span class="math display">\[
\mathbb{E}[g'(X)] = \mathbb{E}[Xg(X)] \, .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof relies on integration by parts. First note that replacing <span class="math inline">\(g\)</span> by <span class="math inline">\(g - g(0)\)</span> changes neither <span class="math inline">\(g'\)</span>, nor <span class="math inline">\(\mathbb{E}[Xg(X)]\)</span>. We may assume that <span class="math inline">\(g(0)=0\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[Xg(X)] &amp; = \int_{\mathbb{R}} xg(x) \phi(x) \mathrm{d}x \\
&amp; = \int_0^\infty xg(x) \phi(x) \mathrm{d}x + \int_{-\infty}^0 xg(x) \phi(x) \mathrm{d}x \\
&amp; = \int_0^\infty x \int_0^\infty g'(y) \mathbb{I}_{y\leq x}\mathrm{d}y \phi(x) \mathrm{d}x -\int^0_{-\infty} x \int^0_{-\infty} g'(y) \mathbb{I}_{y\geq x}\mathrm{d}y \phi(x) \mathrm{d}x\\
&amp; = \int_0^\infty g'(y) \int_0^\infty  \mathbb{I}_{y\leq x} x\phi(x)\mathrm{d}x  \mathrm{d}y -
\int_{-\infty}^0 g'(y) \int^0_{-\infty}  x \phi(x)\mathbb{I}_{y\geq x}\mathrm{d}x \mathrm{d}y  \\
&amp; = \int_0^\infty g'(y) \int_y^\infty x\phi(x)\mathrm{d}x  \mathrm{d}y -
\int_{-\infty}^0 g'(y) \int^y_{-\infty}  x \phi(x)\mathrm{d}x \mathrm{d}y  \\
&amp; = \int_0^\infty g'(y) \phi(y) \mathrm{d}y -
\int_{-\infty}^0 - g'(y) \phi(y)\mathrm{d}y \\
&amp; = \int_{-\infty}^\infty g'(y) \phi(y) \mathrm{d}y \, .
\end{align*}\]</span></p>
<p>The last inequality is justified by Tonelli-Fubini’s Theorem. Then, we rely on <span class="math inline">\(\phi'(x)=-x \phi(x)\)</span>.</p>
</div>
<p>The characteristic function is a very efficient tool when handling Gaussian distributions.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-gausscharfunc" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.1</strong></span> The characteristic function of <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span> is <span class="math display">\[
\widehat{\Phi}(t):= \mathbb{E}\left[\mathrm{e}^{\imath t X}\right] = \mathrm{e}^{\imath t \mu - \frac{t^2 \sigma^2}{2}}  \, .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>It is enough to check the proposition for <span class="math inline">\(\mathcal{N}(0,1)\)</span>. As <span class="math inline">\(\phi\)</span> is even,</p>
<p><span class="math display">\[\begin{eqnarray*}
\widehat{\Phi}(t) &amp;= &amp; \int_{-\infty}^{\infty} \mathrm{e}^{\imath t x} \frac{\mathrm{e}^{- \frac{x^2}{2}}}{\sqrt{2 \pi}} \mathrm{d} x \\
&amp; = &amp; \int_{-\infty}^{\infty} \cos(tx) \frac{\mathrm{e}^{- \frac{x^2}{2}}}{\sqrt{2 \pi}} \mathrm{d} x \, .
\end{eqnarray*}\]</span></p>
<p>Derivation with respect to <span class="math inline">\(t\)</span>, interchanging derivation and expectation (why can we do that?)</p>
<p><span class="math display">\[\begin{eqnarray*}
\widehat{\Phi}'(t)  &amp; = &amp;    \int_{-\infty}^{\infty} -x \sin(tx) \frac{\mathrm{e}^{- \frac{x^2}{2}}}{\sqrt{2 \pi}} \mathrm{d} x    \, .
\end{eqnarray*}\]</span> Now relying on Stein’s Identity with <span class="math inline">\(g(x)=-\sin(tx)\)</span> and <span class="math inline">\(g'(x)=-t\cos(tx)\)</span> <span class="math display">\[\begin{eqnarray*}
\widehat{\Phi}'(t)  &amp; = &amp;
- t \int_{-\infty}^{\infty} \cos(tx) \phi(x) \mathrm{d} x \\
&amp; = &amp; -t \widehat{\Phi}(t) \, .
\end{eqnarray*}\]</span></p>
<p>We immediately get <span class="math inline">\(\widehat{\Phi}(0)=1\)</span>, and solving the differential equation leads to <span class="math display">\[
\log \widehat{\Phi}(t) =  - \frac{t^2}{2} \, .
\]</span></p>
</div>
<p>The fact that the characteristic function completely defines the probability distribution provides us with a converse of <a href="#lem-steinslemma" class="quarto-xref">Lemma&nbsp;<span>16.1</span></a>.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="lem-steinslemmabis" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 16.2 (Stein’s Lemma (bis))</strong></span> Let <span class="math inline">\(X\)</span> be a real-valued random variable on some probability space. If, for any differentialle function <span class="math inline">\(g\)</span> such that <span class="math inline">\(g'\)</span> and <span class="math inline">\(x \mapsto xg(x)\)</span> are integrable, the following holds <span class="math display">\[
\mathbb{E}[g'(X)] = \mathbb{E}[X g(X)]
\]</span> then the distribution of <span class="math inline">\(X\)</span> is standard Gaussian</p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Consider the real <span class="math inline">\(\widehat{F}\)</span> and the imaginary part <span class="math inline">\(\widehat{G}\)</span> of the characteristic function of the distribution pf <span class="math inline">\(X\)</span>, the identity entails that <span class="math inline">\(\widehat{F}'(t) = -t \widehat{F}(t)\)</span> and <span class="math inline">\(\widehat{G}'(t) = -t \widehat{G}(t)\)</span> with <span class="math inline">\(\widehat{F}(0)=1\)</span> and <span class="math inline">\(\widehat{G}(0)=0\)</span>. Solving the two differential equations leads to <span class="math inline">\(\widehat{F}(t) = \mathrm{e}^{-t^2/2}\)</span> and <span class="math inline">\(\widehat{G}(t)=0\)</span>. We just checked that the characteristic function of the distribution of <span class="math inline">\(X\)</span> is the characteristic function of <span class="math inline">\(\mathcal{N}(0,1).\)</span></p>
</div>
<p>It is now easy to check that the distribution of the sum of two independent Gaussian random variables is a Gaussian random variable.</p>
<div id="prp">
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two independent random variables distributed according to <span class="math inline">\(\mathcal{N} (\mu, \sigma^2)\)</span> and <span class="math inline">\(\mathcal{N}
(\mu', \sigma^{\prime 2})\)</span> then <span class="math inline">\(X + Y\)</span> is distributed according to <span class="math inline">\(\text{$\mathcal{N} (\mu + \mu', \sigma^2 +
\sigma^{\prime 2})$}\)</span>.</p>
</div>
<div id="exr">
<p>Check and justify.</p>
</div>
<p>The moment generating function of a Gaussian random variable is given by <span class="math display">\[
s \mapsto \mathbb{E} \left[ \mathrm{e}^{s X} \right] =
\text{e}^{\frac{s^2}{2}} \,  .
\]</span></p>
<p>From Markov’s inequality, we obtain interesting upper bounds on the Gaussian tail function. Some calculus allows us to refine the tail bounds</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-millsratio" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.2 (Tail probabilities for Gaussian distribution)</strong></span> For <span class="math inline">\(x \geq 0,\)</span> <span class="math display">\[
\frac{\phi(x)}{x} \left( 1
- \frac{1}{x^2} \right)  \leq \overline{\Phi} (x) \leq \min \left( \mathrm{e}^{-\frac{x^2}{2}},
\frac{\phi(x)}{x} \right)\, .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof boils down to repeated integration by parts.</p>
<p><span class="math display">\[\begin{eqnarray*}
\overline{\Phi}(x) &amp; = &amp; \int_x^{\infty}  \frac{1}{ \sqrt{2
\pi}} \mathrm{e}^{- \frac{u^2}{2}} \mathrm{d} u\\
&amp; = &amp; \left[ -  \frac{1}{ \sqrt{2 \pi} u} \mathrm{e}^{-
\frac{u^2}{2}} \right]^{\infty}_x - \int_x^{\infty}  \frac{1}{
\sqrt{2 \pi}}  \frac{1}{u^2} \mathrm{e}^{- \frac{u^2}{2}} \mathrm{d} u .
\end{eqnarray*}\]</span></p>
<p>As the second term is non-positive,</p>
<p><span class="math display">\[
\overline{\Phi}(x)\leq  \left[ -  \frac{1}{ \sqrt{2
\pi} u} \mathrm{e}^{- \frac{u^2}{2}} \right]^{\infty}_x =  \frac{\phi(x)}{
x}  .
\]</span></p>
<p>This is the first part of the right-hand inequality, the other part comes from Markov’s inequality. For the left-hand inequality, we have to upper bound <span class="math inline">\(\int_x^{\infty}  \frac{1}{ \sqrt{2 \pi}}  \frac{1}{u^2} \mathrm{e}^{- \frac{u^2}{2}} \mathrm{d} u\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
\int_x^{\infty}  \frac{1}{ \sqrt{2 \pi}}  \frac{1}{u^2} \mathrm{e}^{-
\frac{u^2}{2}} \mathrm{d} u &amp; = &amp; \left[ \frac{- 1}{ \sqrt{2 \pi}}
\frac{1}{u^3} \mathrm{e}^{- \frac{u^2}{2}} \right]_x^{\infty} -
\int_x^{\infty}  \frac{1}{ \sqrt{2 \pi}}  \frac{3}{u^4} \mathrm{e}^{-
\frac{u^2}{2}} \mathrm{d} u\\
&amp; \leq  &amp; \frac{1}{ \sqrt{2 \pi}}  \frac{1}{x^3} \mathrm{e}^{-
\frac{x^2}{2}} .
\end{eqnarray*}\]</span></p>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-gaussmoments" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.3 (Moments)</strong></span> For a standard Gaussian random variable, <span class="math display">\[
\mathbb{E} \left[ X^k \right] =
=
\begin{cases}
0 &amp;  \text{ if } k \text{ is odd}\\
\frac{k!}{2^{k / 2} (k / 2) !} = \frac{\Gamma (k + 1)}{2^{k / 2}
\Gamma (k / 2 + 1)} &amp; \text{ if }  k \text{ is even.}
\end{cases}
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Thanks to distributional symmetry, <span class="math inline">\(\mathbb{E} \left[ X^k \right]=0\)</span> for all odd<span class="math inline">\(k\)</span>. We handle even powers using integration by parts:</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathbb{E} \left[ X^{k+2} \right] &amp; = &amp; (k+1) \mathbb{E} \left[ X^{k} \right]  \, .
\end{eqnarray*}\]</span></p>
<p>Induction on <span class="math inline">\(k\)</span> leads to,</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathbb{E} \left[ X^{2k} \right] &amp;= &amp; \prod_{j=1}^k (2j-1) =  \frac{(2k) !}{2^k k! } \, .
\end{eqnarray*}\]</span></p>
</div>
<p>Note that <span class="math inline">\((2k)!/(2^k k!)\)</span> is also the number of partitions of <span class="math inline">\(\{1, \ldots, 2k\}\)</span> into subsets of cardinality <span class="math inline">\(2\)</span>.</p>
<p>The <em>skewness</em> is null, the kurtosis (ratio of fourth centred moment over squared variance equals <span class="math inline">\(3\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}[X^4] =  3 \times  \mathbb{E}[X^2]^2 \, .
\]</span></p>
</section>
<section id="sec-GaussVec" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="sec-GaussVec"><span class="header-section-number">16.2</span> Gaussian vectors</h2>
<p>A Gaussian vector is a collection of univariate Gaussian random variables that satisfies a very stringent property:</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-gaussVector" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 16.1 (Gaussian Vector)</strong></span> A random vector <span class="math inline">\((X_1, \ldots, X_n)^\top\)</span> is a <em>Gaussian vector</em> iff for any real vector <span class="math inline">\((\lambda_1, \lambda_2, \ldots, \lambda_n)^\top\)</span>, the distribution of the univariate random variable <span class="math inline">\(\sum_{i = 1}^n \ \lambda_i X_i\)</span> is Gaussian.</p>
</div>
</div>
</div>
</div>
<div id="rem-caution-gaussian" class="proof remark">
<p><span class="proof-title"><em>Remark 16.1</em>. </span>Not every collection of Gaussian random variables forms a Gaussian vector.</p>
<p>The random vector <span class="math inline">\((X, \epsilon X)\)</span> with <span class="math inline">\(X \sim \mathcal{N}(0.1)\)</span>, independent of <span class="math inline">\(\epsilon\)</span> which is worth <span class="math inline">\(\pm 1\)</span> with probability <span class="math inline">\(1/2\)</span>, is not a Gaussian vector although both <span class="math inline">\(X\)</span> and <span class="math inline">\(\epsilon X\)</span> are univariate Gaussian random variables.</p>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exr-caution-gaussian" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 16.1</strong></span> Check that <span class="math inline">\(\epsilon X\)</span> is a Gaussian random variable.</p>
</div>
</div>
</div>
</div>
<p>Yet there are Gaussian vectors! A simple way to obtain a Gaussian vector is provided by the next proposition (checked by a characteristic function argument).</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-indep-gaussian-vectors" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.4</strong></span> If <span class="math inline">\(X_1, \ldots, X_n\)</span> is a sequence of independent Gaussian random variables, then <span class="math inline">\((X_1, \ldots, X_n)^\top\)</span> is a Gaussian vector.</p>
</div>
</div>
</div>
</div>
<p>In the sequel, a <em>standard Gaussian vector</em> is a random vector with independent coordinates with each coordinate distributed according to <span class="math inline">\(\mathcal{N}(0,1)\)</span>.</p>
<p>We will see how to construct general Gaussian vectors. Before this, let us check that the joint distribution of a Gaussian random vector is completely characterized by its covariance matrix and its expectation vector.</p>
<p>Recall that the <em>covariance</em> of random vector <span class="math inline">\(X= (X_1, \ldots, X_n)^\top\)</span> is the matrix <span class="math inline">\(K\)</span> with dimension <span class="math inline">\(n \times n\)</span> with coefficients</p>
<p><span class="math display">\[
K [i, j] = \operatorname{Cov} (X_i, X_j) = \mathbb{E} [X_i X_j] - \mathbb{E} [X_i] \mathbb{E} [X_j] .
\]</span></p>
<p>Without loss of generality, we may assume that random vector <span class="math inline">\(X\)</span> is centered For every <span class="math inline">\(\lambda = (\lambda_1, \ldots, \lambda_n)^\top \in \mathbb{R}^n\)</span>, we have:</p>
<p><span class="math display">\[
\operatorname{var}(\langle \lambda, X \rangle) =  \lambda^\top K \lambda = \text{trace} (K \lambda \lambda^\top)\,
\]</span> (this is does not depend on any Gaussianity assumption).</p>
<p>Indeed,</p>
<p><span class="math display">\[\begin{eqnarray*}
\operatorname{var}(\langle \lambda, X \rangle) &amp; = &amp;
\mathbb{E} \left[ \left( \sum_{i=1}^n \lambda_i X_i\right)^2\right]  \\
&amp; = &amp; \sum_{i,j=1}^n \mathbb{E} \left[\lambda_i \lambda_j X_i X_j \right] \\
&amp; = &amp; \sum_{i,j=1}^n \lambda_i \lambda_j K[i,j] \\
&amp; = &amp; \lambda^\top K \lambda\, .
\end{eqnarray*}\]</span></p>
<p>The characteristic function of a Gaussian vector <span class="math inline">\(X\)</span> with expectation vector <span class="math inline">\(\mu\)</span> and covariance <span class="math inline">\(K\)</span> satisfies</p>
<p><span class="math display">\[
\mathbb{E} \mathrm{e}^{\imath \langle \lambda, X \rangle } =  \mathrm{e}^{\imath \langle \lambda, \mu \rangle - \frac{\lambda^\top K \lambda}{2}} \,  \, .
\]</span></p>
<p>A linear transform of a Gaussian vector is a Gaussian vector.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-linear-transform-gaussian" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.5</strong></span> If <span class="math inline">\({Y} = (Y_1, \ldots, Y_n)^\top\)</span> is a Gaussian vector with covariance <span class="math inline">\(K\)</span> and <span class="math inline">\(A\)</span> a real matrix with dimensions <span class="math inline">\(p \times n\)</span>, then <span class="math inline">\(A \times Y\)</span> is Gaussian vector with expectation <span class="math inline">\(A \times \mathbb{E}Y\)</span> and covariance matrix</p>
<p><span class="math display">\[A K A^\top .\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Without loss of generality, we assume <span class="math inline">\(Y\)</span> is centred.</p>
<p>For any <span class="math inline">\(\lambda \in \mathbb{R}^p\)</span>, <span class="math inline">\(\langle \lambda , A Y \rangle = \langle A^\top \lambda, Y \rangle\)</span> , thus <span class="math inline">\(A \times Y\)</span> is Gaussian with variance</p>
<p><span class="math display">\[
\lambda^\top A K A^\top \lambda \, .
\]</span></p>
<p>The covariance of <span class="math inline">\(A \times Y\)</span> is determined by this observation.</p>
</div>
<p>To manufacture Gaussian vectors with general covariance matrices, we rely on an important notion from matrix analysis.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-sdp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 16.2 (Semi-Definite Positive matrices)</strong></span> A symmetric matrix <span class="math inline">\(M\)</span> with dimensions <span class="math inline">\(k \times k\)</span> is Definite Positive (respectively Semi-Definite Positive) iff, for any non-null vector <span class="math inline">\(v \in \mathbb{R}^k\)</span>,</p>
<p><span class="math display">\[
v^\top M v &gt; 0 \qquad (\text{resp.} \qquad v^\top M v \geq 0) \, .
\]</span></p>
<p>We denote by <span class="math inline">\(\textsf{dp}(k)\)</span> (resp. <span class="math inline">\(\textsf{sdp}(k)\)</span>), the cones of Definite Positive (resp. Semi-Definite Positive) matrices.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-sdp-cov" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.6</strong></span> If <span class="math inline">\(K\)</span> is the covariance matrix of a random vector, <span class="math inline">\(K\)</span> is symmetric, Semi-Definite Positive.</p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(X\)</span> is a <span class="math inline">\(\mathbb{R}^k\)</span>-valued random vector, with covariance <span class="math inline">\(K\)</span>, for any vector <span class="math inline">\(\lambda \in \mathbb{R}^n\)</span>,</p>
<p><span class="math display">\[
\lambda^\top K \lambda = \sum_{i,j\leq k} K_{i,j} \lambda_i \lambda_j = \operatorname{cov}(\langle \lambda, X \rangle, \langle \lambda, X \rangle)
\]</span> soit <span class="math inline">\(\lambda^\top K \lambda =  \operatorname{var}(\langle \lambda, X \rangle)\)</span>. The variance of a univariate random variable is always non-negative.</p>
</div>
<p>The next observation is the key to the construction to general Gaussian vectors.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-choleskyfac" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.7 (Cholesky’s factorization)</strong></span> If <span class="math inline">\(A\)</span> is a Semi-definite Positive symmetric matrix then there exists (at least) a real matrix <span class="math inline">\(B\)</span> such that <span class="math inline">\(A = B^\top B\)</span>.</p>
</div>
</div>
</div>
</div>
<p>We do not check this proposition. This is a basic Theorem from matrix analysis. It can be established from the <em>spectral decomposition theorem</em> for symmetric matrices. It can also be established by a simple constructive approach: a positive definite matrix <span class="math inline">\(K\)</span> admits a <em>Cholesky decomposition</em>, in other words, there exists a triangular matrix lower than <span class="math inline">\(L\)</span> such that <span class="math inline">\(K = L \times L^\top\)</span>.</p>
<p>The next proposition is a corollary of the general formula for image densities.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-sdp-gaussian-density" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.8</strong></span> If <span class="math inline">\(A\)</span> is a symmetric positive definite matrix (<span class="math inline">\(A \in \textsf{dp}(n)\)</span>), then the distribution of the centred Gaussian vector with covariance matrix <span class="math inline">\(A\)</span> is absolutely continuous with respect to Lebesgue’s measure on <span class="math inline">\(\mathbb{R}^n\)</span>: <span class="math display">\[
\frac{1}{({2 \pi})^{n/2}  \operatorname{det}(A)^{1/2}} \exp\left( - \frac{x^\top A^{-1} x}{2} \right) \, .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The density formula is trivially correct for standard Gaussian vectors. For the general case, it is enough to invoke the image density formula to the image of the standard Gaussian vector by the bijective linear transformation defined by the Cholesky factorization of <span class="math inline">\(A\)</span>. The determinant of the Cholesky factor is the square root of the determinant of <span class="math inline">\(A\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exr-sing-covariance" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 16.2</strong></span> Is the distribution of a Gaussian vector <span class="math inline">\(X\)</span> with <em>singular</em> covariance matrix absolutely continuous with respect to Lebesgue measure?</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-gaussspace" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 16.3 (Gaussian space)</strong></span> If <span class="math inline">\(X= (X_1, \ldots, X_n)^\top\)</span> is a centered Gaussian vector with covariance matrix <span class="math inline">\(K\)</span> , the set <span class="math inline">\(\left\{ \sum_{i = 1}^n \lambda_i X_i = \langle \lambda, X\rangle ; \lambda \in \mathbb{R}^n\right\}\)</span> is the Gaussian space generated by <span class="math inline">\(X = (X_1, \ldots, X_n)^\top\)</span>).</p>
</div>
</div>
</div>
</div>
<p>The Gaussian space is a real vector space. If <span class="math inline">\((\Omega, \mathcal{F},P)\)</span> denotes the probability space, <span class="math inline">\(X\)</span> lives on, the Gaussian space is a subspace of <span class="math inline">\(L^2_{\mathbb{R}}(\Omega, \mathcal{F},P)\)</span>. It inherits the inner product structure from <span class="math inline">\(L^2_{\mathbb{R}}(\Omega, \mathcal{F},P)\)</span>.</p>
<p>This inner-product is completely defined by the covariance matrix <span class="math inline">\(K\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
\left\langle \sum_{i = 1}^n \lambda_i X_i, \sum_{i = 1}^n \lambda'_i X_i
\right\rangle &amp; \equiv &amp; \mathbb{E}_P \left[ \left( \sum_{i = 1}^n
\lambda_i X_i \right) \left( \sum_{i = 1}^n \lambda'_i X_i \right) \right]\\
&amp; = &amp; \sum^n_{i, i' = 1} \lambda_i \lambda_{i'}'  K [i, i']\\
&amp; = &amp; (\lambda_1, \ldots, \lambda_n) K \left(\begin{array}{c}
\lambda'_1\\
\vdots\\
\lambda'_n
\end{array}\right) \\
&amp; = &amp; \text{trace} \left( K \left(\begin{array}{c}
\lambda_1\\
\vdots\\
\lambda_n
\end{array}\right) \left(\begin{array}{ccc}
\lambda'_1 &amp;
\dots &amp;
\lambda'_n
\end{array}\right) \right).
\end{eqnarray*}\]</span></p>
<div id="rem-diff-gaussian-vectors" class="proof remark">
<p><span class="proof-title"><em>Remark 16.2</em>. </span>Different Gaussian vectors may generate the same Gaussian space. Explain how and why.</p>
</div>
<p>Gaussian spaces enjoy remarkable properties. Independence of random variables belonging to the same Gaussian space may checked very easily.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-indepgaussspace" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.9</strong></span> Two random variables <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span>, belonging to the same Gaussian space, are independent iff they are orthogonal (or decorrelated), that is iff <span class="math display">\[
\operatorname{Cov}_P [Y ,Z] = \mathbb{E}_P [Y Z] = 0 .
\]</span></p>
</div>
</div>
</div>
</div>
<p>Without loss of generality, we assume covariance matrix <span class="math inline">\(K\)</span> is positive definite.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Independence always implies orthogonality.</p>
<p>Without loss of generality, we assume that the Gaussian space is generated by a standard Gaussian vector, let <span class="math inline">\(Z = \sum_{i = 1}^n \lambda_i X_i\)</span> and <span class="math inline">\(Y = \sum_{i = 1}^n \lambda'_i X_i\)</span>.</p>
<p>If <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> are orthogonal (or non-correlated)</p>
<p><span class="math display">\[\mathbb{E} [ZY] =  \sum_{i = 1}^n \lambda_i \lambda_{i}' = 0 .\]</span></p>
<p>To show that <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> are independent, it is enough to check that for all <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\mu'\)</span> in <span class="math inline">\(\mathbb{R}\)</span> <span class="math display">\[\mathbb{E} \left[ \mathrm{e}^{\imath \mu Z} \mathrm{e}^{\imath \mu' Y} \right] =
\mathbb{E} \left[ \mathrm{e}^{\imath \mu Z}  \right] \times \mathbb{E}
\left[ \mathrm{e}^{\imath \mu' Y} \right] .
\]</span></p>
<p><span class="math display">\[\begin{align*}
\mathbb{E} \left[ \mathrm{e}^{\imath \mu Z} \mathrm{e}^{\imath \mu' Y} \right] &amp;
=  \mathbb{E} \left[ \mathrm{e}^{\imath \mu \sum_i \lambda_i    X_i} \mathrm{e}^{\imath \mu' \sum_i \lambda'_i    X_i} \right]\\
&amp; =  \mathbb{E} \left[ \prod_{i = 1}^n    \mathrm{e}^{\imath (\mu
\lambda_i + \mu' \lambda'_i)    X_i} \right]\\
&amp;   X_i \text{ are  independent} \ldots\\
&amp; =  \prod_{i = 1}^n  \mathbb{E} \left[ \mathrm{e}^{\imath (\mu \lambda_i +
\mu' \lambda'_i)    X_i} \right]\\
&amp; =  \prod_{i = 1}^n \mathrm{e}^{- (\mu \lambda_i + \mu' \lambda'_i)
^2 / 2}\\
&amp; =  \exp \left( - \frac{1}{2} \sum_{i = 1}^n \mu^2 \lambda_i^2 + 2 \mu
\mu' \lambda_i \lambda'_i + \mu'^2 \lambda'^2_i    \right)\\
&amp;   \text{orthogonality}\\
&amp; =  \exp \left( - \frac{1}{2} \sum_{i = 1}^n \mu^2 \lambda_i^2 + \mu'^2
\lambda'^2_i    \right)\\
&amp;   \ldots\\
&amp; =  \mathbb{E} \left[ \mathrm{e}^{\imath \mu Z}  \right] \times
\mathbb{E} \left[ \mathrm{e}^{\imath \mu' Y} \right] .
\end{align*}\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<p>The next proposition is a direct consequence.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="cor-indepemdence-correlation" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 16.1</strong></span> If <span class="math inline">\(E\)</span> and <span class="math inline">\(E'\)</span> are two linear sub-spaces of the Gaussian space generated by the Gaussian vector with independent coordinates <span class="math inline">\(X_1, \ldots, X_n\)</span>, the (Gaussian) random variables belonging to subspace <span class="math inline">\(E\)</span> and the random (Gaussian) variables belonging to the <span class="math inline">\(E'\)</span> space are independent if and only these two subspaces are orthogonal.</p>
</div>
</div>
</div>
</div>
</section>
<section id="sec-convergenceGaussVectors" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="sec-convergenceGaussVectors"><span class="header-section-number">16.3</span> Convergence of Gaussian vectors</h2>
<p>Recall the Lévy continuity theorem (<a href="08-convergence-3.html#thm-thmlevycont" class="quarto-xref">Theorem&nbsp;<span>15.2</span></a>)), it relates weak convergence for probability measures and simple convergence for characteristic functions.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-levycont-recall" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.1</strong></span> A sequence of probability distributions <span class="math inline">\((P_n)_{n \in \mathbb{N}}\)</span> over <span class="math inline">\(\mathbb{R}^k\)</span> converges weakly towards <span class="math inline">\(P\)</span> iff for every <span class="math inline">\(\vec{s} \in \mathbb{R}^k\)</span>:</p>
<p><span class="math display">\[\mathbb{E}_{P_n} \left[ \mathrm{e}^{\imath \langle \vec{s},
\vec{X} \rangle} \right] \rightarrow \mathbb{E}_{P_{}} \left[
\mathrm{e}^{\imath \langle \vec{s}, \vec{X} \rangle} \right] .
\]</span></p>
</div>
</div>
</div>
</div>
<p>For every <span class="math inline">\(\vec{s} \in \mathbb{R}^k\)</span>, functions <span class="math inline">\(\vec{x}\mapsto \cos ( \left\langle \vec{s}, \vec{x} \right\rangle)\)</span> and <span class="math inline">\(\vec{x} \mapsto \sin ( \left\langle \vec{s}, \vec{x}\right\rangle)\)</span> are continuous and bounded, They are also infinitely many times differentiable.</p>
<p>It is remarkable and useful that weak convergence can be checked on this small set of functions.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-levycont-bis" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.2 (Lévy-Continuity Theorem (bis))</strong></span> A sequence of probability distributions <span class="math inline">\((P_n)_{n \in \mathbb{N}}\)</span> sur <span class="math inline">\(\mathbb{R}^k\)</span> converges weakly towards a probability distribution iff there exists a function <span class="math inline">\(f\)</span> over <span class="math inline">\(\mathbb{R}^k\)</span>, continuous at <span class="math inline">\(\vec{0}\)</span>, such that for all <span class="math inline">\(\vec{s} \in \mathbb{R}^k\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}_{P_n} \left[ \mathrm{e}^{\imath \langle \vec{s}, \vec{X} \rangle} \right] \rightarrow f(\vec{s})\,.
\]</span></p>
<p>Then, function <span class="math inline">\(f\)</span> is the characteristic function of some probability distribution <span class="math inline">\(P\)</span>.</p>
</div>
</div>
</div>
</div>
<p>The continuity condition at <span class="math inline">\(0\)</span> is necessary. The characteristic function of a probability distribution is always continuous at <span class="math inline">\(0\)</span>. Continuity at <span class="math inline">\(0\)</span> warrants the tightness of the sequence of probability distributions.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-conv-gaussiamn" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.10</strong></span> If a sequence of <span class="math inline">\(k\)</span>-dimensional Gaussian vectors <span class="math inline">\((X_n)\)</span> is defined by a <span class="math inline">\(\mathbb{R}^k\)</span>-valued sequence <span class="math inline">\((\vec{\mu}_n)_n\)</span> and a <span class="math inline">\(\textsf{SDP}(k)\)</span>-valued sequence <span class="math inline">\((K_n)_n\)</span> and</p>
<p><span class="math display">\[\begin{eqnarray*}
\lim_n \vec{\mu}_n &amp; = &amp; \mu \in \mathbb{R}^k\\
\lim_n K_n &amp; = &amp; K \in \textsf{SDP}(k)
\end{eqnarray*}\]</span></p>
<p>then the sequence <span class="math inline">\((X_n)_n\)</span> converges in distribution towards <span class="math inline">\(\mathcal{N}\left(\vec{\mu}, K\right)\)</span> (if <span class="math inline">\(K = 0\)</span>, the limit distribution is <span class="math inline">\(\delta_\mu\)</span>).</p>
</div>
</div>
</div>
</div>
</section>
<section id="sec-gauss-conditioning" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="sec-gauss-conditioning"><span class="header-section-number">16.4</span> Gaussian conditioning</h2>
<p>Let <span class="math inline">\((X_1,\ldots,X_n)^\top\)</span> be a Gaussian vector with distribution <span class="math inline">\(\mathcal{N}(\mu, K)\)</span> where <span class="math inline">\(K \in \textsf{DP}(n)\)</span>. The covariance matrix <span class="math inline">\(K\)</span> is partitioned into blocks <span class="math display">\[
K = \left[
\begin{array}{cc}
A &amp; B^\top \\
B &amp; W
\end{array}
\right]
\]</span></p>
<p>where <span class="math inline">\(A \in \textsf{DP}(k)\)</span>, <span class="math inline">\(1 \leq k &lt; n\)</span>, and <span class="math inline">\(W \in \textsf{DP}(n-k)\)</span>.</p>
<p>We are interested in the conditional expectation of <span class="math inline">\((X_1, \ldots, X_k)^\top\)</span> with repsect to <span class="math inline">\(\sigma(X_{k+1},\ldots,X_n)\)</span> and in the conditional distribution of <span class="math inline">\((X_1, \ldots, X_k)^\top\)</span> with respect to <span class="math inline">\(\sigma(X_{k+1},\ldots,X_n)\)</span>.</p>
<p>The Schur complement of <span class="math inline">\(A\)</span> in <span class="math inline">\(K\)</span> is defined as <span class="math display">\[
W - B A^{-1} B^\top\, .
\]</span></p>
<p>This definition makes sense for symmetric matrices when <span class="math inline">\(A\)</span> is non-singular.</p>
<p>If <span class="math inline">\(K \in \textsf{DP}(n)\)</span> then the Schur complement of <span class="math inline">\(A\)</span> in <span class="math inline">\(K\)</span> also belongs to <span class="math inline">\(\textsf{DP}(n-k)\)</span></p>
<p>In the statement of the next theorems, <span class="math inline">\(A^{-1/2}\)</span> denotes the Cholesky factor of <span class="math inline">\(A^{-1}\)</span>: <span class="math inline">\(A^{-1} = A^{-1/2} \times (A^{-1/2})^\top\)</span>.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-cond-expectation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.3</strong></span> L’espérance conditionnelle de <span class="math inline">\((X_{k+1}, \ldots, X_n)^\top\)</span> sachant <span class="math inline">\((X_{1},\ldots,X_k)^\top\)</span> est une transformation affine de <span class="math inline">\((X_{1},\ldots,X_{k})^\top\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}\left[ \begin{pmatrix}
X_{k+1} \\ \vdots \\ X_{n}
\end{pmatrix} \mid \begin{matrix}
X_{1} \\ \vdots \\ X_k
\end{matrix}\right] = \begin{pmatrix}
\mu_{k+1} \\ \vdots \\ \mu_n \end{pmatrix}   + \left(B A^{-1}  \right) \times  \left( \begin{pmatrix}
X_{1} \\ \vdots \\ X_{k}
\end{pmatrix}  - \begin{pmatrix}
\mu_{1} \\ \vdots \\ \mu_k
\end{pmatrix}\right) \, .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-cond-gaussian-density" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.4</strong></span> The conditional distribution of <span class="math inline">\((X_{k+1}, \ldots, X_n)^\top\)</span> with respect to <span class="math inline">\(\sigma(X_{1},\ldots,X_k)\)</span> is a Gaussian distribution whose expectation is the conditional expectation <span class="math inline">\((X_{k+1}, \ldots, X_n)^\top\)</span> with respect to <span class="math inline">\(\sigma(X_{1},\ldots,X_k)\)</span> and whose variance is the Schur complement of the covariance of <span class="math inline">\((X_{1},\ldots,X_k)^\top\)</span> in the covariance matrix of <span class="math inline">\((X_1, \ldots, X_n)^\top\)</span>.</p>
</div>
</div>
</div>
</div>
<p>We will first study the conditional density, and, with a minimum amount of calculation, establish that it is Gaussian. Conditional expectation will be calculated as expectation under conditional distribution.</p>
<p>To characterize conditional density, we rely on a distributional representation argument (any Gaussian vector is distributed as the image of a standard Gaussian vector by an affine transformation) and a matrix analysis result that is at the core of the Cholesky factorization of positive semi-definite matrices.</p>
<p><span class="math inline">\((X_1, \ldots, X_n)^\top\)</span> is distributed as the image of standard Gaussian vector by a block triangular matrix</p>
<p>, et utiliser des propriétés des lois conditionnelles pour établir à la fois les deux résultats.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-schur-complement" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.11</strong></span> Let <span class="math inline">\(K\)</span> be a symmetric definite positive matrix with dimensions <span class="math inline">\(n \times n\)</span></p>
<p><span class="math display">\[
K = \left[
\begin{array}{cc}
A &amp; B^\top \\
B &amp; W
\end{array}
\right]
\]</span></p>
<p>where <span class="math inline">\(A\)</span> has dimensions <span class="math inline">\(k \times k\)</span>, <span class="math inline">\(1 \leq k &lt; n\)</span>.</p>
<p>Then, the Schur-complement of <span class="math inline">\(A\)</span> with respect to <span class="math inline">\(K\)</span></p>
<p><span class="math display">\[
W - B A^{-1} B^\top
\]</span></p>
<p>is positive definite. Sub-matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(W - B A^{-1} B^\top\)</span> both have a Cholesky decomposition $A = L_1 L_1^<span class="math inline">\(,\)</span>W - B A^{-1} B^= L_2 L_2^$ where <span class="math inline">\(L_1, L_2\)</span> are lower triangular, and <span class="math inline">\(K\)</span>’s factorization reads like:</p>
<p><span class="math display">\[
K = \left[
\begin{array}{cc}
L_1 &amp; 0 \\
B (L_1^\top)^{-1}  &amp; L_2
\end{array}
\right] \times \left[
\begin{array}{cc}
L_1^\top &amp; L_1^{-1} B^\top \\
0 &amp; L_2^\top
\end{array}
\right] \, .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Without loss of generality, we check the statement on centered vectors. The Cholesky factorization of <span class="math inline">\(K\)</span> allows us to write</p>
<p><span class="math display">\[
\begin{pmatrix}
X_1 \\
\vdots \\
X_n
\end{pmatrix} \sim \left[
\begin{array}{cc}
L_1 &amp; 0 \\
B (L_1^\top)^{-1}  &amp; L_2
\end{array}
\right] \times \begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix}
\]</span></p>
<p>where <span class="math inline">\(( Y_1,
\ldots,
Y_n)^\top\)</span> is a centered standard Gaussian vector.</p>
<p>In the sequel, we assume <span class="math inline">\((X_1, \ldots,X_n)^\top\)</span> and <span class="math inline">\((Y_1,\ldots,Y_n)^\top\)</span> live on the same probability space. As <span class="math inline">\(L_1\)</span> is invertible, the <span class="math inline">\(\sigma\)</span>-algebras generated by <span class="math inline">\((X_1, \ldots,X_k)^\top\)</span> and <span class="math inline">\((Y_1, \ldots,Y_k)^\top\)</span> are equal. We agree on <span class="math inline">\(\mathcal{G}=\sigma(X_1, \ldots,X_k)\)</span>. The conditional expectations and conditional distributions also coincide .</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathbb{E} \left[ \begin{pmatrix}
X_{k+1} \\
\vdots \\
X_n
\end{pmatrix} \mid \mathcal{G} \right] &amp;= &amp;\mathbb{E} \left[ B (L_1^\top)^{-1} \begin{pmatrix}
Y_{1} \\
\vdots \\
Y_k
\end{pmatrix} \mid \mathcal{G} \right] + \mathbb{E} \left[ L_2 \begin{pmatrix}
Y_{k+1} \\
\vdots \\
Y_n
\end{pmatrix} \mid \mathcal{G} \right]  \\
&amp; = &amp; B (L_1^\top)^{-1} L_1^{-1}\begin{pmatrix}
X_{1} \\
\vdots \\
X_k
\end{pmatrix} =  B A^{-1} \begin{pmatrix}
X_{1} \\
\vdots \\
X_k
\end{pmatrix} \, ,
\end{eqnarray*}\]</span></p>
<p>car <span class="math inline">\((Y_{k+1}, \ldots,Y_n)^\top\)</span> is centered and independent from <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p>Note that residuals</p>
<p><span class="math display">\[
\begin{pmatrix}
X_{k+1} \\
\vdots \\
X_n
\end{pmatrix} -\mathbb{E} \left[ \begin{pmatrix}
X_{k+1} \\
\vdots \\
X_n
\end{pmatrix} \mid \mathcal{G} \right] = L_2 \begin{pmatrix}
Y_{k+1} \\
\vdots \\
Y_n
\end{pmatrix}
\]</span></p>
<p>are independent from <span class="math inline">\(\mathcal{G}\)</span>. This is a Gaussian property. For general square integrable random variables, we may only assert that residuals are orthogonal to <span class="math inline">\(\mathcal{G}\)</span>-measurable random variables.</p>
<p>The conditional distribution of <span class="math inline">\((X_{k+1},\ldots, X_n)^\top\)</span> with respect to <span class="math inline">\((X_1,\ldots, X_k)^\top\)</span> coincides with the conditional distribution of</p>
<p><span class="math display">\[
B (L_1^\top)^{-1}   \times
\begin{pmatrix}
Y_1\\ \vdots \\ Y_k
\end{pmatrix} + L_2 \times
\begin{pmatrix}
Y_{k+1}\\ \vdots \\ Y_n
\end{pmatrix}
\]</span></p>
<p>conditionally on <span class="math inline">\((Y_1,\ldots, Y_k)^\top\)</span>. As <span class="math inline">\((Y_1,\ldots, Y_k)^\top = L_1^{-1}(X_1,\ldots,X_k)^\top\)</span>, the conditional distribution we are looking for is Gaussian with expectation <span class="math display">\[B A^{-1}   \times
\begin{pmatrix}
X_1\\ \vdots \\ X_k
\end{pmatrix}\]</span> (the conditional expectation) and variance <span class="math inline">\(L_2 \times L_2^\top = W - B A^{-1} B^\top\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<div id="exm-conditional-gaussian" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.1</strong></span> If <span class="math inline">\((X,Y)^\top\)</span> is a centered Gaussian vector with <span class="math inline">\(\operatorname{var}(X)=\sigma_x^2\)</span>, <span class="math inline">\(\operatorname{var}(Y)=\sigma^2_y\)</span> and <span class="math inline">\(\operatorname{cov}(X,Y)= \rho \sigma_x \sigma_y\)</span>, the conditional distribution of <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
\mathcal{N}\left( \rho \sigma_y/\sigma_x X, \sigma^2_y (1- \rho^2)  \right) \, .
\]</span></p>
<p>The quantity <span class="math inline">\(\rho\)</span> is called the <em>linear correlation coefficient</em> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. By Cauchy-Schwarz’s inequality, <span class="math inline">\(\rho \in [-1,1]\)</span>.</p>
</div>
<p>These two theorems are usually addressed in the order in which they are stated. Conditional expectation is characterized by adopting the <span class="math inline">\(L^2\)</span> (predictive) viewpoint: the conditional expectation of the random vector <span class="math inline">\(Y\)</span> knowing <span class="math inline">\(X\)</span> is defined as the best <span class="math inline">\(X\)</span>-measurable predictor of the vector <span class="math inline">\(Y\)</span> with respect to quadratic error (the random vector <span class="math inline">\(Z\)</span>, <span class="math inline">\(X\)</span>-measurable that minimizes <span class="math inline">\(\mathbb{E} \left[ \| Y- Z\|^2 \right]\)</span>).</p>
<p>In order to characterize conditional expectation, we first compute the optimal affine predictor of <span class="math inline">\((X_{k+1},\ldots,X_n)^\top\)</span> based on <span class="math inline">\((X_{1},\ldots,X_k)^\top\)</span>. This optimal affine predictor is</p>
<p><span class="math display">\[
\begin{pmatrix}
\mu_{k+1} \\ \vdots \\ \mu_n \end{pmatrix}   + \left(B A^{-1}  \right) \times  \left( \begin{pmatrix}
X_{1} \\ \vdots \\ X_{k}
\end{pmatrix}  - \begin{pmatrix}
\mu_{1} \\ \vdots \\ \mu_k
\end{pmatrix}\right) \, ,
\]</span></p>
<p>(if Gaussian vectors are centred, this amounts to determine the matrix <span class="math inline">\(P\)</span> with dimensions <span class="math inline">\((n-k)\times k\)</span> which minimizes <span class="math inline">\(\text{trace}(PA P^\top -2 B P^\top\)</span>)). The optimal affine predictor is a Gaussian vector, one can check that the residual vector</p>
<p><span class="math display">\[
\begin{pmatrix}
X_{k+1}\\ \vdots \\ X_n
\end{pmatrix} - \left\{ \begin{pmatrix}
\mu_{k+1} \\ \vdots \\ \mu_n \end{pmatrix}   + \left(B A^{-1}\right) \times  \left( \begin{pmatrix}
X_{1} \\ \vdots \\ X_{k}
\end{pmatrix}  - \begin{pmatrix}
\mu_{1} \\ \vdots \\ \mu_k
\end{pmatrix}\right) \right\}
\]</span></p>
<p>is also Gaussian and orthogonal to the affine predictor. The residual vector is independent from the affine predictor.</p>
<p>This is enough to establish that the affine predictor is the orthogonal projection of <span class="math inline">\((X_{k+1}, \ldots, X_n)^\top\)</span> on the closed linear subspace of square-integrable <span class="math inline">\((X_{1},\ldots,X_k)^\top\)</span>-measurable random vectors.</p>
<p>This proves that the affine predictor is the conditional expectation.</p>
<p>In the notes, we deal with a special case of linear conditioning.</p>
<p>To fugure out general linear conditioning, consider <span class="math inline">\(X \sim \mathcal{N}(0, {K})\)</span> (we assume centering to alleviate notation and computations, translating does not change the relevant <span class="math inline">\(\sigma\)</span>-algebras and thus conditioning), where <span class="math inline">\({K} \in \textsf{DP}(n)\)</span>, and a linear transformation defined by matrix <span class="math inline">\(H\)</span> with dimensions <span class="math inline">\(m \times n\)</span>. <span class="math inline">\(H\)</span> is assumed to have rank <span class="math inline">\(m\)</span>. Agree on <span class="math inline">\(Y= {H} X\)</span>. Considering the Gaussian vector <span class="math inline">\([ X^\top : Y^\top]\)</span> with covariance matrix <span class="math display">\[
\left[
\begin{array}{cc}
{K} &amp; {K} {H}^\top \\
{H}{K} &amp; {H} {K} {H}^\top
\end{array}
\right]
\]</span></p>
<p>and adapting the previous computations (the covariance matrix is not positive definite any more), we may check that the conditional distribution of <span class="math inline">\(X\)</span> with respect to <span class="math inline">\(Y\)</span> is Gaussian with expectation <span class="math display">\[
K H^\top (HKH^\top)^{-1}
\]</span></p>
<p>and variance</p>
<p><span class="math display">\[
K - K H^\top (HKH^\top)^{-1} H K \, .
\]</span></p>
<p>The linearity of conditional expectation is a property of Gaussian vectors and linear conditioning. If you condition with respect to the norm <span class="math inline">\(\| X\|_2\)</span>, the conditional distribution is not Gaussian anymore.</p>
</section>
<section id="sec-about-gamma" class="level2" data-number="16.5">
<h2 data-number="16.5" class="anchored" data-anchor-id="sec-about-gamma"><span class="header-section-number">16.5</span> About Gamma distributions</h2>
<p>Investigating the norm of Gaussian vectors will prompt us to introduce <span class="math inline">\(\chi^2\)</span> distributions, a sub-family of Gamma distributions.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-gamma-distrib" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 16.4 (Gamma distributions)</strong></span> A Gamma distribution with parameters <span class="math inline">\((p, \lambda)\)</span>} (<span class="math inline">\(\lambda \in
\mathbb{R}_+\)</span> and <span class="math inline">\(p \in \mathbb{R}_+\)</span>), is a distribution on <span class="math inline">\((\mathbb{R}_+, \mathcal{B}(\mathbb{R}_+))\)</span> with density</p>
<p><span class="math display">\[g_{p, \lambda} (x) \equiv \frac{\lambda^p}{\Gamma (p)} \mathbf{1}_{x
\geq 0} x^{p - 1} e^{- \lambda x}
\]</span> where <span class="math inline">\(\Gamma (p) \equiv \int_0^{\infty} t^{p - 1} e^{- t} \mathrm{d} t\)</span>.</p>
<p>Parameter <span class="math inline">\(p\)</span> is called the <em>shape</em> parameter, <span class="math inline">\(\lambda\)</span> is called the <em>rate</em> or <em>intensity</em> parameter, <span class="math inline">\(1/\lambda\)</span> is called the <em>scale</em> parameter.</p>
</div>
</div>
</div>
</div>
<p>If <span class="math inline">\(X \sim  \text{Gamma}(p,1)\)</span> then <span class="math inline">\(\sigma X \sim \text{Gamma}(p,1/\sigma)\)</span> for <span class="math inline">\(\sigma&gt;0\)</span>.</p>
<p>Euler’s <span class="math inline">\(\Gamma ()\)</span> function interpolates the factorial. For every positive real <span class="math inline">\(p\)</span>, <span class="math inline">\(\Gamma (p + 1) = p \Gamma
(p) .\)</span> If <span class="math inline">\(p\)</span> is integer, <span class="math inline">\(\Gamma (p + 1) = p!\)</span></p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exr-gamma-half" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 16.3</strong></span> Check that <span class="math inline">\(\Gamma(1/2)=\sqrt{\pi}\)</span>.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-moments-gamma" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.12</strong></span> If <span class="math inline">\(X \sim \mathrm{Gamma}(p, \lambda)\)</span> <span class="math inline">\(\mathbb{E}X = \frac{p}{\lambda}\)</span> and <span class="math inline">\(\operatorname{var}(X) = \frac{p}{\lambda^2}\)</span>.</p>
</div>
</div>
</div>
</div>
<p>The next proposition is a cornerstone of Gamma-calculus. The sum of two independent Gamma-distributed random variables is Gamma distributed if they have the same intensity (or scale) parameter.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-sum-gamma" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.13</strong></span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent Gamma-distributed random variables with the same intensity parameter <span class="math inline">\(\lambda\)</span> <span class="math inline">\(X \sim \mathrm{Gamma}(p,
\lambda), Y\sim \mathrm{Gamma}(q, \lambda)\)</span> then <span class="math inline">\(X + Y \sim \mathrm{Gamma}(p+q, \lambda)\)</span>.</p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The density of the distribution of <span class="math inline">\(X+Y\)</span> is the convolution of the densities <span class="math inline">\(g_{p, \lambda}\)</span> et <span class="math inline">\(g_{q, \lambda}\)</span>. <span class="math display">\[\begin{eqnarray*}
g_{p, \lambda} \ast g_{q, \lambda} (x) &amp; = &amp; \int_{\mathbb{R_{}}}
g_{p, \lambda} (z) g_{_{q, \lambda}} (x - z) \mathrm{d} z\\
&amp; = &amp; \int_0^x    g_{p, \lambda} (z) g_{_{q,
\lambda}} (x - z) \mathrm{d} z\\
&amp; = &amp; \int_0^x  \frac{\lambda^p}{\Gamma (p)} z^{p - 1} \mathrm{e}^{- \lambda
z}     \frac{\lambda^q}{\Gamma (q)} (x - z)^{q - 1} \mathrm{e}^{-
\lambda (x - z)} \mathrm{d} z\\
&amp; = &amp; \frac{\lambda^{p + q}}{\Gamma (p) \Gamma (q)} \mathrm{e}^{- \lambda x}
\int_0^x z^{p - 1} (x - z)^{q - 1}    \mathrm{d} z\\
&amp;  &amp; \operatorname{changement} \operatorname{de} \operatorname{variable} z = x u\\
&amp; = &amp; \frac{\lambda^{p + q}}{\Gamma (p) \Gamma (q)} \mathrm{e}^{- \lambda x}
x^{p + q - 1} \int_0^{1} u^{p-1} (1 - u)^{q - 1}
\mathrm{d} u\\
&amp; = &amp; g_{p + q, \lambda} (x) \frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)} \int_0^{1} u^{p-1} (1 - u)^{q - 1}
\mathrm{d} u \, .
\end{eqnarray*}\]</span> We may pocket the next observation:<br>
<span class="math display">\[
B(p,q):=   \int_0^{1} u^{p-1} (1 - u)^{q - 1}
\mathrm{d} u
\]</span></p>
<p>satisfies <span class="math inline">\(B(p,q) = \frac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}.\)</span></p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<p>Gamma distributions with parameters <span class="math inline">\((k / 2, 1 / 2)\)</span> for <span class="math inline">\(k \in
\mathbb{N}\)</span> deserve to be named: they are <span class="math inline">\(\chi^2\)</span> distributions with <span class="math inline">\(k\)</span> degrees of freedom.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-chixdeux" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.14 (Chi-square distributions)</strong></span> The <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(k\)</span> degrees of freedom (denoted by <span class="math inline">\(\chi^2_k\)</span>) has density over <span class="math inline">\([0,\infty)\)</span>, <span class="math display">\[
\frac{x^{ \frac{1}{2} (k - 2)} e^{- \frac{x}{2}}}{2^{k / 2} \Gamma (k /
2)} .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-chisq-sum-indep-gauss" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.15</strong></span> The sum of <span class="math inline">\(k\)</span> independent squared standard Gaussian random variables is distributed according to the chi-square distributions with <span class="math inline">\(k\)</span> degrees of freedom <span class="math inline">\(\chi^2_k\)</span>.</p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>According to proposition <a href="032-acfamilies.html#prp-gammabeta" class="quarto-xref">Proposition&nbsp;<span>9.2</span></a>), it suffices to establish the proposition <span class="math inline">\(k = 1.\)</span></p>
<p>Let <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span>, for <span class="math inline">\(t\geq 0\)</span>, <span class="math display">\[\begin{align*}
\mathbb{P} \left\{ X^2 \leq t\right\}
&amp; = &amp; \Phi(\sqrt{t}) - \Phi(-\sqrt{t}) \\
&amp; = &amp; 2 \Phi(\sqrt{t})  - 1 \,.
\end{align*}\]</span> Now, differentiating with respect to <span class="math inline">\(t\)</span>, applying the chain rule provides us with a formula for the density: <span class="math display">\[
2 \frac{1}{2\sqrt{t}} \phi(\sqrt{t}) = \frac{1}{\sqrt{2\pi t}} \mathrm{e}^{-\frac{t}{2}} = \left(\frac{1}{2}\right)^{1/2} \frac{t^{-1/2}}{\Gamma(1/2)} \mathrm{e}^{-\frac{t}{2}} \,.
\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
</div>
</section>
<section id="norms-of-centred-gaussian-vectors" class="level2" data-number="16.6">
<h2 data-number="16.6" class="anchored" data-anchor-id="norms-of-centred-gaussian-vectors"><span class="header-section-number">16.6</span> Norms of centred Gaussian vectors</h2>
<p>The distribution of the squared Euclidean norm of a centered Gaussian vector only depends on the spectrum of its covariance matrix.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-norm-centered" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.5</strong></span> If <span class="math inline">\({X}:= (X_1, X_2,
\ldots, X_n)^{^\top} \sim \mathcal{N}\left(0, A\right)\)</span> with <span class="math inline">\(A = L L^\top\)</span> (<span class="math inline">\(L\)</span> lower triangular), if <span class="math inline">\(M \in \mathrm{SDP}(n)\)</span>, then <span class="math inline">\({X}^\top M {X}\)</span> is distributed like <span class="math inline">\(\sum_{i = 1}^n \lambda_i Z_i\)</span> where <span class="math inline">\((\lambda_i)_{i \in \{1, \ldots, n\}}\)</span> denote the eigenvalues of <span class="math inline">\(L^\top \times M\times L\)</span> and where <span class="math inline">\(Z_i\)</span> are independent <span class="math inline">\(\chi^2_1\)</span>-distributed random variables.</p>
</div>
</div>
</div>
</div>
<p>This is a corollary of an important property of standard Gaussian vectors: rotational invariance. The standard Gaussian distribution is invariant under orthogonal transform (a matrix <span class="math inline">\(O\)</span> is orthogonal iff <span class="math inline">\(OO^\top=\text{Id})\)</span>.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Matrix <span class="math inline">\(A\)</span> may be factorized as <span class="math inline">\(A = LL^\top\)</span> (Cholesky), and <span class="math inline">\({X}\)</span> is distributed like <span class="math inline">\(L {Y}\)</span> where <span class="math inline">\({Y}\)</span> is standard Gaussian. The quadratic form <span class="math inline">\({X}^\top M
{X}\)</span> is thus distributed like <span class="math inline">\({Y}^\top {L}^\top M
{L}  {Y}\)</span>. There exist an orthogonal transform <span class="math inline">\(O\)</span> such that <span class="math inline">\(L^\top M L = O^\top \operatorname{diag}
(\lambda_i) O.\)</span> Random vector <span class="math inline">\(O {Y}\)</span> is distributed like <span class="math inline">\(\mathcal{N} (0, I_n)\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
</section>
<section id="sec-norm-non-centered" class="level2" data-number="16.7">
<h2 data-number="16.7" class="anchored" data-anchor-id="sec-norm-non-centered"><span class="header-section-number">16.7</span> Norm of non-centred Gaussian vectors</h2>
<p>The distribution of the squared norm of a Gaussian vector with covariance matrix <span class="math inline">\(\sigma^2 \operatorname{Id}\)</span> depends on the norm of the expectation but does not depend on its direction. In addition, this distribution stochastically can be compared with the distribution of the squared norm of a centred Gaussian vector with the same covariance.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-name" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 16.5 (ordering)</strong></span> In a probability space endowed with distribution <span class="math inline">\(\mathbb{P}\)</span>, a real random variable <span class="math inline">\(X\)</span> is stochastically smaller than random variable <span class="math inline">\(Y\)</span>, if <span class="math display">\[
\mathbb{P} \{ X \leq Y \} =  1 \, .
\]</span></p>
<p>The distribution of <span class="math inline">\(Y\)</span> is said to stochastically dominate the distribution of <span class="math inline">\(X\)</span>.</p>
</div>
</div>
</div>
</div>
<p>If <span class="math inline">\(X\)</span> is stochastichally less than <span class="math inline">\(Y\)</span> and if <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> denote the cumulative distribution functions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, then for all <span class="math inline">\(x \in  \mathbb{R}\)</span>, <span class="math inline">\(F(x)\geq G(x)\)</span>. Quantile functions <span class="math inline">\(F^\leftarrow, G^\leftarrow\)</span> satisfy <span class="math inline">\(F^\leftarrow(p) \leq G^\leftarrow(p)\)</span> for <span class="math inline">\(p \in (0,1)\)</span>.</p>
<p>Conversely.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-stoch-ordering" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.16</strong></span> If <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are two cumulative distribution functions that satisfy <span class="math inline">\(\forall x \in \mathbb{R}\)</span> <span class="math inline">\(F(x)\geq G(x)\)</span> then there exists a probability space equipped with a probability distribution <span class="math inline">\(\mathbb{P}\)</span> and two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with cumulative distribution functions <span class="math inline">\(F, G\)</span> that satisfy:</p>
<p><span class="math display">\[
\mathbb{P}\{ X \leq Y\} = 1 \, .
\]</span></p>
</div>
</div>
</div>
</div>
<p>The proof proceeds by a <em>quantile coupling</em> argument.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>It is enough to endow <span class="math inline">\(([0,1], \mathcal{B}([0,1])\)</span> with the uniform distribution. Let <span class="math inline">\(X (\omega)=  F^{\leftarrow}(\omega)\)</span>, <span class="math inline">\(Y(\omega) = G^\leftarrow(\omega)\)</span>. Then the distribution of <span class="math inline">\(X\)</span> (resp. <span class="math inline">\(Y\)</span>) has cumulative distribution function <span class="math inline">\(F\)</span> (resp. <span class="math inline">\(G\)</span>) and the following holds:</p>
<p><span class="math display">\[
\mathbb{P} \{ X \leq Y\}  = \mathbb{P} \{ F^{\leftarrow}(U) \leq  G^{\leftarrow}(U)\} = 1 \, .
\]</span></p>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-non-central-chi" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.6</strong></span> If <span class="math inline">\(X \sim \mathcal{N}\left( 0, \sigma^2 \operatorname{Id}\right)\)</span> and <span class="math inline">\(Y \sim \mathcal{N}\left( \theta, \sigma^2 \operatorname{Id}\right)\)</span> with <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> then<br>
<span class="math display">\[
\left\Vert Y \right\Vert^2 \sim  \left( (Z_1 + \|\theta\|_2)^2 + \sum_{i=1}^d Z_i^2 \right)
\]</span></p>
<p>where <span class="math inline">\(Z_i\)</span> are i.i.d. according to <span class="math inline">\(\mathcal{N}(0,\sigma^2)\)</span>.</p>
<p>For every <span class="math inline">\(x \geq 0\)</span>,</p>
<p><span class="math display">\[
\mathbb{P} \left\{ \| Y \|\leq x\right\} \leq \mathbb{P} \left\{ \| X \| \leq x \right\} \, .
\]</span></p>
<p>The distribution of <span class="math inline">\(\| Y\|^2/\sigma^2\)</span> (non-centred <span class="math inline">\(\chi^2\)</span> with parameter <span class="math inline">\(\| \theta\|_2/\sigma\)</span>) <em>stochastichally dominates</em> the distribution of <span class="math inline">\(\| X\|^2/\sigma^2\)</span> (centred <span class="math inline">\(\chi^2\)</span> with the same number of degrees of freedom).</p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The Gaussian vector <span class="math inline">\(Y\)</span> is distributed like <span class="math inline">\(\theta + X\)</span>. There exists an orthogonal transform <span class="math inline">\(O\)</span> such that <span class="math display">\[
O \theta = \begin{pmatrix}
\| \theta\|_2 \\
0 \\
\vdots \\
0
\end{pmatrix} \, .
\]</span></p>
<p>Vectors <span class="math inline">\(OY\)</span> and <span class="math inline">\(OX\)</span> respectively have the same norms as <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>The squared norm of <span class="math inline">\(Y\)</span> is distributed as the squared norm of <span class="math inline">\(OY\)</span>, that is like <span class="math inline">\((Z_1+ \|\theta\|_2)^2 +\sum_{i=2}^d Z_i^2\)</span>. This proves the first part of the theorem.</p>
<p>To establish the second part of the theorem, it suffices to check that for every <span class="math inline">\(x\geq 0\)</span>, <span class="math display">\[
\mathbb{P} \left\{ (Z_1+ \|\theta\|_2)^2  \leq x \right\} \leq
\mathbb{P} \left\{ X_1^2 \leq x \right\} \, ,
\]</span></p>
<p>that is<br>
<span class="math display">\[
\mathbb{P} \left\{ |Z_1+ \|\theta\|_2|  \leq \sqrt{x} \right\} \leq
\mathbb{P} \left\{ |X_1| \leq \sqrt{x} \right\} \, ,
\]</span></p>
<p>or <span class="math display">\[
\Phi(\sqrt{x}- \|\theta\|_2) - \Phi(-\sqrt{x}-\|\theta\|_2)  \leq \Phi(\sqrt{x}) -  \Phi(-\sqrt{x}) \, .
\]</span></p>
<p>For <span class="math inline">\(y&gt;0\)</span>, the function mapping <span class="math inline">\([0,\infty)\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, defined by <span class="math inline">\(a \mapsto \Phi(y-a) - \Phi(-y-a)\)</span> is non-increasing with respect to <span class="math inline">\(a\)</span>: it derivative with respect to <span class="math inline">\(a\)</span> equals <span class="math inline">\(-\phi(y-a)+\phi(-y-a)=\phi(y+a)-\phi(y-a)\leq 0\)</span>. The conclusion follows</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<p>The last step of the proof reads as <span class="math display">\[
\mathbb{P} \left\{ X \in \theta + C \right\} \leq \mathbb{P} \left\{ X \in C\right\}
\]</span></p>
<p>where <span class="math inline">\(X \sim \mathcal{N}(0,\operatorname{Id}_1)\)</span>, <span class="math inline">\(\theta \in \mathbb{R}\)</span> and <span class="math inline">\(C = [-\sqrt{x},\sqrt{x}]\)</span>. This inequality holds in dimension <span class="math inline">\(d\geq 1\)</span> if <span class="math inline">\(C\)</span> is compact, convex, symmetric. This (subtle) result is called Anderson’s Lemma.</p>
</section>
<section id="sec-cochran" class="level2" data-number="16.8">
<h2 data-number="16.8" class="anchored" data-anchor-id="sec-cochran"><span class="header-section-number">16.8</span> Cochran Theorem and consequences</h2>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-cochran" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.7 (Cochran)</strong></span> Let <span class="math inline">\(X \sim \mathcal{N}(0, \text{I}_n)\)</span> and <span class="math inline">\(\mathbb{R}^n = \oplus_{j=1}^k E_j\)</span> where <span class="math inline">\(E_j\)</span> are pairwise orthogonal linear subspaces of <span class="math inline">\(\mathbb{R}^n\)</span>. Denote by <span class="math inline">\(\pi_{E_j}\)</span> the orthogonal projection on <span class="math inline">\(E_j\)</span>.</p>
<p>The collection of Gaussian vectors <span class="math inline">\(\left( \pi_{E_j} X\right)_{j \leq k}\)</span> is independent and for each~<span class="math inline">\(j\)</span> <span class="math display">\[
\| \pi_{E_j} X\|_2^2 \sim \chi^2_{\text{dim}(E_j)} \, .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The covariance matrix of <span class="math inline">\(\pi_{E_j} X\)</span> is <span class="math inline">\(\pi_{E_j} \pi_{E_j}^\top = \pi_{E_j}\)</span>. The eigenvalues of <span class="math inline">\(\pi_{E_j}\)</span> are <span class="math inline">\(1\)</span> with multiplicity <span class="math inline">\(\text{dim}(E_j)\)</span> and <span class="math inline">\(0\)</span>. The statement about the distribution of <span class="math inline">\(\| \pi_{E_j} X\|_2^2\)</span> is a corollary of <span class="quarto-unresolved-ref">?prp-normgaussstand</span> and <span class="quarto-unresolved-ref">?prp-normespectre</span>.</p>
<p>To prove stochastic independence, let us consider <span class="math inline">\(\mathcal{I}, \mathcal{J} \subset \{1,\ldots,k\}\)</span> with <span class="math inline">\(\mathcal{I} \cap \mathcal{J} = \emptyset.\)</span> It is enough to check that for all <span class="math inline">\((\alpha)_{j \in \mathcal{I}}, (\beta_j)_{j \in \mathcal{J}}\)</span>, the characteristic functions of <span class="math display">\[
\left(\sum_{j\in \mathcal{I}} \langle \alpha_j, \pi_{E_j} X \rangle, \sum_{j\in \mathcal{J}} \langle \beta_j, \pi_{E_j} X \rangle\right)
\]</span></p>
<p>can be factorized. It suffices to check that the two Gaussians are orthogonal.</p>
<p><span class="math display">\[\begin{eqnarray*}
{ \mathbb{E} \left[ \left(\sum_{j\in \mathcal{I}} \langle \alpha_j, \pi_{E_j} X \rangle \right) \times \left(\sum_{j'\in \mathcal{J}} \langle \beta_{j'}, \pi_{E_{j'}} X \rangle\right)\right]}
&amp; = &amp; \sum_{j \in \mathcal{I}, j' \in \mathcal{J}}
\alpha_j^\top \pi_{E_j} \pi_{E_{j'}} \beta_{j'}
=  0 \, .
\end{eqnarray*}\]</span></p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<p>The next result is a cornerstone of statistical inference in Gaussian models. It is a corollary of Cochran’s Theorem.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-student" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.8 (Student)</strong></span> &nbsp;</p>
Of <span class="math inline">\((X_1, \ldots, X_n)\)</span> are i.i.d. according to <span class="math inline">\(\mathcal{N} (\mu, \sigma^2)\)</span>, if <span class="math inline">\(\overline{X}_n:=
\sum^n_{i = 1}  X_i / n\)</span> et <span class="math inline">\(V:= \sum^{n}_{i = 1} (X_i - \overline{X}_n)^2\)</span>, then
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Without loss of generality, we may assume that <span class="math inline">\(\mu=0\)</span> et <span class="math inline">\(\sigma=1\)</span>.</p>
<p>As <span class="math display">\[
\begin{pmatrix}
\overline{X}_n \\
\vdots\\
\overline{X}_n \\
\end{pmatrix}
=  \frac{1}{n} \begin{pmatrix}
1 \\
\vdots\\
1 \\
\end{pmatrix} \times \begin{pmatrix}
1 &amp; \ldots &amp; 1
\end{pmatrix} X
\]</span> the vector <span class="math inline">\((\overline{X}_n, \ldots , \overline{X}_n)^\top\)</span> is the orthogonal projection of the standard Gaussian vector <span class="math inline">\(X\)</span> on the line generated by <span class="math inline">\((1, \ldots, 1)^\top\)</span>.</p>
<p>Vector <span class="math inline">\((X_1- \overline{X}_n, \ldots , X_n -\overline{X}_n)^\top\)</span> is the orthogonal projection fo Gaussian vector <span class="math inline">\(X\)</span> on the hyperplane which is orthogonal to <span class="math inline">\((1, \ldots, 1)^\top\)</span>.</p>
<p>According to Cochran’s Theorem (<a href="#sec-cochran" class="quarto-xref"><span>Section 16.8</span></a>), random vectors <span class="math inline">\((\overline{X}_n, \ldots , \overline{X}_n)^\top\)</span>, and <span class="math inline">\((X_1- \overline{X}_n, \ldots , X_n -\overline{X}_n)^\top\)</span> are independent.</p>
<p>The distribution of <span class="math inline">\(\overline{X}_n\)</span> is trivially Gaussian.</p>
<p>The distribution of <span class="math inline">\(V\)</span> is characterized using Cochran’s Theorem.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-student-dsitrib" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 16.6 (Distribution)</strong></span> If <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span>, <span class="math inline">\(Y \sim \chi_p^2\)</span> and if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(Z =  X/ \sqrt{Y/p}\)</span> is distributed according to a (centred) Student distribution with <span class="math inline">\(p\)</span> degrees of freedom.</p>
</div>
</div>
</div>
</div>
</section>
<section id="gaussian-concentration" class="level2" data-number="16.9">
<h2 data-number="16.9" class="anchored" data-anchor-id="gaussian-concentration"><span class="header-section-number">16.9</span> Gaussian concentration</h2>
<p>The very definition of Gaussian vectors characterizes he distribution of any affine function of a standard Gaussian vector. If the linear part of the affine function is defined by a vector <span class="math inline">\(\lambda\)</span>, we know that the variance will be <span class="math inline">\(\|\lambda\|^2_2\)</span>. What happens if we are interested in fairly regular functions of a standard Gaussian vector? for example if we consider <span class="math inline">\(L\)</span>-lipschitzian functions? These are generalizations of affine functions. We cannot therefore expect a general increase in the variance of the <span class="math inline">\(L\)</span>-Lipschitzian functions of a standard Gaussian vector better than <span class="math inline">\(L^2\)</span> (in the linear case the Lipschitz constant is the Euclidean norm of <span class="math inline">\(\lambda\)</span>). It is remarkable that the bound provided for linear functions extends to Lipschitzian functions. It is even more remarkable that this bound does not involve the dimension of the ambient space.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-gauss-conc" class="theorem">
<p><span class="theorem-title"><strong>Theorem 16.9</strong></span> Let <span class="math inline">\(X \sim \mathcal{N}(0 , \text{Id}_d)\)</span>.</p>
<ul>
<li><p>if <span class="math inline">\(f\)</span> is differentiable on <span class="math inline">\(\mathbb{R}^d\)</span>, <span class="math display">\[
  \operatorname{var}(f(X)) \leq \mathbb{E} \| \nabla f \|^2   \qquad \text{(Poincaré's inequality)}
  \]</span></p></li>
<li><p>if <span class="math inline">\(f\)</span> is <span class="math inline">\(L\)</span>-Lipschitz on <span class="math inline">\(\mathbb{R}^d\)</span>,</p>
<p><span class="math display">\[
  \operatorname{var}(f(X)) \leq L^2
  \]</span></p>
<p>and for <span class="math inline">\(\lambda&gt;0\)</span> <span class="math display">\[
  \log \mathbb{E} \mathrm{e}^{\lambda(f(X)-\mathbb{E}f)} \leq \frac{\lambda^2 L^2}{2} \, .
  \]</span></p>
<p>For every <span class="math inline">\(t\geq 0\)</span>, <span class="math display">\[
  \mathbb{P} \left\{ f(X) - \mathbb{E} f(X) \geq t \right\} \leq \mathrm{e}^{-\frac{t^2}{2 L^2}} \, .
  \]</span></p></li>
</ul>
</div>
</div>
</div>
</div>
<p>The proof relies on the next identity.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-cov-identity" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 16.17 (Covariance identity)</strong></span> Let <span class="math inline">\(X,Y\)</span> be two independent <span class="math inline">\(\mathbb{R}^d\)</span>-valued standard Gaussian vectors, let <span class="math inline">\(f,g\)</span> be two differentiable functions from <span class="math inline">\(\mathbb{R}^d\)</span> to <span class="math inline">\(\mathbb{R}\)</span>. <span class="math display">\[
\operatorname{cov}(f(X),g(X)) = \int_0^1 \mathbb{E}\left\langle \nabla f(X) , \nabla g\left(\alpha X +\sqrt{1- \alpha^2} Y \right) \right\rangle \mathrm{d} \alpha
\]</span></p>
</div>
</div>
</div>
</div>
<p>We start by checking this proposition on functions <span class="math inline">\(x \mapsto  \mathrm{e}^{\imath \langle \lambda, x\rangle}, x \in \mathbb{R}^d\)</span>.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let us first check Poincaré’s inequality.</p>
<p>We choose <span class="math inline">\(f=g\)</span>. Starting from the covariance identity, thanks to Cauchy-Schwarz’s inequality:</p>
<p><span class="math display">\[\begin{eqnarray*}
\operatorname{var}(f(X) )     &amp;= &amp; \operatorname{cov}(f(X),f(X)) \\
&amp; = &amp; \int_0^1  \mathbb{E}\left\langle \nabla f(X) , \nabla f\left(\alpha X +\sqrt{1- \alpha^2} Y \right) \right\rangle \mathrm{d} \alpha \\
&amp; \leq &amp; \int_0^1 \left( \mathbb{E}\|  \nabla f(X) \|^2\right)^{1/2} \times
\left(\mathbb{E} \|\nabla f\left(\alpha X +\sqrt{1- \alpha^2} Y\right)\|^2 \right)^{1/2} \mathrm{d} \alpha \, .
\end{eqnarray*}\]</span></p>
<p>The desired results follows by noticing that <span class="math inline">\(X\)</span> and <span class="math inline">\(\alpha X + \sqrt{1- \alpha^2}Y\)</span> are both <span class="math inline">\(\mathcal{N}(0,\text{Id})\)</span>-distributed.</p>
<p>To obtain the exponential inequality, choose <span class="math inline">\(f\)</span> differentiable and 1-Lipschitz, and <span class="math inline">\(g = \exp(\lambda f)\)</span> pour <span class="math inline">\(\lambda\geq 0\)</span>. Without loss of generality, assume <span class="math inline">\(\mathbb{E}f(X)=0\)</span>. The covariance identity and the chain rule imply</p>
<p><span class="math display">\[\begin{eqnarray*}
\operatorname{cov}\left(f(X),\mathrm{e}^{\lambda f(X)}\right)  &amp; = &amp; \lambda
\int_0^1  \mathbb{E}\left[\left\langle \nabla f(X) , \nabla f\left(\alpha X +\sqrt{1- \alpha^2} Y \right) \right\rangle \mathrm{e}^{\lambda f\left(\alpha X +\sqrt{1- \alpha^2} Y \right)}\right] \mathrm{d} \alpha \\
&amp; \leq &amp; \lambda L^2
\int_0^1  \mathbb{E}\left[ \mathrm{e}^{\lambda f\left(\alpha X +\sqrt{1- \alpha^2} Y \right)}\right] \mathrm{d} \alpha \\
&amp; = &amp; \lambda L^2 \mathbb{E}\left[ \mathrm{e}^{\lambda f\left(X\right)}\right]
\end{eqnarray*}\]</span></p>
<p>Define <span class="math inline">\(F(\lambda):= \mathbb{E}\left[ \mathrm{e}^{\lambda f\left(X\right)}\right]\)</span>. Note that we have just established a differential inequality for <span class="math inline">\(F\)</span>, checking <span class="math inline">\(\operatorname{cov}( f , \mathrm{e}^{\lambda f})= F'(\lambda)\)</span> since <span class="math inline">\(f\)</span> is centred:</p>
<p><span class="math display">\[
F'( \lambda) \leq \lambda L^2 F(\lambda) \,.
\]</span></p>
<p>Solving this differential inequality under <span class="math inline">\(F(0)=1\)</span>, for <span class="math inline">\(\lambda\geq 0\)</span> <span class="math display">\[
F( \lambda) \leq \mathrm{e}^{\frac{\lambda^2L^2}{2}} \, .
\]</span></p>
<p>The same approach works for <span class="math inline">\(\lambda&lt;0\)</span>. It is enough to invoke Markov’s exponential inequality and to optimize with respect to <span class="math inline">\(\lambda=t/L^2\)</span>.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<p>Concentration inequalities describe the behavior of the norm of high-dimensional Gaussian vectors.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="cor-norm-gaussvector" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 16.2</strong></span> If <span class="math inline">\(X\)</span> is a standard <span class="math inline">\(d\)</span>-dimensional Gaussian vector, then <span class="math display">\[
\operatorname{var}(\|X\|_2) \leq 1 \,
\]</span> and <span class="math display">\[
\sqrt{d-1} \leq \mathbb{E} \|X\|_2 \leq \sqrt{d} \, .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The Euclidean norm is <span class="math inline">\(1\)</span>-Lipschitz (triangle inequality). The first inequality follows fron Poincaré’s inequality.</p>
<p>The upper bound on expectation follows from Jensen’s inequality.</p>
<p>The lower bound on expectation follows from <span class="math inline">\((\mathbb{E} \|X\|_2)^2 = \mathbb{E} \|X\|_2^2 - \operatorname{var}(\|X\|_2)= d -\operatorname{var}(\|X\|_2)\)</span> and from the variance upper bound.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exr-norm-non-centered-gauss-vec" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 16.4</strong></span> Let <span class="math inline">\(X \sim \mathcal{N} (0,K)\)</span> where <span class="math inline">\(K\)</span> is in <span class="math inline">\(\textsf{DP}(d)\)</span> and <span class="math inline">\(Z=  \max_{i\leq d} X_i\)</span>.</p>
<p>Show <span class="math display">\[
\operatorname{Var}(Z) \leq \max_{i \leq d } K_{i,i}:= \max_{i \leq d} \operatorname{Var} (X_i) .
\]</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exr-diff-gaussvec" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 16.5</strong></span> Let <span class="math inline">\(X, Y\sim \mathcal{N} (0,\text{Id}_n)\)</span> with <span class="math inline">\(X,Y\)</span> indépendent.</p>
<p>Show<br>
<span class="math display">\[
\sqrt{2n-1} \leq   \mathbb{E}[\|X-Y\|] \leq \sqrt{2 n}
\]</span> and<br>
<span class="math display">\[
\mathbb{P}   \left\{ \|X-Y\| - \mathbb{E}[\|X-Y\|] \geq t \right\} \leq \mathrm{e}^{-t^2} \, .
\]</span></p>
</div>
</div>
</div>
</div>
</section>
<section id="bibliographic-remarks" class="level2" data-number="16.10">
<h2 data-number="16.10" class="anchored" data-anchor-id="bibliographic-remarks"><span class="header-section-number">16.10</span> Bibliographic remarks</h2>
<p>Gaussian literature is very abundant, see for example <span class="citation" data-cites="janson1997gaussian">(<a href="#ref-janson1997gaussian" role="doc-biblioref"><strong>janson1997gaussian?</strong></a>)</span>. Much of this literature is relevant to statistics.</p>
<p>The lemmas <span class="quarto-unresolved-ref">?lem-stein</span> and <span class="quarto-unresolved-ref">?lem-steinbis</span> that characterize the Gaussian standard are the starting point of Stein’s (Charles) method to demonstrate the central limit theorem (and many other results). This relatively recent development is described in <span class="citation" data-cites="2011arXiv1109.1880R">(<a href="#ref-2011arXiv1109.1880R" role="doc-biblioref">Ross, 2011</a>)</span>.</p>
<p>Matrix analysis and algorithmics play an important role in Gaussian analysis and statistics. The books <span class="citation" data-cites="HorJoh90">(<a href="#ref-HorJoh90" role="doc-biblioref">Horn &amp; Johnson, 1990</a>)</span>, and if we wish to go further <span class="citation" data-cites="Bha97">(<a href="#ref-Bha97" role="doc-biblioref">Bhatia, 1997</a>)</span>, provide an introduction to the concepts and techniques of matrix factorization and elements of perturbation theory.</p>
<p>There is a multi-dimensional version of the laws of <span class="math inline">\(\chi^2\)</span> that appear when determining the law of variance empirical. These are the laws of Wishart. They were the subject of intensive studies in random matrix theory, see for example <span class="citation" data-cites="AnGuZe10">(<a href="#ref-AnGuZe10" role="doc-biblioref">Anderson, Guionnet, &amp; Zeitouni, 2010</a>)</span></p>
<p>Gaussian concentration plays an important role in non-parametric statistics and is a source of inspiration in statistical learning. M. Ledoux’s book <span class="citation" data-cites="ledoux:2001">(<a href="#ref-ledoux:2001" role="doc-biblioref">Ledoux, 2001</a>)</span> provides an elegant perspective on this issue.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-AnGuZe10" class="csl-entry" role="listitem">
Anderson, G. W., Guionnet, A., &amp; Zeitouni, O. (2010). <em>An introduction to random matrices</em> (Vol. 118). Cambridge: Cambridge University Press.
</div>
<div id="ref-Bha97" class="csl-entry" role="listitem">
Bhatia, R. (1997). <em>Matrix analysis</em>. Springer-Verlag.
</div>
<div id="ref-HorJoh90" class="csl-entry" role="listitem">
Horn, R. A., &amp; Johnson, C. R. (1990). <em>Matrix analysis</em>. Cambridge University Press.
</div>
<div id="ref-ledoux:2001" class="csl-entry" role="listitem">
Ledoux, M. (2001). <em>The concentration of measure phenomenon</em>. AMS.
</div>
<div id="ref-2011arXiv1109.1880R" class="csl-entry" role="listitem">
Ross, N. (2011). <span class="nocase">Fundamentals of Stein’s method</span>. <em>ArXiv e-Prints</em>. Retrieved from <a href="https://arxiv.org/abs/1109.1880">https://arxiv.org/abs/1109.1880</a>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/s-v-b\.github\.io\/MA1AY010-CN\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./08-convergence-3.html" class="pagination-link" aria-label="Convergence in distribution">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Convergence in distribution</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/s-v-b/MA1AY010-CN/edit/main/09-gaussian-vectors.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/s-v-b/MA1AY010-CN/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>