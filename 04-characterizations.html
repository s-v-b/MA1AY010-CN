<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Characterizations of probability distributions â€“ MA1AY010 Class Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-conditioning.html" rel="next">
<link href="./032-acfamilies.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3a70adc2469c323d0515427c5f9cb542.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">MA1AY010 Class Notes</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
    <a href="./MA1AY010-Class-Notes.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-characterizations.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Characterizations of probability distributions</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Warm up</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-language.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">A modicum of measure theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./031-moments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A modicum of integration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Families of discrete distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./023-discrete-condition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Discrete Conditioning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./032-acfamilies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Absolutely continuous probability measures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-characterizations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Characterizations of probability distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-conditioning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Conditioning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-convergences-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Convergences I : almost sure, <span class="math inline">\(L_2\)</span>, <span class="math inline">\(L_1\)</span>, in Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-convergence-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Convergence in distribution</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-convergence-3b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Refinments and extensions of thr Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-gaussian-vectors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Gaussian vectors</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-roadmapcharac" id="toc-sec-roadmapcharac" class="nav-link active" data-scroll-target="#sec-roadmapcharac"><span class="header-section-number">7.1</span> Motivation</a></li>
  <li><a href="#sec-PGFbis" id="toc-sec-PGFbis" class="nav-link" data-scroll-target="#sec-PGFbis"><span class="header-section-number">7.2</span> Probability generating function</a></li>
  <li><a href="#sec-laplace" id="toc-sec-laplace" class="nav-link" data-scroll-target="#sec-laplace"><span class="header-section-number">7.3</span> Laplace transform</a>
  <ul class="collapse">
  <li><a href="#definition-and-elementary-properties" id="toc-definition-and-elementary-properties" class="nav-link" data-scroll-target="#definition-and-elementary-properties"><span class="header-section-number">7.3.1</span> Definition and elementary properties</a></li>
  <li><a href="#widder" id="toc-widder" class="nav-link" data-scroll-target="#widder"><span class="header-section-number">7.3.2</span> Injectivity of Laplace transforms and an inversion formula</a></li>
  </ul></li>
  <li><a href="#sec-charfun" id="toc-sec-charfun" class="nav-link" data-scroll-target="#sec-charfun"><span class="header-section-number">7.4</span> Characteristic functions and Fourier transforms</a>
  <ul class="collapse">
  <li><a href="#charFuncDef" id="toc-charFuncDef" class="nav-link" data-scroll-target="#charFuncDef"><span class="header-section-number">7.4.1</span> Characteristic function</a></li>
  <li><a href="#charfungaussian" id="toc-charfungaussian" class="nav-link" data-scroll-target="#charfungaussian"><span class="header-section-number">7.4.2</span> Characteristic function of a univariate Gaussian distribution</a></li>
  <li><a href="#convol" id="toc-convol" class="nav-link" data-scroll-target="#convol"><span class="header-section-number">7.4.3</span> Sums of independent random variables and convolutions</a></li>
  <li><a href="#charinjectivity" id="toc-charinjectivity" class="nav-link" data-scroll-target="#charinjectivity"><span class="header-section-number">7.4.4</span> Injectivity Theorem and inversion formula</a></li>
  <li><a href="#diffintfourier" id="toc-diffintfourier" class="nav-link" data-scroll-target="#diffintfourier"><span class="header-section-number">7.4.5</span> Differentiability and integrability</a></li>
  <li><a href="#cauchy" id="toc-cauchy" class="nav-link" data-scroll-target="#cauchy"><span class="header-section-number">7.4.6</span> Another application: understanding Cauchy distribution</a></li>
  </ul></li>
  <li><a href="#quantiles" id="toc-quantiles" class="nav-link" data-scroll-target="#quantiles"><span class="header-section-number">7.5</span> Quantile functions</a></li>
  <li><a href="#sec-bibcharac" id="toc-sec-bibcharac" class="nav-link" data-scroll-target="#sec-bibcharac"><span class="header-section-number">7.6</span> Bibliographic remarks</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-chapchar" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Characterizations of probability distributions</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="sec-roadmapcharac" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="sec-roadmapcharac"><span class="header-section-number">7.1</span> Motivation</h2>
<p>In full generality, a probability distribution is a complex and opaque object. It is a <span class="math inline">\([0,1]\)</span>-valued function defined over a <span class="math inline">\(\sigma\)</span>-algebra of subsets. A concrete <span class="math inline">\(\sigma\)</span>-algebra, let alone the abstract notion of <span class="math inline">\(\sigma\)</span>-algebra, is not easily grasped. Looking for simpler characterizations of probability distributions is a sensible goal. When facing questions like: ``are two probability distributions equal?â€œ, we know it suffices to check that the two distributions coincide on generating families of events. This makes Cumulative Distribution Functions (CDFs) precious tools. Cumulative Distribution Functions and their generalized inverse functions (quantile functions) are very convenient when handling maxima, minima, or more generally order statistics of collections of independent random variables, but when it comes to handling sums of independent random variables or branching processes, cumulative distribution functions are of moderate help.</p>
<p>In this lesson, we review three related ways of characterizing probability distributions through functions defined on the real line: Probability Generating Functions (<a href="#sec-PGFbis" class="quarto-xref">Section&nbsp;<span>7.2</span></a>)), Laplace transforms (<a href="#sec-laplace" class="quarto-xref">Section&nbsp;<span>7.3</span></a>)) and characteristic functions which extend Fourier transforms to probability distributions (<a href="#sec-charfun" class="quarto-xref">Section&nbsp;<span>7.4</span></a>)). The three methods are distinct in scope but they rely on the same idea and share common features.</p>
<p>Indeed, Probability Generating Functions can be seen as special case of Laplace transforms. The latter can be seen as special cases of Fourier transforms. All three methods do characterize probability distributions. They are equipped with inversion formulae.</p>
<p>The three methods provide us with a seamless treatment of sums of independent random variables.</p>
<p>All three methods relate the integrability of probability distributions and the smoothness of transforms.</p>
<p>In the next lessons (<a href="08-convergence-3b.html" class="quarto-xref">Chapter&nbsp;<span>11</span></a>), we shall see that the three transforms characterize <em>convergence in distribution</em>.</p>
<p>Probability generating functions, Laplace transforms and characteristic functions deliver an important analytical machinery to Probability Theory. From Analysis, we get off-the-shelf arguments to establish smoothness properties of transforms, and with little more work, we can construct the inversion formulae.</p>
</section>
<section id="sec-PGFbis" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="sec-PGFbis"><span class="header-section-number">7.2</span> Probability generating function</h2>
<p>In this section, <span class="math inline">\(X\)</span> is an integer-valued random variable, with distribution <span class="math inline">\(P\)</span>, cumulative distribution function <span class="math inline">\(F\)</span> and probability mass function <span class="math inline">\(p\)</span>. Recall that <span class="math inline">\(P\)</span> is completely characterized by the much simpler objects <span class="math inline">\(F\)</span> and <span class="math inline">\(p\)</span>. Now, let <span class="math inline">\(Y\)</span> be another integer-valued random variable living on the same probability space as <span class="math inline">\(X\)</span>, independent from <span class="math inline">\(X\)</span>, with distribution <span class="math inline">\(Q\)</span>, distribution function <span class="math inline">\(G\)</span> and probability mass function <span class="math inline">\(q\)</span>. What can we tell about the distribution of <span class="math inline">\(X+Y\)</span>? Is it easy to figure out its cumulative distribution function, its probability mass function?</p>
<p>The probability mass function of (the distribution of) <span class="math inline">\(X+Y\)</span> is the <em>convolution</em> of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span></p>
<p><span class="math display">\[\begin{array}{rl}\mathbb{P}\{ X + Y = n\}  &amp; = \sum_{k=0}^n \mathbb{P}\{ X + Y = n \wedge X = k\} \\  &amp; = \sum_{k=0}^n \mathbb{P}\{ Y = n - k \wedge X = k\}  \\  &amp; = \sum_{k=0}^n \mathbb{P}\{ Y = n - k\} \times  \mathbb{P}\{ X = k\}\\  &amp; = \sum_{k=0}^n p(k) \times q(n-k) \\  &amp; =  p \star q (n) \, ,\end{array}\]</span></p>
<p>where the third equality comes from independence of <span class="math inline">\(\sigma(X)\)</span> and <span class="math inline">\(\sigma(Y)\)</span>.</p>
<p>Besides the probability mass function, another function characterizes probability distributions and delivers instantaneous information about the distribution of sums of independent integer-valued random variables and many other things.</p>
<div id="def-pgf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.1 (Probability Generating Function)</strong></span> The probability generating function (PGF) of a probability distribution over <span class="math inline">\(\mathbb{N}\)</span>, defined by its probability mass function (PMF) <span class="math inline">\(p\)</span> is the function <span class="math inline">\(G: [0,1] \to \mathbb{R}\)</span> defined by:</p>
<p><span class="math display">\[G(s) =  \sum_{n=0}^\infty p(n) s^n\, .\]</span></p>
</div>
<div id="exm-pgf1" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.1</strong></span> The probability generating function of basic discrete distributions is easily computed. The results are useful and suggestive.</p>
<ul>
<li><p>Bernoulli distribution with parameter <span class="math inline">\(p\)</span>: <span class="math display">\[1 - p + p s = 1 + p (s-1)\]</span></p></li>
<li><p>Binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>:</p></li>
</ul>
<p><span class="math display">\[\sum_{k=0}^n \binom{n}{k} p^k (1-p)^{n-k} s^k =  \left(ps + 1- p\right)^n = \left( 1 + p(s-1)\right)^n\]</span></p>
<ul>
<li>Poisson distribution with parameter <span class="math inline">\(\mu\)</span>:</li>
</ul>
<p><span class="math display">\[\sum_{n=0}^\infty \mathrm{e}^{-\mu} \frac{\mu^n}{n!} s^n = \mathrm{e}^{\mu (s-1)} \,.\]</span></p>
</div>
<p>The next observation follows almost immediately from the definition of probability generating functions.</p>
<div id="prp-propPGFeasy" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.1</strong></span> A probability generating function <span class="math inline">\(G\)</span> satisfies the following conditions:</p>
<ul>
<li><span class="math inline">\(G\)</span> is non-negative over <span class="math inline">\([0,1]\)</span>;</li>
<li><span class="math inline">\(G(0) = P\{0\}, \quad G(1)=1\)</span>;</li>
<li><span class="math inline">\(G\)</span> is non-decreasing over <span class="math inline">\([0,1]\)</span>;</li>
<li><span class="math inline">\(G\)</span> is continuous and convex.</li>
</ul>
</div>
<br>
<p>
</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Properties 1), 2) and 3) are obvious: <span class="math inline">\(G\)</span> is a convex combination of non-negative, non-decreasing, continuous and convex functions.</p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<br>
<p>
</p>
<p><em>Generatingfunctionology</em> lies at the crossing between combinatorics, real analysis, complex analysis, and probability theory. Defining PGF as a <em>power series</em> brings within probability theory a collection of theorems that facilitate the identification of probability distributions or that connect integrability properties of the probability distribution with smoothness properties of the PGF.</p>
<p>Keep in mind that a generating function defines a function from the set of complex numbers <span class="math inline">\(\mathbb{C}\)</span> to <span class="math inline">\(\mathbb{C}\)</span>:</p>
<p><span class="math display">\[G(z) = \sum_{n=0}^\infty p(n) z^n \qquad\text{for all } z \in \mathbb{C} \text{ such that the series converges}\, .\]</span></p>
<p>Characterizing the domain of a function defined in that way is crucial. The next proposition is at the core of Power Series theory.</p>
<div id="prp-radius" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.2</strong></span> The <em>radius of convergence</em> of the generating function <span class="math inline">\(G\)</span></p>
<p><span class="math display">\[
G(z)\ = \sum_{n\in \mathbb{N}} p(n) z^n, \qquad z \in \mathbb{C}
\]</span></p>
<p>is the unique <span class="math inline">\(R \in [0, \infty) \cup \{+ \infty\}\)</span> such that:</p>
<ul>
<li>for every <span class="math inline">\(z \in \mathbb{C}\)</span> with <span class="math inline">\(|z| &gt; R\)</span>, the series <span class="math inline">\(\sum_{n\in \mathbb{N}} p(n) z^n\)</span> diverges.</li>
<li>for every <span class="math inline">\(z \in \mathbb{C}\)</span> with <span class="math inline">\(|z| &lt; R\)</span>, the series <span class="math inline">\(\sum_{n\in \mathbb{N}} p(n) z^n\)</span> is absolutely convergent.</li>
</ul>
<p>The open disk <span class="math inline">\(\{ z : z \in \mathbb{C}, |z| &lt; R \}\)</span> is called the <em>disk of convergence</em> of <span class="math inline">\(G\)</span>. The circle <span class="math inline">\(\{ z : z \in \mathbb{C}, |z| = R \}\)</span> is called the <em>circle of convergence</em> of <span class="math inline">\(G\)</span>.</p>
<p>The radius of convergence <span class="math inline">\(R\)</span> of the probability generating function <span class="math inline">\(G(z) = \sum_{n \in \mathbb{N}} p(n)z^n\)</span> satisfies</p>
<p><span class="math display">\[
\frac{1}{R} =  \limsup_n (p(n))^{1/n} \, .
\]</span></p>
</div>
<p>The last statement is called Hadamardâ€™s rule for determination of the radius of convergence:</p>
<p>The radius of convergence of a probability generating function is always at least <span class="math inline">\(1\)</span>.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(R\)</span> be the supremum of all real numbers <span class="math inline">\(r\)</span> such that for every <span class="math inline">\(z \in \mathbb{C}\)</span> with <span class="math inline">\(|z| &lt; r\)</span>, the series <span class="math inline">\(\sum_{n\in \mathbb{N}} p(n) z^n\)</span> is absolutely convergent.</p>
<p>As <span class="math inline">\((p(n))_n\)</span> defines an absolutely convergent series, for every <span class="math inline">\(z\)</span> with <span class="math inline">\(|z|\leq 1\)</span>, <span class="math inline">\(\sum_n |p(n)z^n| \leq \sum_n p(n) =1\)</span>. Hence <span class="math inline">\(R\geq 1\)</span>.</p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<div id="exm-radius" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.2</strong></span> The radius of convergence contains qualitative information about tail behavior:</p>
<ul>
<li>For Poisson distributions, the radius of convergence is infinite. This reflects the fast decay of the tail probability of Poisson distributions.</li>
<li>For geometric distributions, <span class="math inline">\(p(n) = q (1-q)^{n-1}\)</span>, the radius of convergence is <span class="math inline">\(1/(1-q)\)</span>.</li>
<li>For power law distributions like <span class="math inline">\(p(n) = n^{-r}/\zeta(r)\)</span> with <span class="math inline">\(r&gt;1\)</span>, the radius of convergence is exactly <span class="math inline">\(1\)</span>.</li>
</ul>
</div>
<p>Just knowing the radius of convergence of a function defined by a Power Series expansion tells us about the smoothness properties of the function.</p>
<div id="thm-radius" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.1</strong></span> If <span class="math inline">\(G\)</span> is defined as a power series <span class="math inline">\(G(z) = \sum_{n \in \mathbb{N}} a_n z^n\)</span> its (complex) derivative is <span class="math inline">\(G'(z)=  \sum_{n \in \mathbb{N}} (n+1) a_{n+1} z^n\)</span>. The derivative <span class="math inline">\(G'\)</span> and <span class="math inline">\(G\)</span> have the same radius of convergence.</p>
</div>
<p>This general statement about power series entails a very useful corollary for probability generating functions.</p>
<div id="cor-invpgf" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 7.1 (Inversion formula)</strong></span> Let <span class="math inline">\(f\)</span> be the probability generating function associated with the probability mass function <span class="math inline">\(p\)</span>. Then <span class="math inline">\(f\)</span> is infinitely many times differentiable over <span class="math inline">\([0,1)\)</span></p>
<p><span class="math display">\[f^{(n)}(s) =  \sum_{k=n}^\infty \frac{k!}{(k-n)!} \times p(k) s^{k-n} \, ,\]</span></p>
<p>more specifically:</p>
<p><span class="math display">\[f^{(n)}(0) =  n! \times p(n) \, .\]</span></p>
<p>A probability distribution over <span class="math inline">\(\mathbb{N}\)</span> is characterized by its probability generating function.</p>
</div>
<br>
<p>
</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The property is true for <span class="math inline">\(n=0\)</span>.</p>
<p>Assume it holds for all integers up to <span class="math inline">\(n\)</span>. For <span class="math inline">\(s\in [0, 1)\)</span> and <span class="math inline">\(|h|&lt;1-s-\delta\)</span> where <span class="math inline">\(\delta\)</span> is a small positive number,</p>
<p><span class="math display">\[\begin{array}{rl}
  \frac{f^{(n)}(s+h) - f^{(n)}(s)}{h}
  &amp; =  \sum_{k=n}^\infty \frac{k!}{(k-n)!} \times p(k)
    \Big(\sum_{j=0}^{k-n-1}(s+h)^{k-n-1-j} s^{j}  \Big)
\end{array}\]</span></p>
<p>The absolute value of the internal sum is smaller than <span class="math inline">\((k-n) (1-\delta)^{k-n-1}\)</span>. As</p>
<p><span class="math display">\[\sum_{k=n}^\infty \frac{k!}{(k-n-1)!} \times p(k) \times  (1-\delta)^{k-n-1} &lt; \infty\]</span></p>
<p>for all <span class="math inline">\(0&lt;\delta&lt;1\)</span>. By the Dominated Convergence Theorem,</p>
<p><span class="math display">\[\begin{array}{rl}
\lim_{h \to 0} \frac{f^{(n)}(s+h) - f^{(n)}(s)}{h}
    &amp; =
   \sum_{k=n+1}^\infty \frac{k!}{(k-n-1)!} \times p(k) \times s^{k-n-1} \,.
\end{array}\]</span></p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<br>
<p>
</p>
<div id="exm">
<p>The Probability Generating Function of a Poisson distribution with parameter <span class="math inline">\(\mu\)</span> equals <span class="math inline">\(\exp(\mu(s-1))\)</span>. If we meet a probability distribution with such a PGF, we know it is a Poisson distribution.</p>
</div>
<br>
<p>
</p>
<p>Probability Generating Functions allow for easy investigations of sums of independent random variables.</p>
<div id="prp-pgf-sum-product" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.3</strong></span> Let <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> be independent integer-valued random variable, with probability generating functions <span class="math inline">\(G_X\)</span> and <span class="math inline">\(G_Y\)</span>. The probability generating function <span class="math inline">\(G_{X+Y}\)</span> of <span class="math inline">\(X+Y\)</span> is <span class="math inline">\(G_X\times G_Y\)</span>:</p>
<p><span class="math display">\[G_{X+Y} = G_X \times G_Y \, .\]</span></p>
</div>
<br>
<p>
</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof relies on the fact that non-negative convergent series is commutatively convergent.</p>
<p><span class="math display">\[\begin{array}{rl}
\sum_{n=0}^\infty \mathbb{P}\{ X + Y = n\}\times s^n   &amp; =
    \sum_{n=0}^\infty \left( \sum_{k=0}^n p(k) q(n-k) \right) s^n \\
    &amp;  =
    \sum_{k=0}^\infty p(k) s^k  \sum_{n\geq k}^\infty q(n-k) s^{n-k} \\
    &amp; = G_X(s) \times G_Y(s)
\end{array}\]</span></p>
<p>In measure theoretical language, the proposition is a consequence of the Tonelli-Fubini Theorem:</p>
<p><span class="math display">\[\begin{array}{rl}
  G_{X+Y}(s)
  &amp; = \mathbb{E}\left[s^{X+Y}\right] \\
  &amp; = \mathbb{E}\left[s^{X} \times s^{Y}\right]  \\
  &amp; = \int_{\mathbb{R}^2} s^x s^y \mathrm{d}P_X \otimes P_Y(x,y) \\
  &amp; = \int_{\mathbb{R}} \int_{\mathbb{R}} s^x s^y \mathrm{d}P_X(x) \mathrm{d}P_Y(y) \\
  &amp; = \int_{\mathbb{R}} s^y \int_{\mathbb{R}} s^x  \mathrm{d}P_X(x) \mathrm{d}P_Y(y) \\
  &amp; = \int_{\mathbb{R}} s^y G_X(s) \mathrm{d}P_Y(y) \\
  &amp; = G_X(s) \times G_Y(s) \, .
\end{array}\]</span></p>
</div>
<br>
<p>
</p>
<div id="exm-indep-poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.3</strong></span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent Poisson random variables with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span>, then <span class="math inline">\(G_{X+Y}(s) = \exp(\mu(s-1))\times \exp(\nu(s-1))=  \exp((\mu+\nu)(s-1))\)</span>. This is (another) proof that <span class="math inline">\(X+Y\)</span> is Poisson distributed with parameter <span class="math inline">\(\mu+\nu\)</span>.</p>
</div>
<p>A PGF is infinitely many times differentiable inside the (open) disk of convergence. If the radius of convergence is larger than <span class="math inline">\(1\)</span> (as for Poisson distributions), this entails that the PGF is infinitely many times differentiable at <span class="math inline">\(1\)</span>, If the radius of convergence is exactly <span class="math inline">\(1\)</span>, the differentiability on the circle of convergence is not prescribed by general theory.</p>
<div id="thm-pgfintegr" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.2 (Integrability and probability generating functions)</strong></span> Let <span class="math inline">\(X\)</span> be an integer-valued random variable, with probability generating functions <span class="math inline">\(f\)</span>, then</p>
<p><span class="math inline">\(\mathbb{E} X^p &lt; \infty\)</span></p>
<p>iff</p>
<p><span class="math inline">\(f\)</span> is <span class="math inline">\(p\)</span>-times differentiable at <span class="math inline">\(1\)</span> and</p>
<p><span class="math display">\[f^{(p)}(1) = \mathbb{E}\left[X (X-1) \ldots (X-p+1)\right] \, .\]</span></p>
</div>
<br>
<p>
</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Assume that <span class="math inline">\(G\)</span> is <span class="math inline">\(p\)</span>-times differentiable on the left at <span class="math inline">\(1\)</span>.</p>
<p>We need to establish that <span class="math inline">\(|X|\)</span> is <span class="math inline">\(p\)</span>-integrable.</p>
<p>Assume that <span class="math inline">\(|X|\)</span> is <span class="math inline">\(p\)</span>-integrable.</p>
</div>
<br>
<p>
</p>
<p>The next question arises quickly: when is a function from <span class="math inline">\([0,1]\)</span> to <span class="math inline">\([0,\infty)\)</span> a probability generating function? This question is addressed in a broader perspective in the next section.</p>
</section>
<section id="sec-laplace" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="sec-laplace"><span class="header-section-number">7.3</span> Laplace transform</h2>
<p>Laplace transforms characterize probability distributions on <span class="math inline">\([0, \infty).\)</span></p>
<section id="definition-and-elementary-properties" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="definition-and-elementary-properties"><span class="header-section-number">7.3.1</span> Definition and elementary properties</h3>
<div id="def-laplace" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.2</strong></span> Let <span class="math inline">\(P\)</span> be a probability distribution function over <span class="math inline">\([0,\infty]\)</span> with cumulative distribution function <span class="math inline">\(F\)</span>. The Laplace transform of <span class="math inline">\(P\)</span> is the function <span class="math inline">\(U\)</span> from <span class="math inline">\([0,\infty)\)</span> to <span class="math inline">\([0,1]\)</span> defined by</p>
<p><span class="math display">\[U(\lambda) =  \mathbb{E}\left[\mathrm{e}^{- \lambda X}\right] = \int_{[0,\infty)} \mathrm{e}^{- \lambda x} \mathrm{d}F(x) \,\]</span></p>
<p>where <span class="math inline">\(X \sim P\)</span>.</p>
</div>
<div id="rem">
<p>A probability distribution <span class="math inline">\(P\)</span> over <span class="math inline">\(\mathbb{N}\)</span> is also a probability distribution over <span class="math inline">\([0,\infty)\)</span>, as such it has both a probability generating function <span class="math inline">\(G\)</span> and a Laplace transform <span class="math inline">\(U\)</span>. They are connected by</p>
<p><span class="math display">\[U(\lambda) =  G(\mathrm{e}^{-\lambda}) \, .\]</span></p>
<p>Which properties of Probability Generating Functions are also satisfied by Laplace transforms?</p>
</div>
<div id="prp-laplace-prop" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.4</strong></span> If <span class="math inline">\(U: [0,\infty) \to [0,1]\)</span> is the Laplace transform of a probability distribution <span class="math inline">\(P\)</span> over <span class="math inline">\([0, \infty)\)</span>, then</p>
<ul>
<li><span class="math inline">\(U(0)=1\)</span>;</li>
<li><span class="math inline">\(U\)</span> is continuous;</li>
<li><span class="math inline">\(U\)</span> is non-increasing.</li>
<li><span class="math inline">\(U\)</span> is convex.</li>
</ul>
</div>
<div id="exr-check-laplace-prop" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.1</strong></span> Check the assertions in the proposition.</p>
</div>
<p>Can we recognize Laplace transform of probability distributions over <span class="math inline">\([0,\infty)\)</span>? This is the content of the next Theorem (which proof is beyond the reach of this course).</p>
<div id="thm-bernstein" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.3 (Bernsteinâ€™s Theorem)</strong></span> A function <span class="math inline">\(U: (0, \infty) \to (0,\infty)\)</span> is the Laplace transform of a probability distribution over <span class="math inline">\([0,\infty)\)</span> iff</p>
<ul>
<li><span class="math inline">\(U\)</span> is infinitely many times differentiable over <span class="math inline">\((0, \infty)\)</span></li>
<li><span class="math inline">\(U(0)=1\)</span></li>
<li><span class="math inline">\(U\)</span> is <em>completely monotonous</em>: <span class="math inline">\((-1)^k U^{(k)} \geq 0\)</span> over <span class="math inline">\((0, \infty)\)</span></li>
</ul>
</div>
<p>Using the connexion between Probability Generating Functions and Laplace transforms, we are in position to characterize those power series that are Probability Generating Functions.</p>
<div id="cor-pgf" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 7.2</strong></span> A function <span class="math inline">\(G: [0, 1] \to [0,1]\)</span> is the Probability Generating Function of a probability distribution over <span class="math inline">\(\mathbb{N}\)</span> iff</p>
<ul>
<li><span class="math inline">\(G\)</span> is infinitely many times differentiable over <span class="math inline">\((0,1)\)</span></li>
<li><span class="math inline">\(G(1)=1\)</span></li>
<li><span class="math inline">\(G\)</span> is completely monotonous: <span class="math inline">\((-1)^k G^{(k)} \geq 0\)</span> over <span class="math inline">\((0, 1)\)</span></li>
</ul>
</div>
<div id="exm-laplace-gamma" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.4</strong></span> Let <span class="math inline">\(X\)</span> be <span class="math inline">\(\text{Gamma}(p, \nu)\)</span>-distributed. The Laplace transform of (the distribution of) <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[\begin{array}{rl}
U(\lambda)
  &amp; = \int_0^\infty \nu \mathrm{e}^{-\lambda x} \mathrm{e}^{-\nu x} \frac{(\nu x)^{p-1}}{\Gamma(p)} \mathrm{d} x \\
  &amp; = \frac{\nu^p}{(\lambda +\nu)^p} \int_0^\infty (\lambda +\nu) \mathrm{e}^{-(\lambda +\nu) x}  \frac{((\nu+\lambda) x)^{p-1}}{\Gamma(p)} \mathrm{d} x \\
  &amp; = \frac{\nu^p}{(\lambda +\nu)^p} \, .
\end{array}\]</span></p>
</div>
</section>
<section id="widder" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="widder"><span class="header-section-number">7.3.2</span> Injectivity of Laplace transforms and an inversion formula</h3>
<div id="thm-invLaplace" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.4 (Widderâ€™s Theorem)</strong></span> A probability distribution on <span class="math inline">\([0, \infty)\)</span> is characterized by its Laplace transform.</p>
</div>
<p>The construction of the inversion formula relies on deviation inequalities for Poisson distribution. The next proposition is easily checked by using Markovâ€™s inequality with exponential functions and optimization.</p>
<div id="thm-bennettpoisson" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.5 (Tail bounds for Poisson distribution)</strong></span> Let <span class="math inline">\(Z\)</span> be Poisson distributed. Let <span class="math inline">\(h(x) = \mathrm{e}^x - x -1\)</span> and <span class="math inline">\(h^*(x)= (x+1)\log (x+1) -x, x\geq -1\)</span> be its convex dual. Then for all <span class="math inline">\(\lambda \in \mathbb{R}\)</span></p>
<p><span class="math display">\[\log \mathbb{E} \mathrm{e}^{\lambda (Z-\mathbb{E}Z)} = \mathbb{E}Z h(\lambda) \, .\]</span></p>
<p>For <span class="math inline">\(t\geq 0\)</span> <span class="math display">\[
\Pr \Big\{ Z \geq \mathbb{E}Z + t \Big\} \leq \mathrm{e}^{-\mathbb{E}Z h^*\Big(\frac{t}{\mathbb{E}Z}\Big)}
\]</span> and for <span class="math inline">\(0 \leq t \leq \mathbb{E}Z\)</span> <span class="math display">\[
\Pr \Big\{ Z \leq \mathbb{E}Z -t \Big\} \leq \mathrm{e}^{-\mathbb{E}Z h^*\Big(\frac{-t}{\mathbb{E}Z}\Big)} \, .
\]</span></p>
</div>
<div id="rem-bennettpoisson" class="proof remark">
<p><span class="proof-title"><em>Remark 7.1</em>. </span></p>
<ul>
<li>See <a href="031-moments.html#sec-jensensec" class="quarto-xref">Section&nbsp;<span>3.7</span></a>) for the notion of convex duality.</li>
<li>The next bounds on <span class="math inline">\(h^*\)</span> deliver looser but easier tail bounds</li>
</ul>
<p><span class="math display">\[\begin{array}{rll}
  h^*(t) &amp; \geq \frac{t^2}{2(1 + t/3)}    &amp; \text{for } t &gt;0 \\
  h^*(t) &amp; \geq \frac{t^2}{2}             &amp; \text{for } t &lt;0 \, .
\end{array}\]</span></p>
</div>
<div id="cor-corrpoisson" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 7.3</strong></span> For all positive <span class="math inline">\(x, y, y \neq x\)</span>, <span class="math display">\[
\lim_{n \to \infty} \sum_{k=0}^{nx} e^{-n y} \frac{(ny)^k}{k!} = \mathbb{I}_{y&lt;x} \,.
\]</span></p>
</div>
<p>We shall check in one of the next lessons that for <span class="math inline">\(x &gt;0\)</span>: <span class="math display">\[
\lim_{n \to \infty} \sum_{k=0}^{\lfloor nx\rfloor} e^{-n x} \frac{(nx)^k}{k!} = \frac{1}{2} \, .
\]</span></p>
<p><br></p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(F\)</span> be the cumulative distribution function of <span class="math inline">\(P\)</span> and <span class="math inline">\(U\)</span> its Laplace transform. Let <span class="math inline">\(X \sim P\)</span>.</p>
<p>It suffices to show that <span class="math inline">\(F(x)\)</span> can be computed from <span class="math inline">\(U\)</span> at any <span class="math inline">\(x\)</span> where <span class="math inline">\(F\)</span> is continuous.</p>
<p>Function <span class="math inline">\(U\)</span> is infinitely many times differentiable on <span class="math inline">\((0, \infty)\)</span>. For <span class="math inline">\(k\in  \mathbb{N},\)</span> <span class="math display">\[
    \frac{\mathrm{d}^kU}{\mathrm{d}\lambda^k}  = (-1)^k \int_{[0,\infty)} x^k e^{-\lambda x} \mathrm{d}F(x)  \, .
\]</span> and <span class="math inline">\(U\)</span> has a power series expansion at every <span class="math inline">\(\lambda \in (0,1)\)</span>, for <span class="math inline">\(\lambda' \in (0,1)\)</span>:</p>
<p><span class="math display">\[\begin{array}{rl}
U(\lambda')
   &amp; = \sum_{k=0}^\infty \frac{(\lambda' -\lambda)^k}{k!} \frac{\mathrm{d}^kU}{\mathrm{d}\lambda^k}  \, .
\end{array}\]</span></p>
<p>By [<a href="#cor-corrpoisson" class="quarto-xref">Corollary&nbsp;<span>7.3</span></a>), for all <span class="math inline">\(0 &lt; y \neq x\)</span>, <span class="math inline">\(\lim_{n \to \infty} \sum_{k=0}^{nx} e^{-n y} \frac{(ny)^k}{k!} = \mathbb{I}_{y&lt;x}\)</span>.</p>
<p><span class="math display">\[\begin{array}{rl}
F(x)
  &amp; =  \int_{\mathbb{R_+}} \mathbb{I}_{y\leq x} \mathrm{d}F(y) \\
  &amp; = \int_{\mathbb{R_+}} \mathbb{I}_{y&lt; x} \mathrm{d}F(y) \\
  &amp; = \int_{(-\infty, x)} \mathbb{I}_{y&lt; x} \mathrm{d}F(y) + \int_{\{x\}} 1 \mathrm{d}F(y) + \int_{(x, \infty)} \mathbb{I}_{y&lt; x} \mathrm{d}F(y) \\
  &amp; = \int_{(-\infty, x)} \mathbb{I}_{y&lt; x} \mathrm{d}F(y) + \int_{\{x\}} 1 \mathrm{d}F(y) + \int_{(x, \infty)} \mathbb{I}_{y&lt; x} \mathrm{d}F(y) \\
  &amp; =  \int_{(-\infty, x) \cup (x, \infty)} \lim_{n \to \infty} \sum_{k=0}^{nx} e^{-n y} \frac{(ny)^k}{k!} \mathrm{d}F(y) + \int_{\{x\}} 1 \mathrm{d}F(y) \\
  &amp; =  \lim_{n \to \infty} \sum_{k=0}^{nx} \frac{(-n)^k}{k!}\int_{(-\infty, x) \cup (x, \infty)}  e^{-n y} {(-y)^k} \mathrm{d}F(y) + \int_{\{x\}} 1 \mathrm{d}F(y)\\
       &amp;  \text{by dominated convergence} \\
  &amp; =  \lim_{n \to \infty} \sum_{k=0}^{nx} \frac{(-n)^k}{k!} \frac{\mathrm{d}^kU}{\mathrm{d}\lambda^k}_{\mid \lambda=n}  \, .
\end{array}\]</span></p>
<p>If <span class="math inline">\(F\)</span> is continuous at <span class="math inline">\(x\)</span>, <span class="math display">\[
F(x) = \lim_{n \to \infty} \sum_{k=0}^{nx} \frac{(-n)^k}{k!} \frac{\mathrm{d}^kU}{\mathrm{d}\lambda^k}_{\mid \lambda=n} \, .
\]</span> If <span class="math inline">\(F\)</span> jumps at <span class="math inline">\(x\)</span>, <span class="math display">\[
F(x) - \frac{P\{X=x\}}{2} =\lim_{n \to \infty} \sum_{k=0}^{nx} \frac{(-n)^k}{k!} \frac{\mathrm{d}^kU}{\mathrm{d}\lambda^k}_{\mid \lambda=n} \, .
\]</span> This process shows that the Laplace transform contains enough information to reconstruct the distribution function which in turn characterizes the probability distribution.</p>
</div>
<p><br></p>
<p>Laplace transforms of sums of independent non-negative random variables are easily obtained from the Laplace transforms of the summands.</p>
<p><br></p>
<div id="prp-laplace-sums" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.5</strong></span> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two independent <span class="math inline">\([0,\infty)\)</span>-valued random variables, with Laplace transforms <span class="math inline">\(U_X\)</span> and <span class="math inline">\(U_Y\)</span>. The Laplace transform of (the distribution of) <span class="math inline">\(X+Y\)</span> is <span class="math display">\[
G_{X+Y} = G_X \times G_Y \, .
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{array}{rl}
G_{X+Y}(\lambda)
  &amp; = \mathbb{E}\Big[\mathrm{e}^{\lambda (X+Y)}\Big] \\
  &amp; = \mathbb{E}\Big[\mathrm{e}^{\lambda X} \times \mathrm{e}^{\lambda Y}\Big] \\
  &amp; = \mathbb{E}\Big[\mathrm{e}^{\lambda X} \Big] \times \mathbb{E}\Big[\mathrm{e}^{\lambda Y}\Big]\\
  &amp; \text{independence}\\
  &amp; = G_X(\lambda) \times G_Y(\lambda) \, .
\end{array}\]</span></p>
</div>
<br>
<p>
</p>
<p>Combining the inversion theorem and the explicit formula for the Laplace transform of Gamma distributions, we recover the fact that sums of independent Gamma-distributed random variables with the same intensity parameter is also Gamma distributed.</p>
<div id="cor">
<p>If <span class="math inline">\(X \sim \text{Gamma}(p, \lambda)\)</span> is independent from <span class="math inline">\(Y \sim \text{Gamma}(q, \lambda)\)</span> then <span class="math inline">\(X+Y\)</span> has Laplace transform <span class="math inline">\(\Big(\frac{\nu}{\lambda+\nu}\Big)^{p+q}\)</span> and is <span class="math inline">\(\text{Gamma}(p+q, \lambda)\)</span>-distributed.</p>
</div>
</section>
</section>
<section id="sec-charfun" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="sec-charfun"><span class="header-section-number">7.4</span> Characteristic functions and Fourier transforms</h2>
<p>The Laplace transform characterizes probability distributions supported by <span class="math inline">\([0, \infty)\)</span>. Characteristic functions deal with general probability distributions. They extend to multivariate distributions.</p>
<section id="charFuncDef" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="charFuncDef"><span class="header-section-number">7.4.1</span> Characteristic function</h3>
<p>The next transform can be defined for all probability distributions over <span class="math inline">\(\mathbb{R}\)</span>. And the definition can be extended to distributions on <span class="math inline">\(\mathbb{R}^k, k\geq 1\)</span>.</p>
<div id="def">
<p>Let the real-valued random variable <span class="math inline">\(X\)</span> be distributed according to <span class="math inline">\(P\)</span> with cumulative distribution function <span class="math inline">\(F\)</span>, the characteristic function of distribution <span class="math inline">\(P\)</span> is the function from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{C}\)</span> defined by <span class="math display">\[
    \widehat{F}(t) = \mathbb{E}\left[\mathrm{e}^{i t X}\right]
  = \int_{\mathbb{R}} \mathrm{e}^{i t x} \mathrm{d}F(x)
  = \int_{\mathbb{R}} \cos(t x) \mathrm{d}F(x) + i \int_{\mathbb{R}} \sin(t x) \mathrm{d}F(x) \, .
\]</span></p>
</div>
<div id="rem">
<p>If <span class="math inline">\(F\)</span> is absolutely continuous with density <span class="math inline">\(f\)</span> then <span class="math inline">\(\widehat{F}\)</span> is (up to a multiplicative constant) the Fourier transform of <span class="math inline">\(f\)</span>.</p>
</div>
<div id="prp">
<p>Let the real-valued random variable <span class="math inline">\(X\)</span> be distributed according to <span class="math inline">\(P\)</span> with characteristic function <span class="math inline">\(\widehat{F}\)</span>.</p>
<ul>
<li><span class="math inline">\(\widehat{F}\)</span> is (uniformly) continuous over <span class="math inline">\(\mathbb{R}\)</span></li>
<li><span class="math inline">\(\widehat{F}(0)=1\)</span></li>
<li>If <span class="math inline">\(X\)</span> is symmetric, <span class="math inline">\(\widehat{F}\)</span> is real-valued</li>
<li>The characteristic function of the distribution of <span class="math inline">\(a X +b\)</span> is <span class="math display">\[\mathrm{e}^{it b} \widehat{F}(at) \, .\]</span></li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let us check the continuity property. The three others are left as exercises.</p>
<p>Trigonometric calculus leads to <span class="math display">\[\begin{array}{rl}
\Big| \mathrm{e}^{i(t+ \delta)x} - \mathrm{e}^{itx}\Big|
  &amp; = \Big| \mathrm{e}^{itx}\Big| \times \Big|\mathrm{e}^{i\delta x} - 1\Big|\\
  &amp; \leq \Big|\mathrm{e}^{i\delta x} - 1\Big| \\
  &amp; \leq 2 \Big( 1 \wedge \big| \delta x \big| \Big)
\end{array}\]</span> for every <span class="math inline">\(t\in \mathbb{R}, \delta \in \mathbb{R}, x \in \mathbb{R}\)</span>. Taking integration with respect to <span class="math inline">\(F\)</span>, <span class="math display">\[\begin{array}{rl}
  \Big| \widehat{F}(t+\delta) - \widehat{F}(t)\Big|
  &amp; \leq \int 2 \Big( 1 \wedge \big| \delta x \big| \Big) \mathrm{d}F(x) \,.
\end{array}\]</span> Resorting to the dominated convergence theorem, we conclude <span class="math display">\[
\lim_{\delta \to 0} \Big| \widehat{F}(t+\delta) - \widehat{F}(t)\Big| = 0
\]</span> uniformly in <span class="math inline">\(t\)</span>.</p>
</div>
<br>
<p>
</p>
<div id="exr">
<p>The next properties are easily checked:</p>
<ul>
<li><span class="math inline">\(|\widehat{F}(t)|\leq 1\)</span> for every <span class="math inline">\(t\in \mathbb{R}\)</span>;</li>
<li></li>
</ul>
</div>
<br>
<p>
</p>
<div id="exm">
<p>Compute the characteristic function of:</p>
<ul>
<li>The Poisson distribution with parameter <span class="math inline">\(\lambda&gt;0\)</span>;</li>
<li>The uniform distribution on <span class="math inline">\([-1,1]\)</span>;</li>
<li>The triangle distribution on <span class="math inline">\([-1,1]\)</span> (density: <span class="math inline">\(1-|x|\)</span> on <span class="math inline">\([-1,1]\)</span>);</li>
<li>The exponential distribution with density <span class="math inline">\(\exp(-x)\)</span> on <span class="math inline">\([0,+\infty)\)</span>;</li>
<li>The Laplace distribution, density <span class="math inline">\(1/2 \exp(-|x|)\)</span>.</li>
</ul>
</div>
<br>
<p>
</p>
<p>Just as Probability Generating Functions and Laplace transforms, Characteristic functions of sums of independent random variables have a simple structure.</p>
<div id="prp-fouriersum" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.6</strong></span> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables with cumulative distribution functions <span class="math inline">\(F_X\)</span> and <span class="math inline">\(F_Y\)</span>, then</p>
<p><span class="math display">\[\widehat{F}_{X+Y}(t) =  \widehat{F}_X(t) \times \widehat{F}_Y(t)\]</span></p>
<p>for all <span class="math inline">\(t \in \mathbb{R}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The third equality is a consequence of the independence of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>: <span class="math display">\[\begin{array}{rl}
\widehat{F}_{X+Y}(t)
  &amp; = \mathbb{E}\Big[\mathrm{e}^{it (X+Y)}\big] \\
  &amp; = \mathbb{E}\Big[\mathrm{e}^{it X} \mathrm{e}^{it Y}\big] \\
  &amp; = \mathbb{E}\Big[\mathrm{e}^{it X} \big] \times \mathbb{E}\big[\mathrm{e}^{it Y}\big] \\
  &amp; =  \widehat{F}_X(t) \times \widehat{F}_Y(t) \, .
\end{array}\]</span></p>
</div>
<div id="exr">
<p>Use a counter-example to prove that <span class="math display">\[
\Big(\forall t \in \mathbb{R}, \quad \widehat{F}_{X+Y}(t) =  \widehat{F}_X(t) \times \widehat{F}_Y(t) \Big)
\not\Rightarrow X \perp\!\!\!\perp Y \, .
\]</span></p>
</div>
</section>
<section id="charfungaussian" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="charfungaussian"><span class="header-section-number">7.4.2</span> Characteristic function of a univariate Gaussian distribution</h3>
<p>It is possible to compute characteristic functions by resorting to Complex Analysis. But we shall refrain from this when computing the most important characteristic function, the characteristic function of the standard Gaussian distribution.</p>
<div id="prp-proCharFunGauss" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.7</strong></span> Let <span class="math inline">\(\widehat{\Phi}\)</span> denote the characteristic function of the standard univariate Gaussian distribution <span class="math inline">\(\mathcal{N}(0,1)\)</span>, the following holds</p>
<p><span class="math display">\[\widehat{\Phi}(t) = \mathrm{e}^{-\frac{t^2}{2}} \, .\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Recall that as the standard Gaussian density is even, the characteristic function is real-valued and even.</p>
<p>Moreover, <span class="math inline">\(\widehat{\Phi}\)</span> is differentiable and the derivative can be computing by interverting expectation and derivation with respect to <span class="math inline">\(t\)</span>. <span class="math display">\[\begin{array}{rl}
\widehat{\Phi}'(t)
&amp; = - \mathbb{E}\left[X \sin(t X) \right] \\
&amp; =  - \frac{1}{\sqrt{2 \pi}}\int_{\mathbb{R}} x \sin(tx) \mathrm{e}^{-\frac{x^2}{2}} \mathrm{d}x \\
&amp; = \frac{1}{\sqrt{2 \pi}} \Big[\sin(tx) \mathrm{e}^{-\frac{x^2}{2}} \Big]_{-\infty}^{\infty} - t \frac{1}{\sqrt{2 \pi}}\int_{\mathbb{R}}  \cos(tx) \mathrm{e}^{-\frac{x^2}{2}} \mathrm{d}x \\
&amp; = - t \widehat{\Phi}(t) \,.
\end{array}\]</span> Hence, <span class="math inline">\(\widehat{F}\)</span> is a solution of the differential equation: <span class="math inline">\(g'(t) = -t g(t)\)</span> with <span class="math inline">\(g(0)=1\)</span>.</p>
<p>The differential equation is readily solved, and the solution is <span class="math inline">\(g(t)= \mathrm{e}^{- \frac{t^2}{2}}\)</span>.</p>
</div>
<br>
<p>
</p>
<div id="exr">
<p>Why is <span class="math inline">\(\widehat{\Phi}\)</span> differentiable? Why are we allowed to interchange expectation and derivation?</p>
</div>
<br>
<p>
</p>
<p>Note that a byproduct of Proposition @ref(prp:proCharFunGauss) is the following integral representation of the Gaussian density.</p>
<p><span class="math display">\[\phi(x)  =  \frac{1}{2 \pi} \int_{\mathbb{R}} \widehat{\Phi}(t) \mathrm{e}^{-itx} \mathrm{d}t \, .\]</span></p>
<p>It does not look interesting, but it is a milestone for the derivation of the general inversion formula below.</p>
</section>
<section id="convol" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="convol"><span class="header-section-number">7.4.3</span> Sums of independent random variables and convolutions</h3>
<p>The interplay between Characteristic functions/Fourier transforms and summation of independent random variables is one of the most attractive features of this transformation. In order to understand it, we shall need an operation stemming from analysis. Recall that if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are two integrable functions, the convolution of <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> is defined as <span class="math display">\[f \star g (x)  = \int_{\mathbb{R}} f(x-y)g(y) \mathrm{d}y = \int_{\mathbb{R}} g(x-y)f(y) \mathrm{d}y \, .\]</span> Note that <span class="math inline">\(f \star g\)</span> is also integrable. It is not too hard to check that if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are two probability densities then so is <span class="math inline">\(f \star g\)</span>, moreover <span class="math inline">\(f \star g\)</span> is the density of the distribution of <span class="math inline">\(X+Y\)</span> where <span class="math inline">\(x \sim f\)</span> is independent from <span class="math inline">\(Y \sim g\)</span>. The next proposition extends this observation.</p>
<div id="prp-convsum" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.8</strong></span> Let <span class="math inline">\(X,Y\)</span> be two independent random variables with distributions <span class="math inline">\(P_X\)</span> and <span class="math inline">\(P_Y\)</span>. Assume that <span class="math inline">\(P_X\)</span> is absolutely continuous with density <span class="math inline">\(p_X\)</span>. Then the distribution of <span class="math inline">\(X+Y\)</span> is absolutely continuous and has density <span class="math display">\[
p_x \star P_Y (z) = \int_{\mathbb{R}} p_X(z -y ) \mathrm{d}P_Y(y) \, .
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(B\)</span> be Borel subset of <span class="math inline">\(\mathbb{R}\)</span>. <span class="math display">\[\begin{array}{rl}
P \Big\{ X+Y  \in B\Big\}
  &amp; = \int_{\mathbb{R}} \Big( \int_{\mathbb{R}} \mathbb{I}_B(x+y) p_X(x)\mathrm{d}x\Big) \mathrm{d}P_Y(y) \\
  &amp; = \int_{\mathbb{R}} \Big(\int_{\mathbb{R}} \mathbb{I}_B(z) p_X(z-y)\mathrm{d}z\Big) \mathrm{d}P_Y(y) \\
  &amp; = \int_{\mathbb{R}} \mathbb{I}_B(z) \Big(\int_{\mathbb{R}}  p_X(z-y) \mathrm{d}P_Y(y) \Big) \mathrm{d}z \\
  &amp; = \int_{\mathbb{R}} \mathbb{I}_B(z) p_x \star P_Y (z) \mathrm{d}z
\end{array}\]</span> where the first equality follows from the Tonelli-Fubini Theorem, the second equality is obtained by change of variable <span class="math inline">\(x \mapsto z = x+y\)</span> for every <span class="math inline">\(y\)</span>, the third equality follows again from the Tonelli-Fubini Theorem.</p>
</div>
<p><br></p>
<div id="rem">
<p>Convolution is not tied to Probability theory.</p>
<ul>
<li>In Analysis, convolution is known to be a regularizing (smoothing) operation. This also holds in Probability theory: if the distribution of either <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> has a density and <span class="math inline">\(X \perp\!\!\!\perp Y\)</span>, then the distribution of <span class="math inline">\(X+Y\)</span> has a density.</li>
<li>Convolution with smooth distributions plays an important role in non-parametric statsitics, it is at the root of kernel density estimation.</li>
<li>Convolution is an important tool in Signal Processing.</li>
</ul>
</div>
<div id="exr">
<p>Check that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent with densities <span class="math inline">\(f_X\)</span> and <span class="math inline">\(f_Y\)</span>, <span class="math inline">\(f_X \star f_Y\)</span> is a density of the distribution of <span class="math inline">\(X+Y\)</span>.</p>
</div>
<p><br></p>
<p>If <span class="math inline">\(Y =0\)</span> almost surely (its distribution is <span class="math inline">\(\delta_0\)</span>), then <span class="math inline">\(p_X \star \delta_0 = p_X\)</span>.</p>
<p>What happens in Proposition @ref(prp:convsum), if we consider the distributions of <span class="math inline">\(\sigma X +Y\)</span> and let <span class="math inline">\(\sigma\)</span> decrease to <span class="math inline">\(0\)</span>?</p>
<div id="prp-approxident" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.9</strong></span> Let <span class="math inline">\(X,Y\)</span> be two independent random variables with distributions <span class="math inline">\(P_X\)</span> and <span class="math inline">\(P_Y\)</span>. Assume that <span class="math inline">\(P_X\)</span> is absolutely continuous with density <span class="math inline">\(p_X\)</span> and that <span class="math inline">\(P_X(-\infty, 0] = \alpha \in (0,1)\)</span>. Then <span class="math display">\[
\lim_{\sigma \downarrow 0} \mathbb{P}\big\{ Y + \sigma X \leq a \Big\} = P_Y(-\infty, a) + \alpha P_Y\{a\} \, .
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{array}{rl}
\mathbb{P}\big\{ Y + \sigma X \leq a \Big\}
    &amp; = \int_{\mathbb{R}} \int_{\mathbb{R}} \mathbb{I}_{x \leq  \frac{a-y}{\sigma}} p_X(x) \mathrm{d}x \mathrm{d}P_Y(y) \\
    &amp; = \int_{(-\infty,a)} \int_{\mathbb{R}} \mathbb{I}_{x \leq  \frac{a-y}{\sigma}} p_X(x) \mathrm{d}x \mathrm{d}P_Y(y) \\
    &amp; + \int_{\mathbb{R}} \mathbb{I}_{x \leq  \frac{a-a}{\sigma}} p_X(x) \mathrm{d}x P_Y\{a\} \\
    &amp; + \int_{(a, \infty)} \int_{\mathbb{R}} \mathbb{I}_{x \leq  \frac{a-y}{\sigma}} p_X(x) \mathrm{d}x \mathrm{d}P_Y(y)
\end{array}\]</span> By monotone convergence, the first and third integrals converge respectively to <span class="math inline">\(P_Y(-\infty, a)\)</span> and <span class="math inline">\(0\)</span> while the second term equals <span class="math inline">\(\alpha P_Y\{a\}\)</span>.</p>
</div>
</section>
<section id="charinjectivity" class="level3" data-number="7.4.4">
<h3 data-number="7.4.4" class="anchored" data-anchor-id="charinjectivity"><span class="header-section-number">7.4.4</span> Injectivity Theorem and inversion formula</h3>
<p>The characteristic function maps probability measures to <span class="math inline">\(\mathbb{C}\)</span>-valued functions. The main result of this section is that characteristic functions/Fourier transforms define is an injective operator on the set of Probability measures on the real line.</p>
<div id="thm-injeccharfunc" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.6</strong></span> If two probability distribution <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> have the same characteristic function, they are equal.</p>
</div>
<p>The injectivity property follows from an explicit inversion recipe. The characteristic function allows us to recover the cumulative distribution function at all its continuity points (just as the Laplace transform did). Again, as continuity points of cumulative distribution functions are dense on <span class="math inline">\(\mathbb{R}\)</span>, this is enough.</p>
<div id="prp-reginversion" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.10</strong></span> Let <span class="math inline">\(X \sim F\)</span> and <span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span> be independent. Let <span class="math inline">\(Y = X + \sigma Z\)</span>, then:</p>
<ul>
<li>the distribution of <span class="math inline">\(Y\)</span> has characteristic function <span class="math display">\[
  \widehat{F}_\sigma(t) = \widehat{\Phi}(t\sigma) \times \widehat{F}(t) = \mathrm{e}^{- \frac{t^2 \sigma^2}{2}} \widehat{F}(t)
\]</span></li>
<li>the distribution of <span class="math inline">\(Y\)</span> is absolutely continuous with respect to Lebesgue measure</li>
<li>a version of the density of the distribution of <span class="math inline">\(Y\)</span> is given by <span class="math display">\[
  \frac{1}{{2 \pi}}\int_{\mathbb{R}} \mathrm{e}^{- \frac{t^2 \sigma^2}{2}} \widehat{F}(t)\mathrm{e}^{-ity} \mathrm{d}t
= \frac{1}{{2 \pi}}\int_{\mathbb{R}}  \widehat{F}_\sigma(t)\mathrm{e}^{-ity} \mathrm{d}t \,.
\]</span></li>
</ul>
</div>
<div id="exr">
<p>Why can we take for granted the existence of a probability space with two independent random variables <span class="math inline">\(X, Z\)</span> distributed as above?</p>
</div>
<p>The proposition states that a density of the distribution of <span class="math inline">\(X + \sigma Z\)</span> can be recovered from the characteristic function of the distribution of <span class="math inline">\(X + \sigma Z\)</span> by the Fourier inversion formula for functions with integrable Fourier transforms.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The fact that for any <span class="math inline">\(\sigma &gt;0\)</span>, the distribution of <span class="math inline">\(Y = X + \sigma Z\)</span> is absolutely continuous with respect to Lebesgue measure comes from Proposition @ref(prp:convsum).</p>
<p>A density of the distribution of <span class="math inline">\(X + \sigma Z\)</span> is given by <span class="math display">\[
    \int_{\mathbb{R}} \frac{1}{\sigma} \phi\Big(\frac{y -x}{\sigma}\Big) \mathrm{d}F(x)
\]</span> The characteristic function of <span class="math inline">\(Y\)</span> at <span class="math inline">\(t\)</span> is <span class="math inline">\(\mathrm{e}^{- \frac{t^2 \sigma^2}{2}} \widehat{F}(t)\)</span>.</p>
<p><span class="math display">\[\begin{array}{rl}
\mathbb{P}\Big\{ Y \leq u\Big\}
&amp; = \int_{-\infty}^u \int_{\mathbb{R}} \frac{1}{\sigma} \phi\Big(\frac{y -x}{\sigma}\Big) \mathrm{d}F(x)  \mathrm{d}y \\
&amp; = \int_{-\infty}^u \int_{\mathbb{R}} \frac{1}{\sigma}
\left(\frac{1}{{2 \pi}} \int_{\mathbb{R}} \mathrm{e}^{- \frac{t^2}{2}} \mathrm{e}^{-it  \frac{y-x}{\sigma}} \mathrm{d}t\right)
\mathrm{d}F(x)  \mathrm{d}y \\
&amp; = \int_{-\infty}^u \left(\int_{\mathbb{R}} \frac{1}{\sigma}
\frac{1}{{2 \pi}} \mathrm{e}^{- \frac{t^2}{2}} \mathrm{e}^{-\frac{ity}{\sigma}} \left(\int_{\mathbb{R}}  \mathrm{e}^{\frac{itx}{\sigma}} \mathrm{d}F(x)\right)  \mathrm{d}t\right)
\mathrm{d}y \\
&amp; = \int_{-\infty}^u \left(\int_{\mathbb{R}} \frac{1}{\sigma}
\frac{1}{{2 \pi}} \mathrm{e}^{- \frac{t^2}{2}} \mathrm{e}^{-\frac{ity}{\sigma}} \widehat{F}(t/\sigma)  \mathrm{d}t\right)
\mathrm{d}y \\
&amp; = \int_{-\infty}^u  \left( \frac{1}{{2 \pi}}\int_{\mathbb{R}} \mathrm{e}^{- \frac{t^2 \sigma^2}{2}}\mathrm{e}^{-ity} \widehat{F}(t)\mathrm{d}t   \right) \mathrm{d}y \,.
\end{array}\]</span> The quantity <span class="math inline">\(\left( \frac{1}{{2 \pi}}\int_{\mathbb{R}} \mathrm{e}^{- \frac{t^2 \sigma^2}{2}}\mathrm{e}^{-ity} \widehat{F}(t)\mathrm{d}t   \right)\)</span> is a version of the density of the distribution of <span class="math inline">\(Y = X + \sigma Z\)</span> (why?). Note that it is obtained from the same inversion formula that readily worked for the Gaussian density.</p>
</div>
<br>
<p>
</p>
<p>Now we have to show that an inversion formula works for all probability distributions, not only for the smooth probability distributions obtained by adding Gaussian noise. We shall check that we can recover the distribution function from the Fourier transform.</p>
<div id="thm-invfourier1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.7</strong></span> Let <span class="math inline">\(X\)</span> be distributed according to <span class="math inline">\(P\)</span>, with cumulative distribution function <span class="math inline">\(F\)</span> and characteristic function <span class="math inline">\(\widehat{F}\)</span>.</p>
<p>Then: <span class="math display">\[
\lim_{\sigma \downarrow 0} \int_{-\infty}^u  \left( \frac{1}{{2 \pi}}\int_{\mathbb{R}} \mathrm{e}^{-ity}  \mathrm{e}^{- \frac{t^2 \sigma^2}{2}}\widehat{F}(t)\mathrm{d}t   \right) \mathrm{d}y =  F(u_-) + \frac{1}{2} P\{u\}
\]</span> where <span class="math display">\[
F(u_-) = \lim_{v \uparrow u} F(v) = P(-\infty, u)\, .
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof consists in combining Propositions @ref(prp:approxident) and @ref(prp:reginversion).</p>
</div>
<br>
<p>
</p>
<p>Note that <a href="#thm-invfourier1" class="quarto-xref">Theorem&nbsp;<span>7.7</span></a>) does not deliver directly the distribution function <span class="math inline">\(F\)</span>. Indeed, if <span class="math inline">\(F\)</span> is not continuous, <span class="math inline">\(u \mapsto \widetilde{F}(u) = F(u_-) + \frac{1}{2} P\{u\}\)</span>, is not a distribution function. But the right-continuous modification of <span class="math inline">\(\widetilde{F}\)</span>: <span class="math inline">\(u \mapsto \lim_{v \downarrow u} \widetilde{F}(v)\)</span> coincides with <span class="math inline">\(F\)</span>. We have established <a href="#thm-injeccharfunc" class="quarto-xref">Theorem&nbsp;<span>7.6</span></a>).</p>
<br>
<p>
</p>
<p>When the distribution function is absolutely continuous, Fourier inversion is simpler.</p>
<div id="thm">
<p>Let <span class="math inline">\(X\)</span> be distributed according to <span class="math inline">\(P\)</span>, with cumulative distribution function <span class="math inline">\(F\)</span> and characteristic function <span class="math inline">\(\widehat{F}\)</span>. Assume that <span class="math inline">\(\widehat{F}\)</span> is integrable (with respect to Lebesgue measure). Then:</p>
<ul>
<li><span class="math inline">\(P\)</span> is absolutely continuous with respect to Lebesgue measure;</li>
<li><span class="math inline">\(y \mapsto \frac{1}{{2 \pi}} \int_{\mathbb{R}} \widehat{F}(t) \mathrm{e}^{-ity} \mathrm{d}t\)</span> is a uniformly continuous version of the density of <span class="math inline">\(P\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(X\)</span> be distributed according to <span class="math inline">\(P\)</span> with cumulative distribution function <span class="math inline">\(F\)</span> and characteristic function <span class="math inline">\(\widehat{F}\)</span>. Let <span class="math inline">\(Z\)</span> be independent from <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathcal{N}(0,1)\)</span>. Let <span class="math inline">\(x\)</span> be a continuity point of <span class="math inline">\(F\)</span>.</p>
<p><span class="math display">\[
\lim_{\sigma \downarrow 0} P\Big\{ X + \sigma Z  \leq x \Big\} = F(x)
\]</span></p>
<p><span class="math display">\[\begin{array}{rl}
\lim_{\sigma \downarrow 0} P\Big\{ X + \sigma Z  \leq x \Big\}
&amp; = \lim_{\sigma \downarrow 0}
\int_{-\infty}^x  \left( \frac{1}{{2 \pi}}\int_{\mathbb{R}} \mathrm{e}^{- \frac{t^2 \sigma^2}{2}}\mathrm{e}^{-ity} \widehat{F}(t)\mathrm{d}t   \right) \mathrm{d}y \\
&amp; = \int_{-\infty}^x   \frac{1}{{2 \pi}}\int_{\mathbb{R}} \lim_{\sigma \downarrow 0} \mathrm{e}^{- \frac{t^2 \sigma^2}{2}}\mathrm{e}^{-ity} \widehat{F}(t)\mathrm{d}t    \mathrm{d}y \\
&amp; = \int_{-\infty}^x   \frac{1}{{2 \pi}}\int_{\mathbb{R}} \mathrm{e}^{-ity} \widehat{F}(t)\mathrm{d}t    \mathrm{d}y \,
\end{array}\]</span> where interversion of limit and integration is justified by dominated convergence.</p>
</div>
<br>
<p>
</p>
<p>We close this section by an alternative inversion formula.</p>
<div id="thm-invformula" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.8 (Inversion formula)</strong></span> Let <span class="math inline">\(P\)</span> be a probability distribution over <span class="math inline">\(\mathbb{R}\)</span> with cumulative distribution function <span class="math inline">\(F\)</span>, then <span class="math display">\[
\lim_{T \to \infty} \frac{1}{2\pi} \int_{-T}^T \frac{\mathrm{e}^{-it a} - \mathrm{e}^{-it b}}{it} \widehat{F}(t) \mathrm{d}t
= F(b_-) - F(a) + \frac{1}{2} \Big(P\{b\} + P\{a\}\Big) \, .
\]</span></p>
</div>
<br>
<p>
</p>
<p>The proof of <a href="#thm-invformula" class="quarto-xref">Theorem&nbsp;<span>7.8</span></a>) can be found in textbooks like <span class="citation" data-cites="Dur10">(<a href="#ref-Dur10" role="doc-biblioref">Durrett, 2010</a>)</span> or <span class="citation" data-cites="MR2893652">(<a href="#ref-MR2893652" role="doc-biblioref">Billingsley, 2012</a>)</span>.</p>
<div id="cor">
<p>Let <span class="math inline">\(\widehat{F}\)</span> denote the characteristic function of the probability distribution <span class="math inline">\(P\)</span>, if <span class="math inline">\(\widehat{F}(t) = \mathrm{e}^{-\frac{t^2}{2}}\)</span>, then <span class="math inline">\(P\)</span> is the standard univariate Gaussian distribution (<span class="math inline">\(\mathcal{N}(0,1)\)</span>).</p>
</div>
<div id="cor">
<p>Let <span class="math inline">\(\widehat{F}\)</span> denote the characteristic function of probability distribution <span class="math inline">\(P\)</span>, if <span class="math inline">\(\widehat{F}(t) = \mathrm{e}^{i\mu t -\frac{\sigma^2 t^2}{2}}\)</span>, then <span class="math inline">\(P\)</span> is the Gaussian distribution ( <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span> ).</p>
</div>
<p>Another important byproduct of the proof of injectivity of the characteristic function is Steinâ€™s identity, an important property of the standard Gaussian distribution.</p>
<div id="thm-steinIdentity" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.9 (Steinâ€™s identity)</strong></span> Let <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span>, and <span class="math inline">\(g\)</span> be a differentiable function such that <span class="math inline">\(\mathbb{E}|g'(X)|&lt; \infty\)</span>, then<span class="math display">\[
\mathbb{E}[g'(X)] = \mathbb{E}[Xg(X)] \, .
\]</span> Conversely, if <span class="math inline">\(X\)</span> is a random variable such that <span class="math display">\[
\mathbb{E}[g'(X)] = \mathbb{E}[X g(X)]
\]</span> holds for any differentiable funtion <span class="math inline">\(g\)</span> such that <span class="math inline">\(g'\)</span> is integrable, then <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The direct part follows by integration by parts.</p>
<p>To check the converse, note that if <span class="math inline">\(X\)</span> satisfies the identity in the Theorem, then for all <span class="math inline">\(t \in \mathbb{R}\)</span>, the functions <span class="math inline">\(t \mapsto \mathbb{E} \cos(tX)\)</span> and <span class="math inline">\(t \mapsto \mathbb{E} \sin(tX)\)</span> satisfy the differential equation <span class="math inline">\(g'(t) = t g(t)\)</span> with conditions <span class="math inline">\(\mathbb{E} \cos(0X)=1\)</span> and <span class="math inline">\(\mathbb{E} \sin(0X) =0\)</span>. This entails <span class="math inline">\(\mathbb{E} \mathrm{e}^{itX} = \exp\Big(-\frac{t^2}{2}\Big)\)</span>, that is <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span></p>
</div>
<br>
<p>
</p>
</section>
<section id="diffintfourier" class="level3" data-number="7.4.5">
<h3 data-number="7.4.5" class="anchored" data-anchor-id="diffintfourier"><span class="header-section-number">7.4.5</span> Differentiability and integrability</h3>
<p>Differentiability of the Fourier transform at <span class="math inline">\(0\)</span> and integrability are intimately related.</p>
<div id="thm-fouriermoments" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.10</strong></span> If <span class="math inline">\(X\)</span> is <span class="math inline">\(p\)</span>-integrable for some <span class="math inline">\(p \in \mathbb{N}\)</span> then the Fourier transform of the distribution of <span class="math inline">\(X\)</span> is <span class="math inline">\(p\)</span>-times differentiable at <span class="math inline">\(0\)</span> and the <span class="math inline">\(p^{\text{th}}\)</span> derivative equals <span class="math inline">\(i^k \mathbb{E}X^k\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof relies on a Taylor expansion with remainder of <span class="math inline">\(x \mapsto \mathrm{e}^{ix}\)</span> at <span class="math inline">\(x=0\)</span>: <span class="math display">\[
\mathrm{e}^{ix} - \sum_{k=0}^n \frac{(ix)^k}{k!} = \frac{i^{n+1}}{n!} \int_0^x (x-s)^n \mathrm{e}^{is} \mathrm{d}s \, .
\]</span> The modulus of the right hand side can be upper-bounded in two different ways. <span class="math display">\[
\frac{1}{n+!}\Big| \int_0^x (x-s)^n \mathrm{e}^{is} \mathrm{d}s \Big|
  \leq \frac{|x|^{n+1}}{(n+1)!}
\]</span> which is good when <span class="math inline">\(|x|\)</span> is small. To handle large values of <span class="math inline">\(|x|\)</span>, integration by parts leads to <span class="math display">\[
\frac{i^{n+1}}{n!} \int_0^x (x-s)^n \mathrm{e}^{is} \mathrm{d}s =  \frac{i^{n}}{(n-1)!} \int_0^x (x-s)^{n-1} \left(\mathrm{e}^{is}-1\right) \mathrm{d}s \,.
\]</span> The modulus of the right hand side can be upper-bounded by <span class="math inline">\(2|x|^n/n!\)</span>.</p>
<p>Applying this Taylor expansion to <span class="math inline">\(x=t X\)</span>, using the pointwise upper bounds and taking expectations leads to <span class="math display">\[\begin{array}{rl}
  \Big| \widehat{F}(t) - \sum_{k=0}^n \mathbb{E}\frac{(itX)^k}{k!}  \Big|
    &amp; \leq \mathbb{E} \Big[\min\Big( \frac{|tX|^{n+1}}{(n+1)!} ,2 \frac{|tX|^n}{n!}\Big)\Big] \\
    &amp; = \frac{|t|^n}{(n+1)!} \mathbb{E} \Big[\min\Big(|tX|^{n+1} ,2 (n+1) |X|^n \Big)\Big] \, .
\end{array}\]</span> Note that the right hand side is well defined as soon as <span class="math inline">\(\mathbb{E}|X|^n &lt; \infty\)</span>. Now, by dominated convergence, <span class="math display">\[
\lim_{t \to 0} \mathbb{E} \Big[\min\Big(|tX|^{n+1} ,2 (n+1) |X|^n \Big)\Big] = 0\,
\]</span> Hence we have established that if <span class="math inline">\(\mathbb{E}|X|^n &lt; \infty\)</span>, <span class="math display">\[
\widehat{F}(t) = \sum_{k=0}^n i^k \mathbb{E}X^k \frac{t^k}{k!} + o(|t|^n) \, .
\]</span></p>
</div>
<br>
<p>
</p>
<p>In the other direction, the connection is not as simple: differentiability of the Fourier transform does not imply integrability. But the following holds.</p>
<div id="thm-fouriervariance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.11</strong></span> If the Fourier transform <span class="math inline">\(\widehat{F}\)</span> of the distribution of <span class="math inline">\(X\)</span> satisfies <span class="math display">\[
\lim_{h \downarrow 0} \frac{2 - \widehat{F}(h) - \widehat{F}(-h)}{h^2} = \sigma^2 &lt; \infty
\]</span> then <span class="math inline">\(\mathbb{E}X^2 = \sigma^2\)</span>, .</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Note that <span class="math display">\[
2 - \widehat{F}(h) - \widehat{F}(-h) =  2\mathbb{E}\Big[1 - \cos(hX)\Big] \, ,
\]</span> and using Taylor with remainder formula for <span class="math inline">\(\cos\)</span> at <span class="math inline">\(0\)</span>: <span class="math display">\[
1 - \cos x = \int_0^x \cos(s) (x-s) \mathrm{d}s = x^ 2 \int_0^1 \cos(sx) (1-s) \mathrm{d}s
\]</span> Note that <span class="math inline">\(\int_0^1 \cos(sx) (1-s) \mathrm{d}s\geq 0\)</span> for all <span class="math inline">\(x \in \mathbb{R}\)</span>. <span class="math display">\[\begin{array}{rl}
  \frac{2\mathbb{E}\Big[1 - \cos(hX)\Big]}{h^2}
    &amp; = 2 \frac{\mathbb{E}\Big[ h^2 X^2 \int_0^1 \cos(shX) (1-s) \mathrm{d}s\Big]}{h^2} \\
    &amp; = 2 \mathbb{E}\Big[ X^2 \int_0^1 \cos(shX) (1-s) \mathrm{d}s\Big] \, .
\end{array}\]</span> By Fatouâ€™s Lemma: <span class="math display">\[
\sigma^2 = \lim_{h \downarrow 0} 2\mathbb{E}\Big[ X^2 \int_0^1 \cos(shX) (1-s) \mathrm{d}s\Big]
\geq 2\mathbb{E}\Big[\liminf_{h \downarrow 0} X^2 \int_0^1 \cos(shX) (1-s) \mathrm{d}s \Big]
\]</span> but for all <span class="math inline">\(x \in \mathbb{R}\)</span>, by dominated convergence, <span class="math display">\[
\liminf_{h \downarrow 0} x^ 2 \int_0^1 \cos(shx) (1-s) \mathrm{d}s = \frac{x^2}{2} \, .
\]</span> Hence <span class="math display">\[
\sigma^2 \geq \mathbb{E} X^2 \, .
\]</span> The proof is completed by invoking <a href="#thm-fouriermoments" class="quarto-xref">Theorem&nbsp;<span>7.10</span></a>).</p>
</div>
</section>
<section id="cauchy" class="level3" data-number="7.4.6">
<h3 data-number="7.4.6" class="anchored" data-anchor-id="cauchy"><span class="header-section-number">7.4.6</span> Another application: understanding Cauchy distribution</h3>
<p>Assume <span class="math inline">\(U\)</span> is uniformly distributed over <span class="math inline">\(]0,1[\)</span>, let the real valued random variable <span class="math inline">\(X\)</span> be defined by <span class="math display">\[X = \tan\left(\frac{\pi}{2} (2 \times U -1)\right)\]</span>.</p>
<p>As <span class="math inline">\(\tan\)</span> is continuously increasing from <span class="math inline">\(-\pi/2\)</span> to <span class="math inline">\(\pi/2\)</span>, the cumulative distribution function of the distribution of <span class="math inline">\(X\)</span> is <span class="math display">\[\begin{array}{rl}
\mathbb{P}\{ X \leq x\} &amp; = \mathbb{P}\left\{\tan\left(\frac{\pi}{2}(2U-1)\right) \leq x\right\} \\
&amp; = \mathbb{P}\left\{U \leq \frac{1}{2} + \frac{1}{\pi}\arctan(x) \right\} \\
&amp; = \frac{1}{2} + \frac{1}{\pi}\arctan(x)
\end{array}\]</span> for <span class="math inline">\(x \in \mathbb{R}\)</span>.</p>
<p>As <span class="math inline">\(\arctan\)</span> has derivative <span class="math inline">\(x \mapsto \frac{1}{1+x^2}\)</span>, the cumulative distribution function is absolutely continuous with density: <span class="math display">\[\frac{1}{\pi} \frac{1}{1  + x^2}\]</span> This is the density of the Cauchy distribution.</p>
<p>Note that <span class="math inline">\(\mathbb{E}(X)_+ = \mathbb{E} (X)_- = \mathbb{E}|X| =\infty\)</span>. The Cauchy distribution is not integrable.</p>
<p>Now, assume <span class="math inline">\(X_1, X_2,, \ldots, X_n\)</span> are i.i.d. and Cauchy distributed. Let <span class="math inline">\(Z = \sum_{i=1}^n X_i/n\)</span>. How is <span class="math inline">\(Z\)</span> distributed? We might compute the convolution power of the Cauchy density. It turns out that starting from the characteristic function is much more simple.</p>
<p>We refrain from computing directly the characteristic function of the Cauchy distribution. We take a roundabout.</p>
<p>Let <span class="math inline">\(Y\)</span> be distributed according to Laplace distribution, that is with density <span class="math inline">\(y \mapsto  \frac{1}{2} \exp(-|y|)\)</span> for <span class="math inline">\(y \in \mathbb{R}\)</span>. The random variable <span class="math inline">\(Y\)</span> is symmetric (<span class="math inline">\(Y \sim -Y\)</span>). Let <span class="math inline">\(\widehat{F}_Y\)</span> denote the characteristic function of (the distribution of) <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[\begin{array}{rl}
  \widehat{F}_Y(t) &amp; = \mathbb{E}\mathrm{e}^{tY} \\
  &amp; = \mathbb{E}\cos(tY) \\
  &amp; =  \int_0^{\infty} \mathrm{e}^{-y} \cos(ty) \mathrm{d}y \\
  &amp; = \left[- \mathrm{e}^{-y} \cos(ty)\right]_0^\infty - t \int_{0}^\infty \mathrm{e}^{-ty} \sin(ty) \mathrm{d}y \\
  &amp; = 1 - t \int_{0}^\infty \mathrm{e}^{-y} \sin(ty) \mathrm{d}y \\
  &amp; = 1 -t \left[- \mathrm{e}^{-y} \sin(ty)\right]_0^\infty  - t^2 \int_0^\infty \mathrm{e}^{-y} \cos(ty) \mathrm{d}y \\
  &amp; = 1 - t^2 \widehat{F}_Y(t)
\end{array}\]</span> where we have performed integration by parts twice.</p>
<p>The characteristic function <span class="math inline">\(\widehat{F}_Y\)</span> satisfies <span class="math display">\[\widehat{F}_Y(t) = \frac{1}{1+ t^2}\, ,\]</span> up to <span class="math inline">\(\frac{1}{\pi}\)</span>, this is the density of the Cauchy distribution.</p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{F}_X(t) &amp; = \mathbb{E}\mathrm{e}^{itX}\\
  &amp; =  \int_{-\infty}^{\infty} \frac{1}{\pi} \frac{1}{1+x^2}  \cos(tx)\mathrm{d}x \\
  &amp; = \frac{2}{\pi} \int_0^\infty \cos(tx) \widehat{F}_Y(x) \mathrm{d}x \\
  &amp; = 2 \times \frac{1}{2\pi} \int_{-\infty}^{\infty} \mathrm{e}^{-itx} \widehat{F}_Y(x) \mathrm{d}x \\
  &amp; = 2 \times \frac{1}{2} \mathrm{e}^{-|t|} = \mathrm{e}^{-|t|}
\end{array}\]</span> where we have used the inversion formula.</p>
<p>Now, the characteristic function of the distribution of <span class="math inline">\(Z\)</span> is <span class="math display">\[\widehat{F}_Z(t) =\left(\mathrm{e}^{-\frac{|t|}{n}}\right)^n=  \widehat{F}_X(t)\]</span> which means <span class="math inline">\(Z \sim X\)</span>.</p>
<p>The basic tools of characteristic functions theory allow us to - compute the characteristic function of the Laplace distribution - compute the characteristic function of the Cauchy distribution by inversion - compute the characteristic function of sums of independant Cauchy random variables - show that the Cauchy distribution is <span class="math inline">\(1\)</span>-stable.</p>
<div id="rem">
<p>The density of the Laplace distribution is not differentiable at <span class="math inline">\(0\)</span>, this is reflected in the fact that its Fourier transform (the characteristic function of the Laplace distribution) is not integrable.</p>
<p>Conversely the lack of integrability of the Cauchy distribution is reflected in the non-differentiability of its characteristic function at <span class="math inline">\(0\)</span>.</p>
</div>
</section>
</section>
<section id="quantiles" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="quantiles"><span class="header-section-number">7.5</span> Quantile functions</h2>
<p>So far we have seen several characterizations of probability distributions: cumulative distribution functions, Laplace transform for distributions supported on <span class="math inline">\([0, \infty)\)</span>, characteristic functions. The last characterization is praised for its behavior with respect to sums of independent random variables.</p>
<p>For univariate distributions, a companion to the cumulative distribution function is the quantile function. It plays a significant role in simulations, statistics and risk theory.</p>
<p>A cumulative distribution function <span class="math inline">\(F\)</span> is non-negative, <span class="math inline">\([0,1]\)</span>-valued, non-decreasing, right-continuous, with left-limit at any point. The cumulative distribution function of a diffuse probability measure is continuous at any point.</p>
<p>The quantile function <span class="math inline">\(F^{\leftarrow}\)</span> is defined as an extended inverse of the cumulative distribution function <span class="math inline">\(F\)</span>.</p>
<div id="def-quantfun" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.3 (Quantile function)</strong></span> The quantile function <span class="math inline">\(F^{\leftarrow}\)</span> of random variable <span class="math inline">\(X\)</span> distributed according to <span class="math inline">\(P\)</span> (with cumulative distribution function <span class="math inline">\(F\)</span>) is defined as <span class="math display">\[\begin{array}{rl}
F^{\leftarrow} (p)
   &amp; = \inf \Big\{ x : P\{X \leq x\} \geq p \Big\} \\
   &amp; = \inf \Big\{ x : F (x) \geq p \Big\} \qquad \text{for } p \in (0,1).
\end{array}\]</span></p>
</div>
<p>The quantile function is non-decreasing and left-continuous. The interplay between the quantile and cumulative distribution functions is summarized in the next proposition.</p>
<div id="prp-quantilekit" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.11</strong></span> If <span class="math inline">\(F\)</span> and <span class="math inline">\(F^\leftarrow\)</span> are the cumulative distribution function and the quantile function of (the distribution of) <span class="math inline">\(X\)</span>, the following statements hold for <span class="math inline">\(p \in] 0, 1 [\)</span>:</p>
<ol type="1">
<li><span class="math inline">\(p \leq  F (x)\)</span> iff <span class="math inline">\(F^\leftarrow (p) \leq  x\)</span>.</li>
<li><span class="math inline">\(F \circ F^\leftarrow (p) \geq p\)</span> .</li>
<li><span class="math inline">\(F^\leftarrow \circ F (x) \leq  x\)</span> .</li>
<li>If <span class="math inline">\(F\)</span> is absolutely continuous, then <span class="math inline">\(F \circ F^\leftarrow (p) = p\)</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>According to the definition of <span class="math inline">\(F^\leftarrow\)</span> if <span class="math inline">\(F (x) \geq p\)</span> then <span class="math inline">\(F^\leftarrow
(p) \leq  x.\)</span></p>
<p>To prove the converse, it suffices to check that <span class="math inline">\(F \circ F^\leftarrow (p) \geq p\)</span>.</p>
<p>Indeed, if <span class="math inline">\(x \geq F^\leftarrow (p),\)</span> as <span class="math inline">\(F\)</span> is non-decreasing <span class="math inline">\(F (x) \geq F \circ F^\leftarrow (p)\)</span>. Si <span class="math inline">\(y = F^\leftarrow
(p),\)</span> par definition de <span class="math inline">\(y = F^\leftarrow (p),\)</span> il existe une non-increasing sequence <span class="math inline">\((z_n)_{n \in \mathbb{N}}\)</span> which converges to <span class="math inline">\(y\)</span> such that <span class="math inline">\(F (z_n) \geq p .\)</span> Mais as <span class="math inline">\(F\)</span> is right-continuous <span class="math inline">\(\lim_n F
(z_n) = F (\lim_n z_n) = F (y) .\)</span> Hence <span class="math inline">\(F (y) \geq p\)</span>.</p>
<p>We just proved 1. and 2.</p>
<p>3.) is an immediate consequence de 1). Let <span class="math inline">\(p = F (x) .\)</span> Hence <span class="math inline">\(p \leq  F (x),\)</span> according to 1.) this is equivalent to <span class="math inline">\(F^\leftarrow (p)
\leq  x,\)</span> that is <span class="math inline">\(F^\leftarrow \circ F (x) \leq  x.\)</span></p>
<p>4.) For every <span class="math inline">\(p\)</span> in <span class="math inline">\(] 0, 1 [,\)</span> <span class="math inline">\(\{ x : p = F (x)\}\)</span> is non-empty (Mean value Theorem). Let <span class="math inline">\(y = \inf \{ x : p = F (x)\} =F^\leftarrow (p)\)</span>. According to <span class="math inline">\(1)\)</span>, <span class="math inline">\(F (y) \geq p\)</span>. Now, if <span class="math inline">\((z_n)_{n \in \mathbb{N}}\)</span> is an increasing sequence converging to <span class="math inline">\(y\)</span>, for every <span class="math inline">\(n\)</span>, <span class="math inline">\(F (z_n) &lt; p,\)</span> and, by left-continuity, <span class="math inline">\(F (y) = F (\lim_n z_n) = \lim_n F (z_n) \leq
p.\)</span> Hence <span class="math inline">\(F (y) = p,\)</span> that is <span class="math inline">\(F \circ F^\leftarrow (p) = p.\)</span></p>
</div>
<p><br></p>
<div id="prp-quatile-transformation" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 7.12 (Quantile transformation)</strong></span> If <span class="math inline">\(U\)</span> is uniformly distributed on <span class="math inline">\((0,1)\)</span>, and <span class="math inline">\(F\)</span> is a cumulative distribution over <span class="math inline">\(\mathbb{R}\)</span>, <span class="math inline">\(F^{\leftarrow}(U)\)</span> has cumulative distribution <span class="math inline">\(F\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{array}{rl}
  P\Big\{ F^\leftarrow(U) \leq x \Big\}
  &amp; = P\Big\{ U \leq F(x) \Big\} \\
  &amp; = F(x) \, .
\end{array}\]</span></p>
<p><span class="math inline">\(\Box\)</span></p>
</div>
<div id="rem-quantile-transformation" class="proof remark">
<p><span class="proof-title"><em>Remark 7.2</em>. </span>The quantile transformation works whatever the continuity properties of <span class="math inline">\(F\)</span>.</p>
</div>
<p>The quantile transformation has many applications. It can be used to show stochastic domination properties.</p>
<div id="exm-exmples-quantiles" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.5</strong></span> In <a href="#fig-quantiles" class="quarto-xref">Figures&nbsp;<span>7.1</span></a> up to <a href="#fig-quantiles4" class="quarto-xref">Figure&nbsp;<span>7.4</span></a>, we illustrate quantile functions for discrete (binomial) distributions and for distributions that are neither discrete nor continuous. The quantile function of a discrete distribution is step function that jumps at the cumulative probability of every possible outcome. If a probability distribution is a mixture of a discrete distribution and a continuous distribution, the quantile function jumps at the cumulative probability of every possible outcome of the discrete component.</p>
</div>
<!-- ::: {#fig-quantiles} -->
<div class="cell">
<div class="cell-output-display">
<div id="fig-quantiles" class="quarto-float quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantiles-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="04-characterizations_files/figure-html/fig-quantiles-1.png" id="fig-quantiles" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-quantiles-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1
</figcaption>
</figure>
</div>
</div>
</div>
<!-- ::: -->
<!-- ::: {#fig-quantiles2} -->
<div class="cell">
<div class="cell-output-display">
<div id="fig-quantiles2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantiles2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="04-characterizations_files/figure-html/fig-quantiles2-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantiles2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.2: Quantile functions <span class="math inline">\(\max(X, \tau)\)</span> where <span class="math inline">\(X \sim \mathcal{N}(0,1)\)</span> for <span class="math inline">\(\tau \in \{0, 2\}\)</span>. Let <span class="math inline">\(\Phi^\leftarrow\)</span> denote the quantile function of <span class="math inline">\(\mathcal{N}(0,1)\)</span>. The quantile function of <span class="math inline">\(\max(X,\tau)\)</span> is <span class="math inline">\(\mathbb{I}_{(0,\Phi(\tau)]}(p) \times \tau + \Phi^{\leftarrow}(p) \times \mathbb{I}_{(\Phi(\tau), 1)}(p)= \Phi^{\leftarrow}(p \vee \Phi(\tau))\)</span>. The two distributions are neither absolutely continuous nor discrete.
</figcaption>
</figure>
</div>
</div>
</div>
<!-- ::: -->
<!-- ::: {#fig-quantiles3} -->
<div class="cell">
<div class="cell-output-display">
<div id="fig-quantiles3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantiles3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="04-characterizations_files/figure-html/fig-quantiles3-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantiles3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.3: Cumulative distribution functions for the probability distributions illustrated in <a href="#fig-quantiles2" class="quarto-xref">Figure&nbsp;<span>7.2</span></a>
</figcaption>
</figure>
</div>
</div>
</div>
<!-- ::: -->
<!-- ::: {#fig-quantiles4} -->
<div class="cell">
<div class="cell-output-display">
<div id="fig-quantiles4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantiles4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="04-characterizations_files/figure-html/fig-quantiles4-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantiles4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.4: Representation of <span class="math inline">\(F \circ F^{\leftarrow}\)</span> for the probability distributions illustrated in <a href="#fig-quantiles2" class="quarto-xref">Figures&nbsp;<span>7.2</span></a> and <a href="#fig-quantiles3" class="quarto-xref">Figure&nbsp;<span>7.3</span></a>. The function <span class="math inline">\(F \circ F^{\leftarrow}\)</span> always lies above the line <span class="math inline">\(y=x\)</span> (dotted line) as prescribed in <a href="#prp-quantilekit" class="quarto-xref">Proposition&nbsp;<span>7.11</span></a>. Plateaux that lie strictly above the dotted line are in correspondence with jumps of the quantile function.
</figcaption>
</figure>
</div>
</div>
</div>
<!-- ::: -->
<!-- ::: {#fig-qqplot} -->
<div class="cell">
<div class="cell-output-display">
<div id="fig-qqplot" class="quarto-float quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qqplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="04-characterizations_files/figure-html/fig-qqplot-1.png" id="fig-qqplot" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-qqplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.5
</figcaption>
</figure>
</div>
</div>
</div>
<!-- ::: -->
<p><br></p>
<p>Let us conclude this section with an important observation. concerning the behavior of <span class="math inline">\(F(X)\)</span> when <span class="math inline">\(X \sim P\)</span> with cumulative distribution function <span class="math inline">\(F\)</span>. <br></p>
<div id="cor-corscore" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 7.4</strong></span> If <span class="math inline">\(X \sim P\)</span> with continuous cumulative distribution function <span class="math inline">\(F\)</span>, then <span class="math inline">\(F (X)\)</span> and <span class="math inline">\(1 - F (X)\)</span> are uniformly distributed on <span class="math inline">\([0, 1]\)</span>.</p>
</div>
<div id="exr-corscore" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.2</strong></span> Prove <a href="#cor-corscore" class="quarto-xref">Corollary&nbsp;<span>7.4</span></a></p>
</div>
</section>
<section id="sec-bibcharac" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="sec-bibcharac"><span class="header-section-number">7.6</span> Bibliographic remarks</h2>
<p><span class="citation" data-cites="wilf2005generatingfunctionology">Wilf (<a href="#ref-wilf2005generatingfunctionology" role="doc-biblioref">2005</a>)</span> explores the interplay between combinatorics, algorithm analysis and generating function theory.</p>
<p><span class="citation" data-cites="widder2015laplace">Widder (<a href="#ref-widder2015laplace" role="doc-biblioref">2015</a>)</span> is a classic reference on Laplace transforms. Laplace transforms play an important role in Point Process Theory, and Extreme Value Theory, to name a few fields of application.</p>
<p>The first part of Chapter 9 from <span class="citation" data-cites="MR1873379">Pollard (<a href="#ref-MR1873379" role="doc-biblioref">2002</a>)</span> describes characteristic functions as Fourier transforms. Properties and applications of characteristic functions are thoroughly discussed in <span class="citation" data-cites="Dur10">(<a href="#ref-Dur10" role="doc-biblioref">Durrett, 2010</a>)</span>, <span class="citation" data-cites="MR2893652">(<a href="#ref-MR2893652" role="doc-biblioref">Billingsley, 2012</a>)</span>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-MR2893652" class="csl-entry" role="listitem">
Billingsley, P. (2012). <em>Probability and measure</em>. John Wiley &amp; Sons, Inc., Hoboken, NJ.
</div>
<div id="ref-Dur10" class="csl-entry" role="listitem">
Durrett, R. (2010). <em>Probability: Theory and examples</em>. Cambridge University Press.
</div>
<div id="ref-MR1873379" class="csl-entry" role="listitem">
Pollard, D. (2002). <em>A userâ€™s guide to measure theoretic probability</em> (Vol. 8, p. xiv+351). Cambridge University Press, Cambridge.
</div>
<div id="ref-widder2015laplace" class="csl-entry" role="listitem">
Widder, D. V. (2015). <em>Laplace transform (PMS-6)</em>. Princeton university press.
</div>
<div id="ref-wilf2005generatingfunctionology" class="csl-entry" role="listitem">
Wilf, H. S. (2005). <em>Generatingfunctionology</em>. AK Peters/CRC Press.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./032-acfamilies.html" class="pagination-link" aria-label="Absolutely continuous probability measures">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Absolutely continuous probability measures</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06-conditioning.html" class="pagination-link" aria-label="Conditioning">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Conditioning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>